var documenterSearchIndex = {"docs":
[{"location":"man/math/#Details-of-Parameter-Estimation","page":"Mathematical Details","title":"Details of Parameter Estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"This note is meant to supplement our paper. For a review on generalized linear models, I recommend chapter 15.3 of Applied regression analysis and generalized linear models by John Fox, or chapter 3-5 of An introduction to generalized linear models by Dobson and Barnett. ","category":"page"},{"location":"man/math/#Generalized-linear-models","page":"Mathematical Details","title":"Generalized linear models","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, phenotypes (bf y) are modeled as a generalized linear model: \\begin{aligned}     \\mui = E(yi) = g({\\bf x}i^t {\\boldsymbol \\beta}) \\end{aligned} where bf x is sample i's p-dimensional vector of covariates (genotypes + other fixed effects), boldsymbol beta is a p-dimensional regression coefficients, g is a non-linear inverse-link function, yi$ is sample i's phenotype value, and mu_i is the average predicted value of y_i given bf x. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The regression coefficients boldsymbol beta are not observed and are estimated via maximum likelihood. The full design matrix bf X (obtained by stacking each bf x_i^t row-by-row) and phenotypes bf y are observed. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases. Of course, when g(alpha) = alpha we get the standard linear model used for Gaussian phenotypes. ","category":"page"},{"location":"man/math/#Implementation-details-of-loglikelihood,-gradient,-and-expected-information","page":"Mathematical Details","title":"Implementation details of loglikelihood, gradient, and expected information","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In GLM, the distribution of bf y is from the exponential family with density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"f(y mid theta phi) = exp left fracy theta - b(theta)a(phi) + c(y phi) right","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"theta","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"is called the canonical (location) parameter and under the canonical link, theta = g(bf x^t bf beta). phi is the dispersion (scale) parameter. The functions a b c are known functions that vary depending on the distribution of y. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Given n independent observations, the loglikelihood is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     L({\\bf \\theta}, \\phi; {\\bf y}) &= \\sum{i=1}^n \\frac{yi\\thetai - b(\\thetai)}{ai(\\phi)} + c(yi, \\phi). \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate the loglikelihood, we use the logpdf function in Distributions.jl.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The perform maximum likelihood estimation, we compute partial derivatives for betas. The jth score component is (eq 4.18 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     \\frac{\\partial L}{\\partial \\betaj} = \\sum{i=1}^n \\left[\\frac{yi - \\mui}{var(yi)}x{ij}\\left(\\frac{\\partial \\mui}{\\partial \\etai}\\right)\\right]. \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus the full gradient is","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     \\nabla L&= {\\bf X}^t{\\bf W}({\\bf y} - \\boldsymbol\\mu), \\quad {\\bf W}{ii} = \\frac{1}{var(yi)}\\left(\\frac{\\partial \\mui}{\\partial \\etai}\\right), \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"and similarly, the expected information is (eq 4.23 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     J = {\\bf X^t\\tilde{W}X}, \\quad {\\bf \\tilde{W}}{ii} = \\frac{1}{var(yi)}\\left(\\frac{\\partial \\mui}{\\partial \\etai}\\right)^2 \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate nabla L and J, note bf y and bf X are known, so we just need to calculate boldsymbolmu fracpartialmu_ipartialeta_i and var(y_i). The first simply uses the inverse link: mu_i = g(bf x_i^t boldsymbol beta). For the second, note fracpartial mu_ipartialeta_i = fracpartial g(bf x_i^t boldsymbol beta)partialbf x_i^t boldsymbol beta is just the derivative of the link function evaluated at the linear predictor eta_i = bf x_i^t boldsymbol beta. This is already implemented for various link functions as mueta in GLM.jl, which we call internally. To compute var(y_i), we note that the exponential family distributions have variance","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     var(y) &= a(\\phi)b''(\\theta) = a(\\phi)\\frac{\\partial^2b(\\theta)}{\\partial\\theta} = a(\\phi) var(\\mu). \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"That is, var(y_i) is a product of 2 terms where the first depends solely on phi, and the second solely on mu = g(bf x_i^t boldsymbol beta). In our code, we use glmvar implemented in GLM.jl to calculate var(mu). Because phi is unknown, we assume a(phi) = 1 for all models except the negative binomial model. For negative binomial model, we discuss how to estimate phi and boldsymbolbeta using alternate block descent below.  ","category":"page"},{"location":"man/math/#Iterative-hard-thresholding","page":"Mathematical Details","title":"Iterative hard thresholding","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, the loglikelihood is maximized using iterative hard thresholding. This is achieved by","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     \\boldsymbol\\beta{n+1} = \\overbrace{P{Sk}}^{(3)}\\big(\\boldsymbol\\betan - \\underbrace{sn}{(2)} \\overbrace{\\nabla f(\\boldsymbol\\beta_n)}^{(1)}\\big) \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where f is the function to minimize (i.e. negative loglikelihood), s_k is the step size, and P_S_k is a projection operator that sets all but k largest entries in magnitude to 0. I already discussed above how to compute the gradient of a GLM loglikelihood. To perform P_S_k, we first partially sort the dense vector beta_n - s_n nabla f(beta_n), and set all k+1  n entries to 0. Finally, the step size s_n is derived in our paper to be","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     sn = \\frac{||\\nabla f(\\boldsymbol\\betan)||2^2}{\\nabla f(\\boldsymbol\\betan)^t J(\\boldsymbol\\betan) \\nabla f(\\boldsymbol\\betan)} \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where J = bf X^ttildeWX is the expected information matrix (derived in the previous section) which should never be explicitly formed. To evaluate the denominator, observe that ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"\\begin{aligned}     \\nabla f(\\boldsymbol\\betan)^t J(\\boldsymbol\\betan) \\nabla f(\\boldsymbol\\betan) = \\left(\\nabla f(\\boldsymbol\\betan)^t{\\bf X}^t \\sqrt(\\tilde{W})\\right)\\left(\\sqrt(\\tilde{W}){\\bf X}\\nabla f(\\boldsymbol\\beta_n)\\right). \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus one computes bf v = sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n) and calculate its inner product with itself. ","category":"page"},{"location":"man/math/#Nuisance-parameter-estimation","page":"Mathematical Details","title":"Nuisance parameter estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Currently MendelIHT.jl only estimates nuisance parameter for the Negative Binomial model. This feature is provided by our 2019 Bruins in Genomics summer student Vivian Garcia and Francis Adusei. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note for Gaussian response, one can use the sample variance formula to estimate phi from the estimated mean hatmu. ","category":"page"},{"location":"man/math/#Parametrization-for-Negative-Binomial-model","page":"Mathematical Details","title":"Parametrization for Negative Binomial model","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The negative binomial distribution has density  \\begin{aligned} \tP(Y = y) = \\binom{y+r-1}{y}p^r(1-p)^y \\end{aligned} where y is the number of failures before the rth success and p is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to WOLFRAM is  \\begin{align} \t\\mui = \\frac{r(1-pi)}{r}, \\quad \tVar(yi) = \\frac{r(1-pi)}{p_i^2}. \\end{align} Note these formula are different than the default on wikipedia because in wiki y is the number of success and r is the number of failure.  Therefore, solving for p_i, we have  \\begin{aligned} \tpi = \\frac{r}{\\mui + r} = \\frac{r}{e^{\\mathbf{x}i^T\\beta} + r} \\in (0, 1). \\end{aligned} And indeed this this is how we parametrize the negative binomial model. **Importantly, we can interpret pi$ as a probability**, since mathbfx_i^Tbeta can take on any number between -infty and +infty (since beta and mathbfx_i can have positive and negative entries), so exp(mathbfx_i^Tbeta)in(0 infty).","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"We can also try to express Var(y_i) in terms of mu_i and r by doing some algebra: \\begin{aligned} \tVar(yi) \t&= \\frac{r(1-pi)}{pi^2} = \\frac{r\\left( 1 - \\frac{r}{\\mui + r} \\right)}{\\frac{r^2}{(\\mui + r)^2}} = \\frac{1}{r}\\left(1 - \\frac{r}{\\mui + r}\\right)(\\mui + r)^2 \\\n\t&= \\frac{1}{r} \\left[ (\\mui + r)^2 - r(\\mur + r) \\right] = \\frac{1}{r}(\\mui + r)\\mui\\\n\t&= \\mui \\left( \\frac{\\mu_i}{r} + 1 \\right) \\end{aligned} You can verify in GLM.jl that this is indeed how they compute the variance of a negative binomial distribution. ","category":"page"},{"location":"man/math/#Estimating-nuisance-parameter-using-MM-algorithms","page":"Mathematical Details","title":"Estimating nuisance parameter using MM algorithms","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The MM algorithm is very stable, but converges much slower than Newton's alogorithm below. Thus use MM only if Newton's method fails.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The loglikelihood for n independent samples under a Negative Binomial model is  \\begin{aligned} \tL(p1, ..., pm, r) \t&= \\sum{i=1}^m \\ln \\binom{yi+r-1}{yi} + r\\ln(pi) + yi\\ln(1-pi)\\\n\t&= \\sum{i=1}^m \\left[ \\sum{j=0}^{yi - 1} \\ln(r+j) + r\\ln(pi) - \\ln(yi!) + yi\\ln(1-pi) \\right]\\\n\t&\\geq \\sum{i=1}^m\\left[ \\sum{j=0}^{yi-1}\\frac{rn}{rn+j}\\ln(r) + cn + r\\ln(pi) - \\ln(yi!) + yi \\ln(1-p_i) \\right] \\end{aligned}","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The last inequality can be seen by applying Jensen's inequality: \\begin{align} \tf\\left[ \\sum{i}ui(\\boldsymbol\\theta)\\right] \\leq \\sum{i} \\frac{ui(\\boldsymbol\\thetan}{\\sumj uj(\\boldsymbol\\thetan)}f \\left[ \\frac{\\sumj uj(\\boldsymbol\\thetan)}{ui(\\boldsymbol\\thetan)} ui(\\boldsymbol\\theta)\\right] \\end{align} to the function f(u) = - ln(u) Maximizing L over r (i.e. differentiating with respect to r and setting equal to zero, then solving for r), we have \\begin{align} \t\\frac{d}{dr} L(p1,...,pm,r)  \t&= \\sum{i=1}^{m} \\left[ \\sum{j=0}^{yi-1} \\frac{rn}{rn + j} \\frac{1}{r} + \\ln(pi) \\right] \\\n\t&= \\sum{i=1}^{m}\\sum{j=0}^{yi-1} \\frac{rn}{rn + j} \\frac{1}{r} + \\sum{i=1}^{m}\\ln(pi)\\  \t&\\equiv 0\\\n\t\\iff r{n+1} &= \\frac{-\\sum{i=1}^{m}\\sum{j=0}^{yi-1} \\frac{rn}{rn + j}}{\\sum{i=1}^{m}\\ln(p_i) }  \\end{align}","category":"page"},{"location":"man/math/#Estimating-Nuisance-parameter-using-Newton's-method","page":"Mathematical Details","title":"Estimating Nuisance parameter using Newton's method","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since we are dealing with 1 parameter optimization, Newton's method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (r), we use maximum likelihood estimates. By p_i = r  (mu_i + r) in above, we have \\begin{aligned} \t& L(p1, ..., pm, r)\\\n\t=& \\sum{i=1}^m \\ln \\binom{yi+r-1}{yi} + r\\ln(pi) + yi\\ln(1-pi)\\\n\t=& \\sum{i=1}^m \\left[ \\ln\\left((yi+r-1)!\\right) - \\ln\\left(yi!\\right) - \\ln\\left((r-1)!\\right) + r\\ln(r) - r\\ln(\\mui+r) + yi\\ln(\\mui) + yi\\ln(\\mui + r)\\right]\\\n\t=& \\sum{i=1}^m\\left[\\ln\\left((yi+r-1)!\\right)-\\ln(yi!) - \\ln\\left((r-1)\\right) + r\\ln(r) - (r+yi)\\ln(\\mui + r) + yi\\ln(\\mui)\\right] \\end{aligned} Recalling the definition of [digamma and trigamma functions](https://en.wikipedia.org/wiki/Digammafunction), the first and second derivative of L with respect to r is: \\begin{aligned} \t\\frac{d}{dr} L(p1, ..., pm, r) = & \\sum{i=1}^m \\left[ \\operatorname{digamma}(yi+r) - \\operatorname{digamma}(r) + 1 + \\ln(r) - \\frac{r+yi}{\\mui+r} - \\ln(\\mui + r) \\right]\\\n\t\\frac{d^2}{dr^2} L(p1, ..., pm, r) =&\\sum{i=1}^m \\left[ \\operatorname{trigamma}(yi+r) - \\operatorname{trigamma}(r) + \\frac{1}{r} - \\frac{2}{\\mui + r} + \\frac{r+yi}{(\\mui + r)^2} \\right] \\end{aligned} So the iteration to use is: \\begin{aligned} \tr{n+1} = rn - \\frac{\\frac{d}{dr}L(p1,...,pm,r)}{\\frac{d^2}{dr^2}L(p1,...,pm,r)}. \\end{aligned} You can verify that this is the same as the first and second derivative formula in GLM.jl(they used theta in place of r). The sign difference is because in GLM.jl, they are minimizing the negative loglikelihood instead of maximizing the loglikelihood. They are equivalent, but in mathematical optimization the standard form is to minimize an objective function. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"I am one developer. We are a community. ","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you would like to contribute to this project, we compiled a list of desired features for this project. Developers of any level is welcomed. Do not be shy because it can't hurt to ask. ","category":"page"},{"location":"man/contributing/#Bug-Fixes-and-User-Support","page":"Contributing","title":"Bug Fixes & User Support","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you encounter a bug or you need some user support, please open a new issue here. If you can, provide the error message and, ideally, a reproducible code that generated the error.","category":"page"},{"location":"man/contributing/#Citation","page":"Contributing","title":"Citation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you use MendelIHT.jl in an academic manuscript, please cite:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Benjamin B. Chu, Kevin L. Keys, Christopher A. German, Hua Zhou, Jin J. Zhou, Janet S. Sinsheimer, Kenneth Lange. Iterative Hard Thresholding in GWAS: Generalized Linear Models, Prior Weights, and Double Sparsity. bioRxiv doi:10.1101/697755","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Bibtex:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"@article{zhou2019openmendel,\n  title={{Iterative Hard Thresholding in GWAS: Generalized Linear Models, Prior Weights, and Double Sparsity}},\n  author={Chu, Benjamin B and Keys, Kevin L and German, Christopher A and Zhou, Hua and Zhou, Jin J and Sinsheimer, Janet S and Lange, Kenneth},\n  journal={BioRxiv},\n  pages={697755v2},\n  year={2019},\n  publisher={Cold Spring Harbor Laboratory}\n}","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you could also press star on the upper right hand corner on our github page, that would be very helpful. ","category":"page"},{"location":"man/getting_started/#Getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"In this section, we outline the basic procedure to analyze your GWAS data with MendelIHT. ","category":"page"},{"location":"man/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Download and install Julia. Within Julia, copy and paste the following:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/SnpArrays.jl.git\"))\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/VCFTools.jl.git\"))\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/MendelIHT.jl.git\"))","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT.jl supports Julia 1.5+ for Mac, Linux, and window machines. A few features are disabled for windows users, and users will be warned when trying to use them.","category":"page"},{"location":"man/getting_started/#Typical-Workflow","page":"Getting Started","title":"Typical Workflow","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Run cross_validate() to determine best sparsity level (k).\nRun iht on optimal k.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"We believe the best way to learn is through examples. Head over to the example section on the left to see these steps in action. ","category":"page"},{"location":"man/getting_started/#Wrapper-Functions","page":"Getting Started","title":"Wrapper Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Most users will use the following wrapper functions, which automatically handles everything. The user only has to specify where the PLINK files (and possibly the phenotype/covariate files) are located. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  iht","category":"page"},{"location":"man/getting_started/#MendelIHT.iht","page":"Getting Started","title":"MendelIHT.iht","text":"iht(plinkfile, k, kwargs...)\n\nRuns IHT with sparsity level k. Example:\n\nresult = iht(\"plinkfile\", 10)\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\niht(plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, with additional covariates stored separately. Example:\n\nresult = iht(\"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\niht(phenotypes, plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, where both phenotypes and additional covariates are stored separately. Example:\n\nresult = iht(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cross_validate","category":"page"},{"location":"man/getting_started/#MendelIHT.cross_validate","page":"Getting Started","title":"MendelIHT.cross_validate","text":"cross_validate(plinkfile, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", 1:20)\nmses = cross_validate(\"plinkfile\", [1, 2, 3, 4, 5]) # alternative syntax\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(phenotypes, plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/#Core-Functions","page":"Getting Started","title":"Core Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"For advanced users, one can also run IHT regression or cross-validation directly. For cross validation, we generally recommend using cv_iht. This function cycles through the testing sets sequentially and fits different sparsity models in parallel. For larger problems (e.g. UK Biobank sized), one can instead choose to run cv_iht_distribute_fold. This function fits different sparsity models sequentially but initializes all training/testing model in parallel, which consumes more memory (see below). The later strategy allows one to distribute different sparsity parameters to different computers, achieving greater parallel power. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  fit","category":"page"},{"location":"man/getting_started/#StatsBase.fit","page":"Getting Started","title":"StatsBase.fit","text":"fit(Mod::Type{<:StatisticalModel}, f::FormulaTerm, data, args...; \n    contrasts::Dict{Symbol}, kwargs...)\n\nConvert tabular data into a numeric response vector and predictor matrix using the formula f, and then fit the specified model type, wrapping the result in a TableRegressionModel or TableStatisticalModel (as appropriate).\n\nThis is intended as a backstop for modeling packages that implement model types that are subtypes of StatsBase.StatisticalModel but do not explicitly support the full StatsModels terms-based interface.  Currently this works by creating a ModelFrame from the formula and data, and then converting this to a ModelMatrix, but this is an internal implementation detail which may change in the near future.\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cv_iht","category":"page"},{"location":"man/getting_started/#MendelIHT.cv_iht","page":"Getting Started","title":"MendelIHT.cv_iht","text":"cv_iht(y, x, z; kwargs...)\n\nFor each model specified in path, performs q-fold cross validation and  returns the (averaged) deviance residuals. \n\nThe purpose of this function is to find the best sparsity level k, obtained from selecting the model with the minimum out-of-sample error. Different cross validation folds are cycled through sequentially different paths are fitted in parallel on different CPUs. Currently there are no routines to cross validate different group sizes. \n\nWarning\n\nDo not remove files with random file names when you run this function. These are  memory mapped files that will be deleted automatically once they are no longer needed.\n\nArguments\n\ny: Response vector (phenotypes), should be an Array{T, 1}.\nx: A design matrix (genotypes). Should be a SnpArray or an Array{T, 2}. \nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}. The first column should be the intercept (i.e. column of 1). \n\nOptional Arguments:\n\npath: Different model sizes to be tested in cross validation (default 1:20)\nq: Number of cross validation folds. (default 5)\nd: Distribution of your phenotype. (default Normal)\nl: A link function (default IdentityLink)\nest_r: Symbol for whether to estimate nuisance parameters. Only supported distribution is negative binomial and choices include :Newton or :MM.\ngroup: vector storing group membership for each predictor\nweight: vector storing vector of weights containing prior knowledge on each predictor\nfolds: Vector that separates the sample into q disjoint subsets\ndestin: Directory where intermediate files will be generated. Directory name must end with /.\ninit: Boolean indicating whether we should initialize IHT algorithm at a good starting guess\nuse_maf: Boolean indicating we should scale the projection step by a weight vector \ndebias: Boolean indicating whether we should debias at each IHT step\nverbose: Whether we want IHT to print meaningful intermediate steps\nparallel: Whether we want to run cv_iht using multiple CPUs (highly recommended)\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cv_iht_distribute_fold","category":"page"},{"location":"man/getting_started/#MendelIHT.cv_iht_distribute_fold","page":"Getting Started","title":"MendelIHT.cv_iht_distribute_fold","text":"Performs q-fold cross validation for Iterative hard thresholding to  determine the best model size k. The function is the same as cv_iht  except here each fold is distributed to a different CPU as opposed  to each path to a different CPU. \n\nThis function has the edge over cv_iht because one can fit different  sparsity levels on different computers. But this is assuming you have  enough RAM and disk space to store all training data simultaneously.\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nDo not delete intermediate files with random file names created by cv_iht and cv_iht_distribute_fold (windows users will be instructed to manually do so via print statements). These are memory-mapped files necessary for cross validation. For cv_iht, you must have x GB of free space and RAM on your hard disk where x is your .bed file size. For cv_iht_distribute_fold, you must have enough RAM and disk space to fit all q training datasets simultaneously, each of which typically requires (q - 1)/q * x GB. ","category":"page"},{"location":"man/getting_started/#Specifying-Groups-and-Weights","page":"Getting Started","title":"Specifying Groups and Weights","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"When you have group and weight information, you input them as optional arguments in L0_reg and cv_iht. The weight vector is a vector of Float64, while the group vector is a vector of integers. For instance,","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"    g = #import group vector\n    w = #import weight vector\n    J = length(unique(g)) # specify number of non-zero groups\n    result = L0_reg(x, xbm, z, y, J, k, d(), l, group=g, weight=w)","category":"page"},{"location":"man/getting_started/#Simulation-Utilities","page":"Getting Started","title":"Simulation Utilities","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT provides some simulation utilities that help users explore the function and capabilities of iterative hard thresholding. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_random_snparray","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_random_snparray","page":"Getting Started","title":"MendelIHT.simulate_random_snparray","text":"simulate_random_snparray(s::String, n::Integer, p::Integer; \n    [mafs::Vector{Float64}], [min_ma::Integer])\n\nCreates a random SnpArray in the current directory without missing value,  where each SNP has ⫺5 (default) minor alleles. \n\nNote: if supplied minor allele frequency is extremely small, it could take a long time for the simulation to generate samples where at least min_ma (defaults to 5) are present. \n\nArguments:\n\ns: name of SnpArray that will be created in the current directory. To not   create file, use undef.\nn: number of samples\np: number of SNPs\n\nOptional Arguments:\n\nmafs: vector of desired minor allele freuqencies (uniform(0,0.5) by default)\nmin_ma: the minimum number of minor alleles that must be present for each   SNP (defaults to 5)\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_correlated_snparray","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_correlated_snparray","page":"Getting Started","title":"MendelIHT.simulate_correlated_snparray","text":"simulate_correlated_snparray(s, n, p; block_length, hap, prob)\n\nSimulates a SnpArray with correlation. SNPs are divided into blocks where each adjacent SNP is the same with probability prob. There are no correlation between blocks.\n\nArguments:\n\nn: number of samples\np: number of SNPs\ns: name of SnpArray that will be created (memory mapped) in the current directory. To not memory map, use undef.\n\nOptional arguments:\n\nblock_length: length of each LD block\nhap: number of haplotypes to simulate for each block\nprob: with probability prob an adjacent SNP would be the same. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nSimulating a SnpArray with n subjects and p SNPs requires up to 4np bits of RAM. Make sure you have enough RAM before simulating very large SnpArrays.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_random_response","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_random_response","page":"Getting Started","title":"MendelIHT.simulate_random_response","text":"simulate_random_response(x, k, d, l; kwargs...)\n\nThis function simulates a random response (trait) vector y. When the  distribution d is from Poisson, Gamma, or Negative Binomial, we simulate  β ∼ N(0, 0.3) to roughly ensure the mean of response y doesn't become too large. For other distributions, we choose β ∼ N(0, 1). \n\nArguments\n\nx: Design matrix\nk: the true number of predictors. \nd: The distribution of the simulated trait (note typeof(d) = UnionAll but typeof(d()) is an actual distribution: e.g. Normal)\nl: The link function. Input canonicallink(d()) if you want to use the canonical link of d.\n\nOptional arguments\n\nr: The number of success until stopping in negative binomial regression, defaults to 10\nα: Shape parameter of the gamma distribution, defaults to 1\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor negative binomial and gamma, the link function must be LogLink. For Bernoulli, the probit link seems to work better than logitlink when used in cv_iht or L0_reg. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  make_bim_fam_files","category":"page"},{"location":"man/getting_started/#MendelIHT.make_bim_fam_files","page":"Getting Started","title":"MendelIHT.make_bim_fam_files","text":"make_bim_fam_files(x::SnpArray, y, name::String)\n\nCreates .bim and .bed files from a SnpArray. \n\nArguments:\n\nx: A SnpArray (i.e. .bed file on the disk) for which you wish to create corresponding .bim and .fam files.\nname: string that should match the .bed file (Do not include .bim or .fam extensions in name).\ny: Trait vector that will go in to the 6th column of .fam file. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/#Other-Useful-Functions","page":"Getting Started","title":"Other Useful Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT additionally provides useful utilities that may be of interest to a few advanced users. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  iht_run_many_models","category":"page"},{"location":"man/getting_started/#MendelIHT.iht_run_many_models","page":"Getting Started","title":"MendelIHT.iht_run_many_models","text":"Runs IHT across many different model sizes specifed in path using the full design matrix. Same as cv_iht but DOES NOT validate in a holdout set, meaning that this will definitely induce overfitting as we increase model size. Use this if you want to quickly estimate a range of feasible model sizes before  engaging in full cross validation. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  maf_weights","category":"page"},{"location":"man/getting_started/#MendelIHT.maf_weights","page":"Getting Started","title":"MendelIHT.maf_weights","text":"maf_weights(x::SnpArray; max_weight::T = Inf)\n\nCalculates the prior weight based on minor allele frequencies. \n\nReturns an array of weights where w[i] = 1 / (2 * sqrt(p[i] (1 - p[i]))) ∈ (1, ∞). Here p is the minor allele frequency computed by maf() in SnpArrays. \n\nx: A SnpArray \nmax_weight: Maximum weight for any predictor. Defaults to Inf. \n\n\n\n\n\n","category":"function"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we give numerous example analysis of GWAS data with MendelIHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# machine information for reproducibility\nversioninfo()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Julia Version 1.0.3\nCommit 099e826241 (2018-12-18 01:34 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin14.5.0)\n  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#first add workers needed for parallel computing. Add only as many CPU cores available\nusing Distributed\naddprocs(4)\n\n#load necessary packages for running all examples below\nusing MendelIHT\nusing SnpArrays\nusing DataFrames\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing GLM\nusing DelimitedFiles\nusing Statistics\nusing BenchmarkTools","category":"page"},{"location":"man/examples/#Example-1:-How-to-Import-Data","page":"Examples","title":"Example 1: How to Import Data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We use SnpArrays.jl as backend to process genotype files. Internally, the genotype file is a memory mapped SnpArray, which will not be loaded into RAM. If you wish to run L0_reg, you need to convert a SnpArray into a SnpBitMatrix, which consumes n times p times 2 bits of RAM. Non-genetic predictors should be read into Julia in the standard way, and should be stored as an Array{Float64, 2}. One should include the intercept (typically in the first column), but an intercept is not required to run IHT. ","category":"page"},{"location":"man/examples/#Simulate-example-data-(to-be-imported-later)","page":"Examples","title":"Simulate example data (to be imported later)","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate an example PLINK trio (.bim, .bed, .fam) and non-genetic covariates, then we illustrate how to import them. For genotype matrix simulation, we simulate under the model:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# rows and columns\nn = 1000\np = 10000\n\n#random seed\nRandom.seed!(1111)\n\n# simulate random `.bed` file\nx = simulate_random_snparray(n, p, \"example.bed\")\n\n# create accompanying `.bim`, `.fam` files (randomly generated)\nmake_bim_fam_files(x, ones(n), \"example\")\n\n# simulate non-genetic covariates (in this case, we include intercept and sex)\nz = [ones(n, 1) rand(0:1, n)]\nwritedlm(\"example_nongenetic_covariates.txt\", z)","category":"page"},{"location":"man/examples/#Reading-Genotype-data-and-Non-Genetic-Covariates-from-disk","page":"Examples","title":"Reading Genotype data and Non-Genetic Covariates from disk","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x = SnpArray(\"example.bed\")\nz = readdlm(\"example_nongenetic_covariates.txt\")","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"1000×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  0.0\n 1.0  0.0\n 1.0  1.0\n 1.0  0.0\n 1.0  0.0\n 1.0  1.0\n 1.0  0.0\n 1.0  1.0\n 1.0  1.0\n 1.0  0.0\n 1.0  0.0\n 1.0  1.0\n ⋮       \n 1.0  0.0\n 1.0  1.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  0.0\n 1.0  1.0\n 1.0  0.0","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"note: Note\n(1) MendelIHT.jl assumes there are NO missing genotypes, (2) 1 always encode the minor allele, and (3) the trios (.bim, .bed, .fam) are all be present in the same directory. ","category":"page"},{"location":"man/examples/#Standardizing-Non-Genetic-Covariates.","page":"Examples","title":"Standardizing Non-Genetic Covariates.","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We recommend standardizing all genetic and non-genetic covarariates (including binary and categorical), except for the intercept. This ensures equal penalization for all predictors. For genotype matrix, SnpBitMatrix efficiently achieves this standardization. For non-genetic covariates, we use the built-in function standardize!. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# SnpBitMatrix can automatically standardizes .bed file (without extra memory) and behaves like a matrix\nxbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n\n# using view is important for correctness\nstandardize!(@view(z[:, 2:end])) \nz","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"1000×2 Array{Float64,2}:\n 1.0   1.0015  \n 1.0  -0.997503\n 1.0  -0.997503\n 1.0   1.0015  \n 1.0  -0.997503\n 1.0  -0.997503\n 1.0   1.0015  \n 1.0  -0.997503\n 1.0   1.0015  \n 1.0   1.0015  \n 1.0  -0.997503\n 1.0  -0.997503\n 1.0   1.0015  \n ⋮             \n 1.0  -0.997503\n 1.0   1.0015  \n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0  -0.997503\n 1.0   1.0015  \n 1.0  -0.997503","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# remove simulated data once they are no longer needed\nrm(\"example.bed\", force=true)\nrm(\"example.bim\", force=true)\nrm(\"example.fam\", force=true)\nrm(\"example_nongenetic_covariates.txt\", force=true)","category":"page"},{"location":"man/examples/#Example-2:-Running-IHT-on-Quantitative-Traits","page":"Examples","title":"Example 2: Running IHT on Quantitative Traits","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Quantitative traits are continuous phenotypes whose distribution can be modeled by the normal distribution. Then using the genotype matrix mathbfX and phenotype vector mathbfy, we want to recover beta such that mathbfy approx mathbfXbeta. In this example, we assume we know the true sparsity level k. ","category":"page"},{"location":"man/examples/#Step-1:-Import-data","page":"Examples","title":"Step 1: Import data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In Example 1 we illustrated how to import data into Julia. So here we use simulated data (code) because, only then, can we compare IHT's result to the true solution. Below we simulate a GWAS data with n=1000 patients and p=10000 SNPs. Here the quantitative trait vector are affected by k = 10 causal SNPs, with no non-genetic confounders. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, our model is simulated as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim mathbfx_i^Tmathbfbeta + epsilon_i","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"epsilon_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Define model dimensions, true model size, distribution, and link functions\nn = 1000\np = 10000\nk = 10\nd = Normal\nl = canonicallink(d())\n\n# set random seed for reproducibility\nRandom.seed!(2019) \n\n# simulate SNP matrix, store the result in a file called tmp.bed\nx = simulate_random_snparray(n, p, \"tmp.bed\")\n\n#construct the SnpBitMatrix type (needed for L0_reg() and simulate_random_response() below)\nxbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n\n# intercept is the only nongenetic covariate\nz = ones(n, 1) \n\n# simulate response y, true model b, and the correct non-0 positions of b\ny, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);","category":"page"},{"location":"man/examples/#Step-2:-Run-cross-validation-to-determine-best-model-size","page":"Examples","title":"Step 2: Run cross validation to determine best model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To run cv_iht, you must specify path and num_fold, defined below:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"path are all the model sizes you wish to test, stored in a vector of integers.\nnum_fold indicates how many disjoint partitions of the samples is requested. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"By default, we partition the training/testing data randomly, but you can change this by inputing the fold vector as optional argument. In this example we tested k = 1 2  20 across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by parallel=true). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"path = collect(1:20)\nnum_folds = 3\nmses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Crossvalidation Results:\n\tk\tMSE\n\t1\t1927.0765190526674\n\t2\t1443.8788742220863\n\t3\t1080.041135323195\n\t4\t862.2385953735204\n\t5\t705.1014346627649\n\t6\t507.3949359364219\n\t7\t391.96868764622843\n\t8\t368.45440222003174\n\t9\t350.642794092518\n\t10\t345.8380848576577\n\t11\t350.5188147284578\n\t12\t359.42391568519577\n\t13\t363.70956969599075\n\t14\t377.30732985896975\n\t15\t381.0310879522694\n\t16\t392.56439238382615\n\t17\t396.81166049333797\n\t18\t397.3010019298764\n\t19\t406.47023764639624\n\t20\t410.4672260807978","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"note: Note\nDO NOT remove intermediate files with random filenames as generated by cv_iht(). These are necessary auxiliary files that will be automatically removed when cross validation completes. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"tip: Tip\nBecause Julia employs a JIT compiler, the first round of any function call run will always take longer and consume extra memory. Therefore it is advised to always run a small \"test example\" (such as this one!) before running cross validation on a large dataset. ","category":"page"},{"location":"man/examples/#Step-3:-Run-full-model-on-the-best-estimated-model-size","page":"Examples","title":"Step 3: Run full model on the best estimated model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"cv_iht finished in less than a minute. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"According to our cross validation result, the best model size that minimizes deviance residuals (i.e. MSE on the q-th subset of samples) is attained at k = 10. That is, cross validation detected that we need 10 SNPs to achieve the best model size. Using this information, one can re-run the IHT algorithm on the full dataset to obtain the best estimated model.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"k_est = argmin(mses)\nresult = L0_reg(x, xbm, z, y, 1, k_est, d(), l) ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 10 nonzero SNP predictors and 0 non-genetic predictors.\n\nCompute time (sec):     0.4135408401489258\nFinal loglikelihood:    -1406.8807653835697\nIterations:             6\n\nSelected genetic predictors:\n10×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 853      │ -1.24117    │\n│ 2   │ 877      │ -0.234676   │\n│ 3   │ 924      │ 0.82014     │\n│ 4   │ 2703     │ 0.583403    │\n│ 5   │ 4241     │ 0.298304    │\n│ 6   │ 4783     │ -1.14459    │\n│ 7   │ 5094     │ 0.673012    │\n│ 8   │ 5284     │ -0.709736   │\n│ 9   │ 7760     │ 0.16866     │\n│ 10  │ 8255     │ 1.08117     │\n\nSelected nongenetic predictors:\n0×2 DataFrame","category":"page"},{"location":"man/examples/#Step-4-(only-for-simulated-data):-Check-final-model-against-simulation","page":"Examples","title":"Step 4 (only for simulated data): Check final model against simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since all our data and model was simulated, we can see how well cv_iht combined with L0_reg estimated the true model. For this example, we find that IHT found every simulated predictor, with 0 false positives. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = DataFrame(\n    true_β      = true_b[correct_position], \n    estimated_β = result.beta[correct_position])\n@show compare_model\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×2 DataFrame\n│ Row │ true_β   │ estimated_β │\n│     │ Float64  │ Float64     │\n├─────┼──────────┼─────────────┤\n│ 1   │ -1.29964 │ -1.24117    │\n│ 2   │ -0.2177  │ -0.234676   │\n│ 3   │ 0.786217 │ 0.82014     │\n│ 4   │ 0.599233 │ 0.583403    │\n│ 5   │ 0.283711 │ 0.298304    │\n│ 6   │ -1.12537 │ -1.14459    │\n│ 7   │ 0.693374 │ 0.673012    │\n│ 8   │ -0.67709 │ -0.709736   │\n│ 9   │ 0.14727  │ 0.16866     │\n│ 10  │ 1.03477  │ 1.08117     │","category":"page"},{"location":"man/examples/#Example-3:-Logistic-Regression-Controlling-for-Sex","page":"Examples","title":"Example 3: Logistic Regression Controlling for Sex","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We show how to use IHT to handle case-control studies, while handling non-genetic covariates. In this example, we fit a logistic regression model with IHT using simulated case-control data, while controling for sex as a nongenetic covariate. ","category":"page"},{"location":"man/examples/#Step-1:-Import-Data","page":"Examples","title":"Step 1: Import Data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Again we use a simulated model:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Bernoulli(mathbfx_i^Tmathbfbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm intercept = 1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm sex = 15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We assumed there are k=8 genetic predictors and 2 non-genetic predictors (intercept and sex) that affects the trait. The simulation code in our package does not yet handle simulations with non-genetic predictors, so we must simulate these phenotypes manually. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Define model dimensions, true model size, distribution, and link functions\nn = 1000\np = 10000\nk = 10\nd = Bernoulli\nl = canonicallink(d())\n\n# set random seed for reproducibility\nRandom.seed!(2019)\n\n# construct SnpArray and SnpBitMatrix\nx = simulate_random_snparray(n, p, \"tmp.bed\")\nxbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n\n# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\nz = ones(n, 2) \nz[:, 2] .= rand(0:1, n)\n\n# randomly set genetic predictors\ntrue_b = zeros(p) \ntrue_b[1:k-2] = randn(k-2)\nshuffle!(true_b)\n\n# find correct position of genetic predictors\ncorrect_position = findall(!iszero, true_b)\n\n# define effect size of non-genetic predictors: intercept & sex\ntrue_c = [1.0; 1.5] \n\n# simulate phenotype using genetic and nongenetic predictors\nprob = GLM.linkinv.(l, xbm * true_b .+ z * true_c)\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # y must be floating point numbers","category":"page"},{"location":"man/examples/#Step-2:-Run-cross-validation-to-determine-best-model-size-2","page":"Examples","title":"Step 2: Run cross validation to determine best model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To run cv_iht, you must specify path and num_fold, defined below:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"path are all the model sizes you wish to test, stored in a vector of integers.\nnum_fold indicates how many disjoint partitions of the samples is requested. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"By default, we partition the training/testing data randomly, but you can change this by inputing the fold vector as optional argument. In this example we tested k = 1 2  20 across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by parallel=true). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"path = collect(1:20)\nnum_folds = 3\nmses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Crossvalidation Results:\n\tk\tMSE\n\t1\t391.44426892225727\n\t2\t365.7520496496987\n\t3\t332.3801926093215\n\t4\t273.2081512206048\n\t5\t253.31631985207758\n\t6\t234.93701288953315\n\t7\t221.997334181244\n\t8\t199.01688783420346\n\t9\t208.31087723164197\n\t10\t216.00575628120708\n\t11\t227.91834469184545\n\t12\t242.42456871743065\n\t13\t261.46346209331466\n\t14\t263.5229307137862\n\t15\t283.30379378951073\n\t16\t328.2771991160804\n\t17\t290.4512419547228\n\t18\t350.3516280657363\n\t19\t361.61016297810295\n\t20\t418.2370935226805","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"tip: Tip\nIn our experience, using the ProbitLink for logistic regressions deliver better results than LogitLink (which is the canonical link). But of course, one should choose the link that gives the higher loglikelihood. ","category":"page"},{"location":"man/examples/#Step-3:-Run-full-model-on-the-best-estimated-model-size-2","page":"Examples","title":"Step 3: Run full model on the best estimated model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"cv_iht finished in about a minute. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Cross validation have declared that k_best = 8. Using this information, one can re-run the IHT algorithm on the full dataset to obtain the best estimated model.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"k_est = argmin(mses)\nresult = L0_reg(x, xbm, z, y, 1, k_est, d(), l)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 6 nonzero SNP predictors and 2 non-genetic predictors.\n\nCompute time (sec):     1.6644580364227295\nFinal loglikelihood:    -290.4509381564733\nIterations:             37\n\nSelected genetic predictors:\n6×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 1152     │ 0.966731    │\n│ 2   │ 1576     │ 1.56183     │\n│ 3   │ 3411     │ 0.87674     │\n│ 4   │ 5765     │ -1.75611    │\n│ 5   │ 5992     │ -2.04506    │\n│ 6   │ 8781     │ 0.760213    │\n\nSelected nongenetic predictors:\n2×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 1        │ 0.709909    │\n│ 2   │ 2        │ 1.65049     │","category":"page"},{"location":"man/examples/#Step-4-(only-for-simulated-data):-Check-final-model-against-simulation-2","page":"Examples","title":"Step 4 (only for simulated data): Check final model against simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since all our data and model was simulated, we can see how well cv_iht combined with L0_reg estimated the true model. For this example, we find that IHT found both nongenetic predictor, but missed 2 genetic predictors. The 2 genetic predictors that we missed had much smaller effect size, so given that we only had 1000 samples, this is hardly surprising. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model_genetics = DataFrame(\n    true_β      = true_b[correct_position], \n    estimated_β = result.beta[correct_position])\n\ncompare_model_nongenetics = DataFrame(\n    true_c      = true_c[1:2], \n    estimated_c = result.c[1:2])\n\n@show compare_model_genetics\n@show compare_model_nongenetics\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model_genetics = 8×2 DataFrame\n│ Row │ true_β   │ estimated_β │\n│     │ Float64  │ Float64     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 0.961937 │ 0.966731    │\n│ 2   │ 0.189267 │ 0.0         │\n│ 3   │ 1.74008  │ 1.56183     │\n│ 4   │ 0.879004 │ 0.87674     │\n│ 5   │ 0.213066 │ 0.0         │\n│ 6   │ -1.74663 │ -1.75611    │\n│ 7   │ -1.93402 │ -2.04506    │\n│ 8   │ 0.632786 │ 0.760213    │\ncompare_model_nongenetics = 2×2 DataFrame\n│ Row │ true_c  │ estimated_c │\n│     │ Float64 │ Float64     │\n├─────┼─────────┼─────────────┤\n│ 1   │ 1.0     │ 0.709909    │\n│ 2   │ 1.5     │ 1.65049     │","category":"page"},{"location":"man/examples/#Example-4:-Poisson-Regression-with-Acceleration-(i.e.-debias)","page":"Examples","title":"Example 4: Poisson Regression with Acceleration (i.e. debias)","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how debiasing can potentially achieve dramatic speedup. Our model is:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Poisson(mathbfx_i^T mathbfbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 03)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Define model dimensions, true model size, distribution, and link functions\nn = 1000\np = 10000\nk = 10\nd = Poisson\nl = canonicallink(d())\n\n# set random seed for reproducibility\nRandom.seed!(2019)\n\n# construct SnpArray, SnpBitMatrix, and intercept\nx = simulate_random_snparray(n, p, \"tmp.bed\")\nxbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\nz = ones(n, 1) \n\n# simulate response, true model b, and the correct non-0 positions of b\ny, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);","category":"page"},{"location":"man/examples/#First-Compare-Reconstruction-Result","page":"Examples","title":"First Compare Reconstruction Result","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we show that, with or without debiasing, we obtain comparable results with L0_reg.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"no_debias  = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false)\nyes_debias = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = DataFrame(\n    position    = correct_position,\n    true_β      = true_b[correct_position], \n    no_debias_β = no_debias.beta[correct_position],\n    yes_debias_β = yes_debias.beta[correct_position])\n@show compare_model;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×4 DataFrame\n│ Row │ position │ true_β     │ no_debias_β │ yes_debias_β │\n│     │ Int64    │ Float64    │ Float64     │ Float64      │\n├─────┼──────────┼────────────┼─────────────┼──────────────┤\n│ 1   │ 853      │ -0.389892  │ -0.384161   │ -0.38744     │\n│ 2   │ 877      │ -0.0653099 │ 0.0         │ 0.0          │\n│ 3   │ 924      │ 0.235865   │ 0.246213    │ 0.240514     │\n│ 4   │ 2703     │ 0.17977    │ 0.237651    │ 0.225127     │\n│ 5   │ 4241     │ 0.0851134  │ 0.0         │ 0.0894244    │\n│ 6   │ 4783     │ -0.33761   │ -0.300663   │ -0.307515    │\n│ 7   │ 5094     │ 0.208012   │ 0.223384    │ 0.215149     │\n│ 8   │ 5284     │ -0.203127  │ -0.225593   │ -0.209308    │\n│ 9   │ 7760     │ 0.0441809  │ 0.0         │ 0.0          │\n│ 10  │ 8255     │ 0.310431   │ 0.287363    │ 0.301717     │","category":"page"},{"location":"man/examples/#Compare-Speed-and-Memory-Usage","page":"Examples","title":"Compare Speed and Memory Usage","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Now we illustrate that debiasing may dramatically reduce computational time (in this case ~50%), at a cost of increasing the memory usage. In practice, this extra memory usage hardly matters because the matrix size will dominate for larger problems. See our paper for complete benchmark figure.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false) seconds=15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"BenchmarkTools.Trial: \n  memory estimate:  2.16 MiB\n  allocs estimate:  738\n  --------------\n  minimum time:     673.188 ms (0.00% GC)\n  median time:      706.368 ms (0.00% GC)\n  mean time:        708.080 ms (0.04% GC)\n  maximum time:     741.125 ms (0.00% GC)\n  --------------\n  samples:          22\n  evals/sample:     1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true) seconds=15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"BenchmarkTools.Trial: \n  memory estimate:  2.62 MiB\n  allocs estimate:  1135\n  --------------\n  minimum time:     337.368 ms (0.00% GC)\n  median time:      350.194 ms (0.00% GC)\n  mean time:        349.958 ms (0.10% GC)\n  maximum time:     358.095 ms (0.00% GC)\n  --------------\n  samples:          43\n  evals/sample:     1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/#Example-5:-Negative-Binomial-regression-with-group-information","page":"Examples","title":"Example 5: Negative Binomial regression with group information","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most J = 5 groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter k is known. ","category":"page"},{"location":"man/examples/#Data-simulation","page":"Examples","title":"Data simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in our paper. In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; j = 5 distinct groups are each assigned 125 causal SNPs with effect sizes randomly chosen from 0202. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity lambda_g = 125. The remaining groups are assigned lambda_g = 1 (i.e. only 1 active predictor are allowed).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# define problem size\nd = NegativeBinomial\nl = LogLink()\nn = 1000\np = 10000\nblock_size = 20                  #simulation parameter\nnum_blocks = Int(p / block_size) #simulation parameter\n\n# set seed\nRandom.seed!(2019)\n\n# assign group membership\nmembership = collect(1:num_blocks)\ng = zeros(Int64, p + 1)\nfor i in 1:length(membership)\n    for j in 1:block_size\n        cur_row = block_size * (i - 1) + j\n        g[block_size*(i - 1) + j] = membership[i]\n    end\nend\ng[end] = membership[end]\n\n#simulate correlated snparray\nx = simulate_correlated_snparray(n, p, \"tmp.bed\")\nz = ones(n, 1) # the intercept\nx_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n\n#simulate true model, where 5 groups each with 1~5 snps contribute\ntrue_b = zeros(p)\ntrue_groups = randperm(num_blocks)[1:5]\nsort!(true_groups)\nwithin_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n                randperm(block_size)[1:3], randperm(block_size)[1:4], \n                randperm(block_size)[1:5]]\ncorrect_position = zeros(Int64, 15)\nfor i in 1:5\n    cur_group = block_size * (true_groups[i] - 1)\n    cur_group_snps = cur_group .+ within_group[i]\n    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n    correct_position[start:last] .= cur_group_snps\nend\nfor i in 1:15\n    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\nend\nsort!(correct_position)\n\n# simulate phenotype\nr = 10 #nuisance parameter\nμ = GLM.linkinv.(l, x_float * true_b)\nclamp!(μ, -20, 20)\nprob = 1 ./ (1 .+ μ ./ r)\ny = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT without groups\nk = 15\nungrouped = L0_reg(x_float, z, y, 1, k, d(), l, verbose=false)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n\nCompute time (sec):     0.11840415000915527\nFinal loglikelihood:    -1441.522293255591\nIterations:             27\n\nSelected genetic predictors:\n15×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 3464     │ -0.234958   │\n│ 2   │ 4383     │ -0.135693   │\n│ 3   │ 4927     │ 0.158171    │\n│ 4   │ 4938     │ -0.222613   │\n│ 5   │ 5001     │ -0.193739   │\n│ 6   │ 5011     │ -0.162718   │\n│ 7   │ 5018     │ -0.190532   │\n│ 8   │ 5090     │ 0.226509    │\n│ 9   │ 5092     │ -0.17756    │\n│ 10  │ 5100     │ -0.140337   │\n│ 11  │ 7004     │ 0.151748    │\n│ 12  │ 7011     │ 0.206449    │\n│ 13  │ 7015     │ -0.284706   │\n│ 14  │ 7016     │ 0.218126    │\n│ 15  │ 9902     │ 0.119059    │\n\nSelected nongenetic predictors:\n0×2 DataFrame","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\nJ = 5\nmax_group_snps = ones(Int, num_blocks)\nmax_group_snps[true_groups] .= collect(1:5)\nvariable_group = L0_reg(x_float, z, y, J, max_group_snps, d(), l, verbose=false, group=g)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n\nCompute time (sec):     0.30719614028930664\nFinal loglikelihood:    -1446.3808810786898\nIterations:             16\n\nSelected genetic predictors:\n15×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 3464     │ -0.245853   │\n│ 2   │ 4927     │ 0.160904    │\n│ 3   │ 4938     │ -0.213439   │\n│ 4   │ 5001     │ -0.19624    │\n│ 5   │ 5011     │ -0.149913   │\n│ 6   │ 5018     │ -0.181966   │\n│ 7   │ 5086     │ -0.0560478  │\n│ 8   │ 5090     │ 0.21164     │\n│ 9   │ 5092     │ -0.141968   │\n│ 10  │ 5100     │ -0.157655   │\n│ 11  │ 7004     │ 0.190224    │\n│ 12  │ 7011     │ 0.21294     │\n│ 13  │ 7015     │ -0.256058   │\n│ 14  │ 7016     │ 0.19746     │\n│ 15  │ 7020     │ 0.111755    │\n\nSelected nongenetic predictors:\n0×2 DataFrame","category":"page"},{"location":"man/examples/#Group-IHT-found-1-more-SNPs-than-ungrouped-IHT","page":"Examples","title":"Group IHT found 1 more SNPs than ungrouped IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#check result\ncorrect_position = findall(!iszero, true_b)\ncompare_model = DataFrame(\n    position = correct_position,\n    correct_β = true_b[correct_position],\n    ungrouped_IHT_β = ungrouped.beta[correct_position], \n    grouped_IHT_β = variable_group.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 15×4 DataFrame\n│ Row │ position │ correct_β │ ungrouped_IHT_β │ grouped_IHT_β │\n│     │ Int64    │ Float64   │ Float64         │ Float64       │\n├─────┼──────────┼───────────┼─────────────────┼───────────────┤\n│ 1   │ 3464     │ -0.2      │ -0.234958       │ -0.245853     │\n│ 2   │ 4927     │ 0.2       │ 0.158171        │ 0.160904      │\n│ 3   │ 4938     │ -0.2      │ -0.222613       │ -0.213439     │\n│ 4   │ 5001     │ -0.2      │ -0.193739       │ -0.19624      │\n│ 5   │ 5011     │ -0.2      │ -0.162718       │ -0.149913     │\n│ 6   │ 5018     │ -0.2      │ -0.190532       │ -0.181966     │\n│ 7   │ 5084     │ -0.2      │ 0.0             │ 0.0           │\n│ 8   │ 5090     │ 0.2       │ 0.226509        │ 0.21164       │\n│ 9   │ 5098     │ -0.2      │ 0.0             │ 0.0           │\n│ 10  │ 5100     │ -0.2      │ -0.140337       │ -0.157655     │\n│ 11  │ 7004     │ 0.2       │ 0.151748        │ 0.190224      │\n│ 12  │ 7011     │ 0.2       │ 0.206449        │ 0.21294       │\n│ 13  │ 7015     │ -0.2      │ -0.284706       │ -0.256058     │\n│ 14  │ 7016     │ 0.2       │ 0.218126        │ 0.19746       │\n│ 15  │ 7020     │ 0.2       │ 0.0             │ 0.111755      │","category":"page"},{"location":"man/examples/#Example-6:-Linear-Regression-with-prior-weights","page":"Examples","title":"Example 6: Linear Regression with prior weights","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include (predetermined) prior weights for each SNP. You can check out our paper for references of why/how to choose these weights. In this case, we mimic our paper and randomly set 10 of all SNPs to have a weight of 20. Other predictors have weight of 10. All causal SNPs have weights of 20. Under this scenario, SNPs with weight 20 is twice as likely to enter the model identified by IHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Our model is simulated as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim mathbfx_i^Tmathbfbeta + epsilon_i","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"epsilon_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#random seed\nRandom.seed!(4)\n\nd = Normal\nl = canonicallink(d())\nn = 1000\np = 10000\nk = 10\n\n# construct snpmatrix, covariate files, and true model b\nx = simulate_random_snparray(n, p, \"tmp.bed\")\nX = convert(Matrix{Float64}, x, center=true, scale=true)\nz = ones(n, 1) # the intercept\n    \n#define true_b \ntrue_b = zeros(p)\ntrue_b[1:10] .= collect(0.1:0.1:1.0)\nshuffle!(true_b)\ncorrect_position = findall(!iszero, true_b)\n\n#simulate phenotypes (e.g. vector y)\nprob = GLM.linkinv.(l, X * true_b)\nclamp!(prob, -20, 20)\ny = [rand(d(i)) for i in prob]\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# construct weight vector\nw = ones(p + 1)\nw[correct_position] .= 2.0\none_tenth = round(Int, p/10)\nidx = rand(1:p, one_tenth)\nw[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT\nunweighted = L0_reg(X, z, y, 1, k, d(), l, verbose=false)\nweighted   = L0_reg(X, z, y, 1, k, d(), l, verbose=false, weight=w)\n\n#check result\ncompare_model = DataFrame(\n    position    = correct_position,\n    correct     = true_b[correct_position],\n    unweighted  = unweighted.beta[correct_position], \n    weighted    = weighted.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×4 DataFrame\n│ Row │ position │ correct │ unweighted │ weighted │\n│     │ Int64    │ Float64 │ Float64    │ Float64  │\n├─────┼──────────┼─────────┼────────────┼──────────┤\n│ 1   │ 1254     │ 0.4     │ 0.452245   │ 0.450405 │\n│ 2   │ 1495     │ 0.3     │ 0.306081   │ 0.305738 │\n│ 3   │ 4856     │ 0.8     │ 0.853536   │ 0.862223 │\n│ 4   │ 5767     │ 0.1     │ 0.0        │ 0.117286 │\n│ 5   │ 5822     │ 0.7     │ 0.656213   │ 0.651908 │\n│ 6   │ 5945     │ 0.9     │ 0.891915   │ 0.894997 │\n│ 7   │ 6367     │ 0.5     │ 0.469718   │ 0.472524 │\n│ 8   │ 6996     │ 1.0     │ 0.963236   │ 0.973512 │\n│ 9   │ 7052     │ 0.6     │ 0.602162   │ 0.600055 │\n│ 10  │ 7980     │ 0.2     │ 0.231389   │ 0.234094 │","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this case, weighted IHT found an extra predictor than non-weighted IHT.","category":"page"},{"location":"man/examples/#Other-examples-and-functionalities","page":"Examples","title":"Other examples and functionalities","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We explored a few more examples in our manuscript, with reproducible code. We invite users to experiment with them as well. ","category":"page"},{"location":"#Mendel-Iterative-Hard-Thresholding","page":"Home","title":"Mendel - Iterative Hard Thresholding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A modern approach to analyze data from a Genome Wide Association Studies (GWAS)","category":"page"},{"location":"#Package-Feature","page":"Home","title":"Package Feature","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Analyze large GWAS datasets intuitively.\nBuilt-in support for PLINK binary files via SnpArrays.jl and VCF files via VCFTools.jl.\nOut-of-the-box parallel computing routines for q-fold cross-validation.\nFits a variety of generalized linear models with any choice of link function.\nComputation directly on raw genotype files.\nEfficient handlings for non-genetic covariates.\nOptional acceleration (debias) step to dramatically improve speed.\nAbility to explicitly incorporate weights for predictors.\nAbility to enforce within and between group sparsity. \nNaive genotype imputation. \nEstimates nuisance parameter for negative binomial regression using Newton or MM algorithm. \nExcellent flexibility to handle different data structures and complements well with other Julia packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read our paper for more detail.","category":"page"},{"location":"#Supported-GLM-models-and-Link-functions","page":"Home","title":"Supported GLM models and Link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MendelIHT borrows distribution and link functions implementationed in GLM.jl and Distributions.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distribution Canonical Link Status\nNormal IdentityLink checkmark\nBernoulli LogitLink checkmark\nPoisson LogLink checkmark\nNegativeBinomial LogLink checkmark\nGamma InverseLink experimental\nInverseGaussian InverseSquareLink experimental","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples of these distributions in their default value is visualized in this post.","category":"page"},{"location":"#Available-link-functions","page":"Home","title":"Available link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nProbitLink\nSqrtLink","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/getting_started.md\",\n    \"man/examples.md\",\n    \"man/math.md\",\n    \"man/contributing.md\",\n]\nDepth = 2","category":"page"}]
}

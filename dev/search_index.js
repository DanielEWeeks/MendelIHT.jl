var documenterSearchIndex = {"docs":
[{"location":"man/math/#Details-of-Parameter-Estimation","page":"Mathematical Details","title":"Details of Parameter Estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"This note is meant to supplement our paper. For a review on generalized linear models, I recommend chapter 15.3 of Applied regression analysis and generalized linear models by John Fox, or chapter 3-5 of An introduction to generalized linear models by Dobson and Barnett. ","category":"page"},{"location":"man/math/#Generalized-linear-models","page":"Mathematical Details","title":"Generalized linear models","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, phenotypes (bf y) are modeled as a generalized linear model:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    mu_i = E(y_i) = g(bf x_i^t boldsymbol beta)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where bf x is sample i's p-dimensional vector of covariates (genotypes + other fixed effects), boldsymbol beta is a p-dimensional regression coefficients, g is a non-linear inverse-link function, y_i is sample i's phenotype value, and mu_i is the average predicted value of y_i given bf x. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The regression coefficients boldsymbol beta are not observed and are estimated via maximum likelihood. The full design matrix bf X (obtained by stacking each bf x_i^t row-by-row) and phenotypes bf y are observed. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases under the GLM framework. Of course, when g(alpha) = alpha we get the standard linear model used for Gaussian phenotypes. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Traditionally, boldsymbol beta is estimated via iteratively reweighted least squares. For high dimensional problems where number of samples n is smaller than number of covariates p, this regime breaks down. We propose iterative hard threshodling as a solution, but obviously many other possibilities exist (e.g. lasso). ","category":"page"},{"location":"man/math/#Implementation-details-of-loglikelihood,-gradient,-and-expected-information","page":"Mathematical Details","title":"Implementation details of loglikelihood, gradient, and expected information","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In GLM, the distribution of bf y is from the exponential family with density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    f(y mid theta phi) = exp left fracy theta - b(theta)a(phi) + c(y phi) right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"theta","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"is called the canonical (location) parameter and under the canonical link, theta = g(bf x^t bf beta). phi is the dispersion (scale) parameter. The functions a b c are known functions that vary depending on the distribution of y. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Given n independent observations, the loglikelihood is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    L(bf theta phi bf y) = sum_i=1^n fracy_itheta_i - b(theta_i)a_i(phi) + c(y_i phi)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate the loglikelihood, we use the logpdf function in Distributions.jl.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The perform maximum likelihood estimation, we compute partial derivatives for betas. The jth score component is (eq 4.18 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    fracpartial Lpartial beta_j = sum_i=1^n leftfracy_i - mu_ivar(y_i)x_ijleft(fracpartial mu_ipartial eta_iright)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus the full gradient is","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla L= bf X^tbf W(bf y - boldsymbolmu) quad bf W_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"and similarly, the expected information is (eq 4.23 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    J = bf X^ttildeWX quad bf tildeW_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate nabla L and J, note bf y and bf X are known, so we just need to calculate boldsymbolmu fracpartialmu_ipartialeta_i and var(y_i). The first simply uses the inverse link: mu_i = g(bf x_i^t boldsymbol beta). For the second, note fracpartial mu_ipartialeta_i = fracpartial g(bf x_i^t boldsymbol beta)partialbf x_i^t boldsymbol beta is just the derivative of the link function evaluated at the linear predictor eta_i = bf x_i^t boldsymbol beta. This is already implemented for various link functions as mueta in GLM.jl, which we call internally. To compute var(y_i), we note that the exponential family distributions have variance","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    var(y) = a(phi)b(theta) = a(phi)fracpartial^2b(theta)partialtheta = a(phi) var(mu)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"That is, var(y_i) is a product of 2 terms where the first depends solely on phi, and the second solely on mu = g(bf x_i^t boldsymbol beta). In our code, we use glmvar implemented in GLM.jl to calculate var(mu). Because phi is unknown, we assume a(phi) = 1 for all models except the negative binomial model. For negative binomial model, we discuss how to estimate phi and boldsymbolbeta using alternate block descent below.  ","category":"page"},{"location":"man/math/#Iterative-hard-thresholding","page":"Mathematical Details","title":"Iterative hard thresholding","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, the loglikelihood is maximized using iterative hard thresholding. This is achieved by","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    boldsymbolbeta_n+1 = overbraceP_S_k^(3)big(boldsymbolbeta_n - underbraces_n_(2) overbracenabla f(boldsymbolbeta_n)^(1)big)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where f is the function to minimize (i.e. negative loglikelihood), s_k is the step size, and P_S_k is a projection operator that sets all but k largest entries in magnitude to 0. I already discussed above how to compute the gradient of a GLM loglikelihood. To perform P_S_k, we first partially sort the dense vector beta_n - s_n nabla f(beta_n), and set the smallest k+1  n entries in magnitude to 0. Finally, the step size s_n is derived in our paper to be","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    s_n = fracnabla f(boldsymbolbeta_n)_2^2nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where J = bf X^ttildeWX is the expected information matrix (derived in the previous section) which should never be explicitly formed. To evaluate the denominator, observe that ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n) = left(nabla f(boldsymbolbeta_n)^tbf X^t sqrt(tildeW)right)left(sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n)right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus one computes bf v = sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n) and calculate its inner product with itself. ","category":"page"},{"location":"man/math/#Nuisance-parameter-estimation","page":"Mathematical Details","title":"Nuisance parameter estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Currently MendelIHT.jl only estimates nuisance parameter for the Negative Binomial model. This feature is provided by our 2019 Bruins in Genomics summer student Vivian Garcia and Francis Adusei. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note for Gaussian response, one can use the sample variance formula to estimate phi from the estimated mean hatmu. ","category":"page"},{"location":"man/math/#Parametrization-for-Negative-Binomial-model","page":"Mathematical Details","title":"Parametrization for Negative Binomial model","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The negative binomial distribution has density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tP(Y = y) = binomy+r-1yp^r(1-p)^y\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where y is the number of failures before the rth success and p is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to WOLFRAM is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tmu_i = fracr(1-p_i)r quad\n\tVar(y_i) = fracr(1-p_i)p_i^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note these formula are different than the default on wikipedia because in wiki y is the number of success and r is the number of failure.  Therefore, solving for p_i, we have ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tp_i = fracrmu_i + r = fracre^mathbfx_i^Tbeta + r in (0 1)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"And indeed this this is how we parametrize the negative binomial model. Importantly, we can interpret p_i as a probability, since mathbfx_i^Tbeta can take on any number between -infty and +infty (since beta and mathbfx_i can have positive and negative entries), so exp(mathbfx_i^Tbeta)in(0 infty).","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"We can also try to express Var(y_i) in terms of mu_i and r by doing some algebra:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tVar(y_i)\n\t= fracr(1-p_i)p_i^2 = fracrleft( 1 - fracrmu_i + r right)fracr^2(mu_i + r)^2 = frac1rleft(1 - fracrmu_i + rright)(mu_i + r)^2 \n\t= frac1r left (mu_i + r)^2 - r(mu_r + r) right = frac1r(mu_i + r)mu_i\n\t= mu_i left( fracmu_ir + 1 right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"You can verify in GLM.jl that this is indeed how they compute the variance of a negative binomial distribution. ","category":"page"},{"location":"man/math/#Estimating-nuisance-parameter-using-MM-algorithms","page":"Mathematical Details","title":"Estimating nuisance parameter using MM algorithms","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The MM algorithm is very stable, but converges much slower than Newton's alogorithm below. Thus use MM only if Newton's method fails.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The loglikelihood for n independent samples under a Negative Binomial model is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tL(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left sum_j=0^y_i - 1 ln(r+j) + rln(p_i) - ln(y_i) + y_iln(1-p_i) right\n\tgeq sum_i=1^mleft sum_j=0^y_i-1fracr_nr_n+jln(r) + c_n + rln(p_i) - ln(y_i) + y_i ln(1-p_i) right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The last inequality can be seen by applying Jensen's inequality:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfleft sum_iu_i(boldsymboltheta)right leq sum_i fracu_i(boldsymboltheta_nsum_j u_j(boldsymboltheta_n)f left fracsum_j u_j(boldsymboltheta_n)u_i(boldsymboltheta_n) u_i(boldsymboltheta)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"to the function f(u) = - ln(u) Maximizing L over r (i.e. differentiating with respect to r and setting equal to zero, then solving for r), we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfracddr L(p_1p_mr) \n\t= sum_i=1^m left sum_j=0^y_i-1 fracr_nr_n + j frac1r + ln(p_i) right \n\t= sum_i=1^msum_j=0^y_i-1 fracr_nr_n + j frac1r + sum_i=1^mln(p_i) \n\tequiv 0\n\tiff r_n+1 = frac-sum_i=1^msum_j=0^y_i-1 fracr_nr_n + jsum_i=1^mln(p_i)  \nendaligned","category":"page"},{"location":"man/math/#Estimating-Nuisance-parameter-using-Newton's-method","page":"Mathematical Details","title":"Estimating Nuisance parameter using Newton's method","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since we are dealing with 1 parameter optimization, Newton's method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (r), we use maximum likelihood estimates. By p_i = r  (mu_i + r) in above, we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\t L(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left lnleft((y_i+r-1)right) - lnleft(y_iright) - lnleft((r-1)right) + rln(r) - rln(mu_i+r) + y_iln(mu_i) + y_iln(mu_i + r)right\n\t= sum_i=1^mleftlnleft((y_i+r-1)right)-ln(y_i) - lnleft((r-1)right) + rln(r) - (r+y_i)ln(mu_i + r) + y_iln(mu_i)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Recalling the definition of digamma and trigamma functions, the first and second derivative of L with respect to r is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfracddr L(p_1  p_m r) =  sum_i=1^m left operatornamedigamma(y_i+r) - operatornamedigamma(r) + 1 + ln(r) - fracr+y_imu_i+r - ln(mu_i + r) right\n\tfracd^2dr^2 L(p_1  p_m r) =sum_i=1^m left operatornametrigamma(y_i+r) - operatornametrigamma(r) + frac1r - frac2mu_i + r + fracr+y_i(mu_i + r)^2 right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"So the iteration to use is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tr_n+1 = r_n - fracfracddrL(p_1p_mr)fracd^2dr^2L(p_1p_mr)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"You can verify that this is the same as the first and second derivative formula in GLM.jl (they used theta in place of r). The sign difference is because in GLM.jl, they are minimizing the negative loglikelihood instead of maximizing the loglikelihood. They are equivalent, but in mathematical optimization the standard form is to minimize an objective function. ","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"I am one developer. We are a community. ","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you would like to contribute to this project, we compiled a list of desired features for this project. Developers of any level is welcomed. Do not be shy because it can't hurt to ask. ","category":"page"},{"location":"man/contributing/#Bug-Fixes-and-User-Support","page":"Contributing","title":"Bug Fixes & User Support","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you encounter a bug or you need some user support, please open a new issue here. If you can, provide the error message and, ideally, a reproducible code that generated the error.","category":"page"},{"location":"man/contributing/#Citation","page":"Contributing","title":"Citation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you use MendelIHT.jl in an academic manuscript, please cite:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Benjamin B. Chu, Kevin L. Keys, Christopher A. German, Hua Zhou, Jin J. Zhou, Janet S. Sinsheimer, Kenneth Lange. Iterative Hard Thresholding in GWAS: Generalized Linear Models, Prior Weights, and Double Sparsity. bioRxiv doi:10.1101/697755","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Bibtex:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"@article{zhou2019openmendel,\n  title={{Iterative Hard Thresholding in GWAS: Generalized Linear Models, Prior Weights, and Double Sparsity}},\n  author={Chu, Benjamin B and Keys, Kevin L and German, Christopher A and Zhou, Hua and Zhou, Jin J and Sinsheimer, Janet S and Lange, Kenneth},\n  journal={BioRxiv},\n  pages={697755v2},\n  year={2019},\n  publisher={Cold Spring Harbor Laboratory}\n}","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you could also press star on the upper right hand corner on our github page, that would be very helpful. ","category":"page"},{"location":"man/getting_started/#Getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"In this section, we outline the basic procedure to analyze your GWAS data with MendelIHT. ","category":"page"},{"location":"man/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Download and install Julia. Within Julia, copy and paste the following:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/SnpArrays.jl.git\"))\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/VCFTools.jl.git\"))\nPkg.add(PackageSpec(url=\"https://github.com/OpenMendel/MendelIHT.jl.git\"))","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT.jl supports Julia 1.5+ for Mac, Linux, and window machines. A few features are disabled for windows users, and users will be warned when trying to use them.","category":"page"},{"location":"man/getting_started/#Typical-Workflow","page":"Getting Started","title":"Typical Workflow","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Run cross_validate() to determine best sparsity level (k).\nRun iht on optimal k.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"We believe the best way to learn is through examples. Head over to the example section on the left to see these steps in action. ","category":"page"},{"location":"man/getting_started/#Wrapper-Functions","page":"Getting Started","title":"Wrapper Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Most users will use the following wrapper functions, which automatically handles everything. The user only has to specify where the PLINK files (and possibly the phenotype/covariate files) are located. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  iht","category":"page"},{"location":"man/getting_started/#MendelIHT.iht","page":"Getting Started","title":"MendelIHT.iht","text":"iht(plinkfile, k, kwargs...)\n\nRuns IHT with sparsity level k. Example:\n\nresult = iht(\"plinkfile\", 10)\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\niht(plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, with additional covariates stored separately. Example:\n\nresult = iht(\"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\niht(phenotypes, plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, where both phenotypes and additional covariates are stored separately. Example:\n\nresult = iht(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cross_validate","category":"page"},{"location":"man/getting_started/#MendelIHT.cross_validate","page":"Getting Started","title":"MendelIHT.cross_validate","text":"cross_validate(plinkfile, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", 1:20)\nmses = cross_validate(\"plinkfile\", [1, 2, 3, 4, 5]) # alternative syntax\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(phenotypes, plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/#Core-Functions","page":"Getting Started","title":"Core Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"For advanced users, one can also run IHT regression or cross-validation directly. For cross validation, we generally recommend using cv_iht. This function cycles through the testing sets sequentially and fits different sparsity models in parallel. For larger problems (e.g. UK Biobank sized), one can instead choose to run cv_iht_distribute_fold. This function fits different sparsity models sequentially but initializes all training/testing model in parallel, which consumes more memory (see below). The later strategy allows one to distribute different sparsity parameters to different computers, achieving greater parallel power. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  fit","category":"page"},{"location":"man/getting_started/#StatsBase.fit","page":"Getting Started","title":"StatsBase.fit","text":"fit(Mod::Type{<:StatisticalModel}, f::FormulaTerm, data, args...; \n    contrasts::Dict{Symbol}, kwargs...)\n\nConvert tabular data into a numeric response vector and predictor matrix using the formula f, and then fit the specified model type, wrapping the result in a TableRegressionModel or TableStatisticalModel (as appropriate).\n\nThis is intended as a backstop for modeling packages that implement model types that are subtypes of StatsBase.StatisticalModel but do not explicitly support the full StatsModels terms-based interface.  Currently this works by creating a ModelFrame from the formula and data, and then converting this to a ModelMatrix, but this is an internal implementation detail which may change in the near future.\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cv_iht","category":"page"},{"location":"man/getting_started/#MendelIHT.cv_iht","page":"Getting Started","title":"MendelIHT.cv_iht","text":"cv_iht(y, x, z; kwargs...)\n\nFor each model specified in path, performs q-fold cross validation and  returns the (averaged) deviance residuals. \n\nThe purpose of this function is to find the best sparsity level k, obtained from selecting the model with the minimum out-of-sample error. Different cross validation folds are cycled through sequentially different paths are fitted in parallel on different CPUs. Currently there are no routines to cross validate different group sizes. \n\nWarning\n\nDo not remove files with random file names when you run this function. These are  memory mapped files that will be deleted automatically once they are no longer needed.\n\nArguments\n\ny: Response vector (phenotypes), should be an Array{T, 1}.\nx: A design matrix (genotypes). Should be a SnpArray or an Array{T, 2}. \nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}. The first column should be the intercept (i.e. column of 1). \n\nOptional Arguments:\n\npath: Different model sizes to be tested in cross validation (default 1:20)\nq: Number of cross validation folds. (default 5)\nd: Distribution of your phenotype. (default Normal)\nl: A link function (default IdentityLink)\nest_r: Symbol for whether to estimate nuisance parameters. Only supported distribution is negative binomial and choices include :Newton or :MM.\ngroup: vector storing group membership for each predictor\nweight: vector storing vector of weights containing prior knowledge on each predictor\nfolds: Vector that separates the sample into q disjoint subsets\ndestin: Directory where intermediate files will be generated. Directory name must end with /.\ninit: Boolean indicating whether we should initialize IHT algorithm at a good starting guess\nuse_maf: Boolean indicating we should scale the projection step by a weight vector \ndebias: Boolean indicating whether we should debias at each IHT step\nverbose: Whether we want IHT to print meaningful intermediate steps\nparallel: Whether we want to run cv_iht using multiple CPUs (highly recommended)\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  cv_iht_distribute_fold","category":"page"},{"location":"man/getting_started/#MendelIHT.cv_iht_distribute_fold","page":"Getting Started","title":"MendelIHT.cv_iht_distribute_fold","text":"Performs q-fold cross validation for Iterative hard thresholding to  determine the best model size k. The function is the same as cv_iht  except here each fold is distributed to a different CPU as opposed  to each path to a different CPU. \n\nThis function has the edge over cv_iht because one can fit different  sparsity levels on different computers. But this is assuming you have  enough RAM and disk space to store all training data simultaneously.\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nDo not delete intermediate files with random file names created by cv_iht and cv_iht_distribute_fold (windows users will be instructed to manually do so via print statements). These are memory-mapped files necessary for cross validation. For cv_iht, you must have x GB of free space and RAM on your hard disk where x is your .bed file size. For cv_iht_distribute_fold, you must have enough RAM and disk space to fit all q training datasets simultaneously, each of which typically requires (q - 1)/q * x GB. ","category":"page"},{"location":"man/getting_started/#Specifying-Groups-and-Weights","page":"Getting Started","title":"Specifying Groups and Weights","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"When you have group and weight information, you input them as optional arguments in L0_reg and cv_iht. The weight vector is a vector of Float64, while the group vector is a vector of integers. For instance,","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"    g = #import group vector\n    w = #import weight vector\n    J = length(unique(g)) # specify number of non-zero groups\n    result = L0_reg(x, xbm, z, y, J, k, d(), l, group=g, weight=w)","category":"page"},{"location":"man/getting_started/#Simulation-Utilities","page":"Getting Started","title":"Simulation Utilities","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT provides some simulation utilities that help users explore the function and capabilities of iterative hard thresholding. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_random_snparray","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_random_snparray","page":"Getting Started","title":"MendelIHT.simulate_random_snparray","text":"simulate_random_snparray(s::String, n::Integer, p::Integer; \n    [mafs::Vector{Float64}], [min_ma::Integer])\n\nCreates a random SnpArray in the current directory without missing value,  where each SNP has ⫺5 (default) minor alleles. \n\nNote: if supplied minor allele frequency is extremely small, it could take a long time for the simulation to generate samples where at least min_ma (defaults to 5) are present. \n\nArguments:\n\ns: name of SnpArray that will be created in the current directory. To not   create file, use undef.\nn: number of samples\np: number of SNPs\n\nOptional Arguments:\n\nmafs: vector of desired minor allele freuqencies (uniform(0,0.5) by default)\nmin_ma: the minimum number of minor alleles that must be present for each   SNP (defaults to 5)\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_correlated_snparray","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_correlated_snparray","page":"Getting Started","title":"MendelIHT.simulate_correlated_snparray","text":"simulate_correlated_snparray(s, n, p; block_length, hap, prob)\n\nSimulates a SnpArray with correlation. SNPs are divided into blocks where each adjacent SNP is the same with probability prob. There are no correlation between blocks.\n\nArguments:\n\nn: number of samples\np: number of SNPs\ns: name of SnpArray that will be created (memory mapped) in the current directory. To not memory map, use undef.\n\nOptional arguments:\n\nblock_length: length of each LD block\nhap: number of haplotypes to simulate for each block\nprob: with probability prob an adjacent SNP would be the same. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nSimulating a SnpArray with n subjects and p SNPs requires up to 4np bits of RAM. Make sure you have enough RAM before simulating very large SnpArrays.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  simulate_random_response","category":"page"},{"location":"man/getting_started/#MendelIHT.simulate_random_response","page":"Getting Started","title":"MendelIHT.simulate_random_response","text":"simulate_random_response(x, k, d, l; kwargs...)\n\nThis function simulates a random response (trait) vector y. When the  distribution d is from Poisson, Gamma, or Negative Binomial, we simulate  β ∼ N(0, 0.3) to roughly ensure the mean of response y doesn't become too large. For other distributions, we choose β ∼ N(0, 1). \n\nArguments\n\nx: Design matrix\nk: the true number of predictors. \nd: The distribution of the simulated trait (note typeof(d) = UnionAll but typeof(d()) is an actual distribution: e.g. Normal)\nl: The link function. Input canonicallink(d()) if you want to use the canonical link of d.\n\nOptional arguments\n\nr: The number of success until stopping in negative binomial regression, defaults to 10\nα: Shape parameter of the gamma distribution, defaults to 1\n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor negative binomial and gamma, the link function must be LogLink. For Bernoulli, the probit link seems to work better than logitlink when used in cv_iht or L0_reg. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  make_bim_fam_files","category":"page"},{"location":"man/getting_started/#MendelIHT.make_bim_fam_files","page":"Getting Started","title":"MendelIHT.make_bim_fam_files","text":"make_bim_fam_files(x::SnpArray, y, name::String)\n\nCreates .bim and .bed files from a SnpArray. \n\nArguments:\n\nx: A SnpArray (i.e. .bed file on the disk) for which you wish to create corresponding .bim and .fam files.\nname: string that should match the .bed file (Do not include .bim or .fam extensions in name).\ny: Trait vector that will go in to the 6th column of .fam file. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/#Other-Useful-Functions","page":"Getting Started","title":"Other Useful Functions","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT additionally provides useful utilities that may be of interest to a few advanced users. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  iht_run_many_models","category":"page"},{"location":"man/getting_started/#MendelIHT.iht_run_many_models","page":"Getting Started","title":"MendelIHT.iht_run_many_models","text":"Runs IHT across many different model sizes specifed in path using the full design matrix. Same as cv_iht but DOES NOT validate in a holdout set, meaning that this will definitely induce overfitting as we increase model size. Use this if you want to quickly estimate a range of feasible model sizes before  engaging in full cross validation. \n\n\n\n\n\n","category":"function"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"  maf_weights","category":"page"},{"location":"man/getting_started/#MendelIHT.maf_weights","page":"Getting Started","title":"MendelIHT.maf_weights","text":"maf_weights(x::SnpArray; max_weight::T = Inf)\n\nCalculates the prior weight based on minor allele frequencies. \n\nReturns an array of weights where w[i] = 1 / (2 * sqrt(p[i] (1 - p[i]))) ∈ (1, ∞). Here p is the minor allele frequency computed by maf() in SnpArrays. \n\nx: A SnpArray \nmax_weight: Maximum weight for any predictor. Defaults to Inf. \n\n\n\n\n\n","category":"function"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we give numerous example analysis of GWAS data with MendelIHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# machine information for reproducibility\nversioninfo()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Julia Version 1.5.0\nCommit 96786e22cc (2020-08-01 23:44 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#first add workers needed for parallel computing. Add only as many CPU cores available\nusing Distributed\naddprocs(4)\n\n#load necessary packages for running all examples below\nusing Revise\nusing MendelIHT\nusing SnpArrays\nusing DataFrames\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing GLM\nusing DelimitedFiles\nusing Statistics\nusing BenchmarkTools\nusing Plots","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"┌ Info: Precompiling MendelIHT [921c7187-1484-5754-b919-5d3ed9ac03c4]\n└ @ Base loading.jl:1278","category":"page"},{"location":"man/examples/#Example-1:-GWAS-with-PLINK-files","page":"Examples","title":"Example 1: GWAS with PLINK files","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"For PLINK files, users are exposed to a few simple wrapper functions. For demonstration, we use simulated data under the data directory, as shown below. This data simulates quantitative (Gaussian) traits using n=1000 samples and p=10000 SNPs. There are 8 causal variants and 2 causal non-genetic covariates (intercept and sex). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Start Julia and execute the following:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# change directory to where example data is located\ncd(normpath(MendelIHT.datadir()))\n\n# show working directory\n@show pwd() \n\n# show files in current directory\nreaddir()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n\n\n\n\n\n7-element Array{String,1}:\n \"covariates.txt\"\n \"normal.bed\"\n \"normal.bim\"\n \"normal.fam\"\n \"normal_true_beta.txt\"\n \"phenotypes.txt\"\n \"simulate.jl\"","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here covariates.txt contains non-genetic covariates, normal.bed/bim/fam are the PLINK files storing genetic covariates, phenotypes.txt are phenotypes for each sample, normal_true_beta.txt is the true statistical model used to generate the phenotypes, and simulate.jl is the script used to generate all the files. ","category":"page"},{"location":"man/examples/#Step-1:-Run-cross-validation-to-determine-best-model-size","page":"Examples","title":"Step 1: Run cross validation to determine best model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"If phenotypes are stored in the .fam file and there are no other covariates (except for the intercept which is automatically included), one can run cross validation as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# test k = 1, 2, ..., 20\nmses = cross_validate(\"normal\", 1:20)\nargmin(mses)\n\n# Alternative syntax\n# mses = cross_validate(\"normal\", [1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n# mses = cross_validate(\"normal\", \"covariates.txt\", 1:20) # include additional covariates in separate file\n# mses = cross_validate(\"phenotypes.txt\", \"normal\", \"covariates.txt\", 1:20) # when phenotypes are stored separately","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Crossvalidation Results:\n\tk\tMSE\n\t1\t1424.4209463397158\n\t2\t877.4127442461745\n\t3\t698.4610947750848\n\t4\t573.1504682310128\n\t5\t476.31578846449054\n\t6\t409.82530303194505\n\t7\t359.5017949797407\n\t8\t325.0831239222133\n\t9\t331.76688175689503\n\t10\t335.24897480823256\n\t11\t342.2539099548487\n\t12\t349.5580549505318\n\t13\t352.87834253489024\n\t14\t351.1138715603811\n\t15\t351.0544198232595\n\t16\t350.27000489574243\n\t17\t352.9226806566691\n\t18\t357.8264018809541\n\t19\t365.6812419015122\n\t20\t372.10901493254187\n\n\n\n\n\n8","category":"page"},{"location":"man/examples/#Step-2:-Run-IHT-on-best-k","page":"Examples","title":"Step 2: Run IHT on best k","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"According to cross validation, k = 8 achieves the minimum MSE. Thus we run IHT on the full dataset.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"normal\", 8)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.2.0                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse linear regression\nLink functin = IdentityLink()\nSparsity parameter (k) = 8\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nConverging when tol < 0.0001\n\nIteration 1: tol = 0.7845860052299409\nIteration 2: tol = 0.02358096868235321\nIteration 3: tol = 0.001550076526387469\nIteration 4: tol = 0.00010521336604120053\nIteration 5: tol = 8.430366413828275e-6\n\n\n\n\n\n\nIHT estimated 7 nonzero SNP predictors and 1 non-genetic predictors.\n\nCompute time (sec):     0.0751640796661377\nFinal loglikelihood:    -1627.2792448761559\nIterations:             5\n\nSelected genetic predictors:\n\u001b[1m7×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │     3137     0.411838\n   2 │     4246     0.572452\n   3 │     4717     0.909215\n   4 │     6290    -0.693302\n   5 │     7755    -0.54482\n   6 │     8375    -0.788884\n   7 │     9415    -2.15858\n\nSelected nongenetic predictors:\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.65223","category":"page"},{"location":"man/examples/#Step-3:-Examine-results","page":"Examples","title":"Step 3: Examine results","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here IHT picked 7 SNPs and the intercept as the 8 most significant predictor. The SNP position is the order in which the SNP appeared in the PLINK file. To extract more information (for instance to extract rs IDs), we can do","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"snpdata = SnpData(\"normal\")                   # import PLINK information\nselected_snps = findall(!iszero, result.beta) # indices of SNPs selected by IHT\nsnpdata.snp_info[selected_snps, :]            # see which SNPs are selected","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"<table class=\"data-frame\"><thead><tr><th></th><th>chromosome</th><th>snpid</th><th>genetic_distance</th><th>position</th><th>allele1</th><th>allele2</th></tr><tr><th></th><th>String</th><th>String</th><th>Float64</th><th>Int64</th><th>String</th><th>String</th></tr></thead><tbody><p>7 rows × 6 columns</p><tr><th>1</th><td>1</td><td>snp3137</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>2</th><td>1</td><td>snp4246</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>3</th><td>1</td><td>snp4717</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>4</th><td>1</td><td>snp6290</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>5</th><td>1</td><td>snp7755</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>6</th><td>1</td><td>snp8375</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr><tr><th>7</th><td>1</td><td>snp9415</td><td>0.0</td><td>1</td><td>1</td><td>2</td></tr></tbody></table>","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The table above displays the SNP information for the selected SNPs. Since data is simulated, the fields genetic_distance, position, allele1, allele2 are arbitrary and snpid are fake. ","category":"page"},{"location":"man/examples/#Example-2:-How-to-simulate-data","page":"Examples","title":"Example 2: How to simulate data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we demonstrate how to use MendelIHT.jl and SnpArrays.jl to simulate data, allowing you to design your own genetic studies. Note all linear algebra routines involving PLINK files are handled by SnpArrays.jl. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate an example PLINK trio (.bim, .bed, .fam) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Bernoulli(mathbfx_i^Tboldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm intercept = 1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm sex = 15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000            # number of samples\np = 10000           # number of SNPs\nk = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\nd = Bernoulli       # Binary (continuous) phenotypes\nl = LogitLink()     # canonical link function\n\n# set random seed\nRandom.seed!(1111)\n\n# simulate `sim.bed` file with no missing data\nx = simulate_random_snparray(\"sim.bed\", n, p)\nxla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n\n# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\nz = ones(n, 2) \nz[:, 2] .= rand(0:1, n)\nstandardize!(@view(z[:, 2:end])) \n\n# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\ntrue_b = zeros(p) \ntrue_b[1:k-2] = randn(k-2)\nshuffle!(true_b)\n\n# find correct position of genetic predictors\ncorrect_position = findall(!iszero, true_b)\n\n# define effect size of non-genetic predictors: intercept & sex\ntrue_c = [1.0; 1.5] \n\n# simulate phenotype using genetic and nongenetic predictors\nprob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # turn y into floating point numbers\n\n# create `sim.bim` and `sim.bam` files using phenotype\nmake_bim_fam_files(x, y, \"sim\")\n\n#save covariates and phenotypes (without header)\nwritedlm(\"sim.covariates.txt\", z, ',')\nwritedlm(\"sim.phenotypes.txt\", y)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"note: Note\nPlease standardize your non-genetic covariates. If you use our iht() or cross_validation() functions, standardization is automatic. For genotype matrix, SnpLinAlg efficiently achieves this standardization. For non-genetic covariates, please use the built-in function standardize!. ","category":"page"},{"location":"man/examples/#Example-3:-Logistic/Poisson/Negative-binomial-GWAS","page":"Examples","title":"Example 3: Logistic/Poisson/Negative-binomial GWAS","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true k = 10. Let's try running a logistic regression on this data. This is specified using keyword arguments. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"sim\", \"sim.covariates.txt\", 10, d=Bernoulli(), l=LogitLink())\n\n# other responses\n# result = iht(\"sim\", 10, d=Bernoulli(), l=ProbitLink())     # Logistic regression using ProbitLink\n# result = iht(\"sim\", 10, d=Poisson(), l=LogLink())          # Poisson regression using canonical link\n# result = iht(\"sim\", 10, d=NegativeBinomial(), l=LogLink()) # Negative Binomial regression using canonical link","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.2.0                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse logistic regression\nLink functin = LogitLink()\nSparsity parameter (k) = 10\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nConverging when tol < 0.0001\n\nIteration 1: tol = 0.5882274462947447\nIteration 2: tol = 0.2825138668458021\nIteration 3: tol = 0.19289827584644756\nIteration 4: tol = 0.14269962283934917\nIteration 5: tol = 0.022831477149267632\nIteration 6: tol = 0.019792429262653115\nIteration 7: tol = 0.019845664939460095\nIteration 8: tol = 0.00765066824120313\nIteration 9: tol = 0.006913691748350025\nIteration 10: tol = 0.006159757548662376\nIteration 11: tol = 0.0054730856932040705\nIteration 12: tol = 0.004854846133978282\nIteration 13: tol = 0.004300010671833966\nIteration 14: tol = 0.0038033387186573\nIteration 15: tol = 0.003359795402130408\nIteration 16: tol = 0.0029645876127234955\nIteration 17: tol = 0.0026131815201996976\nIteration 18: tol = 0.002301318021364227\nIteration 19: tol = 0.002025024729429744\nIteration 20: tol = 0.001780622915677454\nIteration 21: tol = 0.0015647287330161268\nIteration 22: tol = 0.0013742489121423226\nIteration 23: tol = 0.0012063716814260336\nIteration 24: tol = 0.0010585539268262742\nIteration 25: tol = 0.0009285056582307361\nIteration 26: tol = 0.0008141727680124563\nIteration 27: tol = 0.0007137189219975595\nIteration 28: tol = 0.0006255072563945025\nIteration 29: tol = 0.0005480823925167159\nIteration 30: tol = 0.00048015313770763916\nIteration 31: tol = 0.0004205761209236388\nIteration 32: tol = 0.00036834051544793833\nIteration 33: tol = 0.00032255392716466075\nIteration 34: tol = 0.00028242947163362354\nIteration 35: tol = 0.0002472740235281775\nIteration 36: tol = 0.00021647759466681234\nIteration 37: tol = 0.00018950377910949886\nIteration 38: tol = 0.00016588119325870545\nIteration 39: tol = 0.00014519583372057257\nIteration 40: tol = 0.0001270842743388486\nIteration 41: tol = 0.00011122762514495094\nIteration 42: tol = 9.734617909701182e-5\n\n\n\n\n\n\nIHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n\nCompute time (sec):     0.22192096710205078\nFinal loglikelihood:    -331.6518739156732\nIterations:             42\n\nSelected genetic predictors:\n\u001b[1m8×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │     3137     0.503252\n   2 │     4246     0.590809\n   3 │     4248    -0.37987\n   4 │     4717     1.04006\n   5 │     6290    -0.741734\n   6 │     7755    -0.437585\n   7 │     8375    -0.942293\n   8 │     9415    -2.11206\n\nSelected nongenetic predictors:\n\u001b[1m2×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.03892\n   2 │        2      1.5844","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since data is simulated, we can compare IHT's estimated effect size with the truth. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"8×2 Array{Float64,2}:\n  0.469278    0.406926\n  0.554408    0.493509\n  0.923213    0.76469\n  0.0369732   0.0\n -0.625634   -0.550352\n -0.526553   -0.311351\n -0.815561   -0.677824\n -2.18271    -1.56627","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT found 7/8 genetic predictors, and estimates are reasonably close to truth. IHT missed one SNP with very small effect size (beta = 00369). The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# remove simulated data once they are no longer needed\nrm(\"sim.bed\", force=true)\nrm(\"sim.bim\", force=true)\nrm(\"sim.fam\", force=true)\nrm(\"sim.covariates.txt\", force=true)\nrm(\"sim.phenotypes.txt\", force=true)","category":"page"},{"location":"man/examples/#Example-4:-Running-IHT-on-general-matrices","page":"Examples","title":"Example 4: Running IHT on general matrices","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To run IHT on genotypes in VCF files, or other general data, one must call fit and cv_iht directly. These functions are designed to work on AbstractArray{T, 2} type where T is a Float64 or Float32. Thus, one must first import the data, and then call fit and cv_iht on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"tip: Tip\nCheck out VCFTools.jl to learn how to import VCF data.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate some count response using the model:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Poisson(mathbfx_i^T boldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Normal(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 03)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000             # number of samples\np = 10000            # number of SNPs\nk = 10               # 9 causal predictors + intercept\nd = Poisson          # Response follows Poisson distribution (count data)\nl = LogLink()        # canonical link\n\n# set random seed for reproducibility\nRandom.seed!(2020)\n\n# simulate design matrix\nx = randn(n, p)\n\n# simulate response, true model b, and the correct non-0 positions of b\ntrue_b = zeros(p)\ntrue_b[1:k] .= rand(Normal(0, 0.5), k)\nshuffle!(true_b)\nintercept = 1.0\ncorrect_position = findall(!iszero, true_b)\nprob = GLM.linkinv.(l, intercept .+ x * true_b)\nclamp!(prob, -20, 20) # prevents overflow\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # convert phenotypes to double precision","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Now we have the response y, design matrix x. Let's run IHT and compare with truth.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# first run cross validation \nmses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Crossvalidation Results:\n\tk\tMSE\n\t1\t1486.5413848514968\n\t2\t705.22250019531\n\t3\t541.7969789881145\n\t4\t465.1963943709811\n\t5\t442.6796495698108\n\t6\t458.13373854130487\n\t7\t460.9838544350322\n\t8\t484.25939604486814\n\t9\t474.21817442883844\n\t10\t505.0327194683676\n\t11\t499.2379415031341\n\t12\t504.0934101269238\n\t13\t487.9485902855123\n\t14\t548.1525696940757\n\t15\t507.86874709147395\n\t16\t530.6537481762397\n\t17\t511.2047414611475\n\t18\t583.6122865525828\n\t19\t548.3901670703196\n\t20\t569.6848731697289","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# run IHT on best k (achieved at k = 5)\nresult = fit(y, x, k=argmin(mses), d=Poisson(), l=LogLink())","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.2.0                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse Poisson regression\nLink functin = LogLink()\nSparsity parameter (k) = 5\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nConverging when tol < 0.0001\n\nIteration 1: tol = 0.2928574304111577\nIteration 2: tol = 0.05230999409875649\nIteration 3: tol = 0.07104164424891493\nIteration 4: tol = 0.026208176849564724\nIteration 5: tol = 0.02023001613423483\nIteration 6: tol = 0.011080308351803689\nIteration 7: tol = 0.00930914236578197\nIteration 8: tol = 0.00556416943618412\nIteration 9: tol = 0.004609772037770406\nIteration 10: tol = 0.002861799512474864\nIteration 11: tol = 0.002340881298705853\nIteration 12: tol = 0.001479832987797599\nIteration 13: tol = 0.0012011066579049358\nIteration 14: tol = 0.0007661746047595665\nIteration 15: tol = 0.0006191995770663082\nIteration 16: tol = 0.00039678836835178535\nIteration 17: tol = 0.00031993814923964465\nIteration 18: tol = 0.00020549845888243352\nIteration 19: tol = 0.00016549800033345948\nIteration 20: tol = 0.00010642830220001078\nIteration 21: tol = 8.565816720868677e-5\n\n\n\n\n\n\nIHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n\nCompute time (sec):     0.08859395980834961\nFinal loglikelihood:    -2335.176167840737\nIterations:             21\n\nSelected genetic predictors:\n\u001b[1m4×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │       83    -0.809284\n   2 │      989     0.378376\n   3 │     4294    -0.274544\n   4 │     4459     0.169417\n\nSelected nongenetic predictors:\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.26918","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# compare IHT result with truth\n[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"10×2 Array{Float64,2}:\n -1.303      -0.809284\n  0.585809    0.378376\n -0.0700563   0.0\n -0.0901341   0.0\n -0.0620201   0.0\n -0.441452   -0.274544\n  0.271429    0.169417\n -0.164888    0.0\n -0.0790484   0.0\n  0.0829054   0.0","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we ran IHT on count response with a general Array{T, 2} design matrix. Since many of the true beta are small, we were only able to find 5 true signals (4 predictors + intercept). ","category":"page"},{"location":"man/examples/#Example-5:-Group-IHT","page":"Examples","title":"Example 5: Group IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most J = 5 groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter k is known. ","category":"page"},{"location":"man/examples/#Data-simulation","page":"Examples","title":"Data simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in our paper. In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; j = 5 distinct groups are each assigned 125 causal SNPs with effect sizes randomly chosen from 0202. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity lambda_g = 125. The remaining groups are assigned lambda_g = 1 (i.e. only 1 active predictor are allowed).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# define problem size\nd = NegativeBinomial\nl = LogLink()\nn = 1000\np = 10000\nblock_size = 20                  #simulation parameter\nnum_blocks = Int(p / block_size) #simulation parameter\n\n# set seed\nRandom.seed!(2019)\n\n# assign group membership\nmembership = collect(1:num_blocks)\ng = zeros(Int64, p + 1)\nfor i in 1:length(membership)\n    for j in 1:block_size\n        cur_row = block_size * (i - 1) + j\n        g[block_size*(i - 1) + j] = membership[i]\n    end\nend\ng[end] = membership[end]\n\n#simulate correlated snparray\nx = simulate_correlated_snparray(n, p, \"tmp.bed\")\nz = ones(n, 1) # the intercept\nx_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n\n#simulate true model, where 5 groups each with 1~5 snps contribute\ntrue_b = zeros(p)\ntrue_groups = randperm(num_blocks)[1:5]\nsort!(true_groups)\nwithin_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n                randperm(block_size)[1:3], randperm(block_size)[1:4], \n                randperm(block_size)[1:5]]\ncorrect_position = zeros(Int64, 15)\nfor i in 1:5\n    cur_group = block_size * (true_groups[i] - 1)\n    cur_group_snps = cur_group .+ within_group[i]\n    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n    correct_position[start:last] .= cur_group_snps\nend\nfor i in 1:15\n    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\nend\nsort!(correct_position)\n\n# simulate phenotype\nr = 10 #nuisance parameter\nμ = GLM.linkinv.(l, x_float * true_b)\nclamp!(μ, -20, 20)\nprob = 1 ./ (1 .+ μ ./ r)\ny = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT without groups\nk = 15\nungrouped = L0_reg(x_float, z, y, 1, k, d(), l, verbose=false)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n\nCompute time (sec):     0.11840415000915527\nFinal loglikelihood:    -1441.522293255591\nIterations:             27\n\nSelected genetic predictors:\n15×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 3464     │ -0.234958   │\n│ 2   │ 4383     │ -0.135693   │\n│ 3   │ 4927     │ 0.158171    │\n│ 4   │ 4938     │ -0.222613   │\n│ 5   │ 5001     │ -0.193739   │\n│ 6   │ 5011     │ -0.162718   │\n│ 7   │ 5018     │ -0.190532   │\n│ 8   │ 5090     │ 0.226509    │\n│ 9   │ 5092     │ -0.17756    │\n│ 10  │ 5100     │ -0.140337   │\n│ 11  │ 7004     │ 0.151748    │\n│ 12  │ 7011     │ 0.206449    │\n│ 13  │ 7015     │ -0.284706   │\n│ 14  │ 7016     │ 0.218126    │\n│ 15  │ 9902     │ 0.119059    │\n\nSelected nongenetic predictors:\n0×2 DataFrame","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\nJ = 5\nmax_group_snps = ones(Int, num_blocks)\nmax_group_snps[true_groups] .= collect(1:5)\nvariable_group = L0_reg(x_float, z, y, J, max_group_snps, d(), l, verbose=false, group=g)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n\nCompute time (sec):     0.30719614028930664\nFinal loglikelihood:    -1446.3808810786898\nIterations:             16\n\nSelected genetic predictors:\n15×2 DataFrame\n│ Row │ Position │ Estimated_β │\n│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n├─────┼──────────┼─────────────┤\n│ 1   │ 3464     │ -0.245853   │\n│ 2   │ 4927     │ 0.160904    │\n│ 3   │ 4938     │ -0.213439   │\n│ 4   │ 5001     │ -0.19624    │\n│ 5   │ 5011     │ -0.149913   │\n│ 6   │ 5018     │ -0.181966   │\n│ 7   │ 5086     │ -0.0560478  │\n│ 8   │ 5090     │ 0.21164     │\n│ 9   │ 5092     │ -0.141968   │\n│ 10  │ 5100     │ -0.157655   │\n│ 11  │ 7004     │ 0.190224    │\n│ 12  │ 7011     │ 0.21294     │\n│ 13  │ 7015     │ -0.256058   │\n│ 14  │ 7016     │ 0.19746     │\n│ 15  │ 7020     │ 0.111755    │\n\nSelected nongenetic predictors:\n0×2 DataFrame","category":"page"},{"location":"man/examples/#Group-IHT-found-1-more-SNPs-than-ungrouped-IHT","page":"Examples","title":"Group IHT found 1 more SNPs than ungrouped IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#check result\ncorrect_position = findall(!iszero, true_b)\ncompare_model = DataFrame(\n    position = correct_position,\n    correct_β = true_b[correct_position],\n    ungrouped_IHT_β = ungrouped.beta[correct_position], \n    grouped_IHT_β = variable_group.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 15×4 DataFrame\n│ Row │ position │ correct_β │ ungrouped_IHT_β │ grouped_IHT_β │\n│     │ Int64    │ Float64   │ Float64         │ Float64       │\n├─────┼──────────┼───────────┼─────────────────┼───────────────┤\n│ 1   │ 3464     │ -0.2      │ -0.234958       │ -0.245853     │\n│ 2   │ 4927     │ 0.2       │ 0.158171        │ 0.160904      │\n│ 3   │ 4938     │ -0.2      │ -0.222613       │ -0.213439     │\n│ 4   │ 5001     │ -0.2      │ -0.193739       │ -0.19624      │\n│ 5   │ 5011     │ -0.2      │ -0.162718       │ -0.149913     │\n│ 6   │ 5018     │ -0.2      │ -0.190532       │ -0.181966     │\n│ 7   │ 5084     │ -0.2      │ 0.0             │ 0.0           │\n│ 8   │ 5090     │ 0.2       │ 0.226509        │ 0.21164       │\n│ 9   │ 5098     │ -0.2      │ 0.0             │ 0.0           │\n│ 10  │ 5100     │ -0.2      │ -0.140337       │ -0.157655     │\n│ 11  │ 7004     │ 0.2       │ 0.151748        │ 0.190224      │\n│ 12  │ 7011     │ 0.2       │ 0.206449        │ 0.21294       │\n│ 13  │ 7015     │ -0.2      │ -0.284706       │ -0.256058     │\n│ 14  │ 7016     │ 0.2       │ 0.218126        │ 0.19746       │\n│ 15  │ 7020     │ 0.2       │ 0.0             │ 0.111755      │","category":"page"},{"location":"man/examples/#Example-6:-Linear-Regression-with-prior-weights","page":"Examples","title":"Example 6: Linear Regression with prior weights","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include (predetermined) prior weights for each SNP. You can check out our paper for references of why/how to choose these weights. In this case, we mimic our paper and randomly set 10 of all SNPs to have a weight of 20. Other predictors have weight of 10. All causal SNPs have weights of 20. Under this scenario, SNPs with weight 20 is twice as likely to enter the model identified by IHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Our model is simulated as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim mathbfx_i^Tmathbfbeta + epsilon_i","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"epsilon_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#random seed\nRandom.seed!(4)\n\nd = Normal\nl = canonicallink(d())\nn = 1000\np = 10000\nk = 10\n\n# construct snpmatrix, covariate files, and true model b\nx = simulate_random_snparray(n, p, \"tmp.bed\")\nX = convert(Matrix{Float64}, x, center=true, scale=true)\nz = ones(n, 1) # the intercept\n    \n#define true_b \ntrue_b = zeros(p)\ntrue_b[1:10] .= collect(0.1:0.1:1.0)\nshuffle!(true_b)\ncorrect_position = findall(!iszero, true_b)\n\n#simulate phenotypes (e.g. vector y)\nprob = GLM.linkinv.(l, X * true_b)\nclamp!(prob, -20, 20)\ny = [rand(d(i)) for i in prob]\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# construct weight vector\nw = ones(p + 1)\nw[correct_position] .= 2.0\none_tenth = round(Int, p/10)\nidx = rand(1:p, one_tenth)\nw[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT\nunweighted = L0_reg(X, z, y, 1, k, d(), l, verbose=false)\nweighted   = L0_reg(X, z, y, 1, k, d(), l, verbose=false, weight=w)\n\n#check result\ncompare_model = DataFrame(\n    position    = correct_position,\n    correct     = true_b[correct_position],\n    unweighted  = unweighted.beta[correct_position], \n    weighted    = weighted.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×4 DataFrame\n│ Row │ position │ correct │ unweighted │ weighted │\n│     │ Int64    │ Float64 │ Float64    │ Float64  │\n├─────┼──────────┼─────────┼────────────┼──────────┤\n│ 1   │ 1254     │ 0.4     │ 0.452245   │ 0.450405 │\n│ 2   │ 1495     │ 0.3     │ 0.306081   │ 0.305738 │\n│ 3   │ 4856     │ 0.8     │ 0.853536   │ 0.862223 │\n│ 4   │ 5767     │ 0.1     │ 0.0        │ 0.117286 │\n│ 5   │ 5822     │ 0.7     │ 0.656213   │ 0.651908 │\n│ 6   │ 5945     │ 0.9     │ 0.891915   │ 0.894997 │\n│ 7   │ 6367     │ 0.5     │ 0.469718   │ 0.472524 │\n│ 8   │ 6996     │ 1.0     │ 0.963236   │ 0.973512 │\n│ 9   │ 7052     │ 0.6     │ 0.602162   │ 0.600055 │\n│ 10  │ 7980     │ 0.2     │ 0.231389   │ 0.234094 │","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this case, weighted IHT found an extra predictor than non-weighted IHT.","category":"page"},{"location":"man/examples/#Other-examples-and-functionalities","page":"Examples","title":"Other examples and functionalities","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We explored a few more examples in our manuscript, with reproducible code. We invite users to experiment with them as well. ","category":"page"},{"location":"#Mendel-Iterative-Hard-Thresholding","page":"Home","title":"Mendel - Iterative Hard Thresholding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A modern approach to analyze data from a Genome Wide Association Studies (GWAS)","category":"page"},{"location":"#Package-Feature","page":"Home","title":"Package Feature","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Analyze large GWAS datasets intuitively.\nBuilt-in support for PLINK binary files via SnpArrays.jl and VCF files via VCFTools.jl.\nOut-of-the-box parallel computing routines for q-fold cross-validation.\nFits a variety of generalized linear models with any choice of link function.\nComputation directly on raw genotype files.\nEfficient handlings for non-genetic covariates.\nOptional acceleration (debias) step to dramatically improve speed.\nAbility to explicitly incorporate weights for predictors.\nAbility to enforce within and between group sparsity. \nNaive genotype imputation. \nEstimates nuisance parameter for negative binomial regression using Newton or MM algorithm. \nExcellent flexibility to handle different data structures and complements well with other Julia packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read our paper for more detail.","category":"page"},{"location":"#Supported-GLM-models-and-Link-functions","page":"Home","title":"Supported GLM models and Link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MendelIHT borrows distribution and link functions implementationed in GLM.jl and Distributions.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distribution Canonical Link Status\nNormal IdentityLink checkmark\nBernoulli LogitLink checkmark\nPoisson LogLink checkmark\nNegativeBinomial LogLink checkmark\nGamma InverseLink experimental\nInverseGaussian InverseSquareLink experimental","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples of these distributions in their default value is visualized in this post.","category":"page"},{"location":"#Available-link-functions","page":"Home","title":"Available link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nProbitLink\nSqrtLink","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/getting_started.md\",\n    \"man/examples.md\",\n    \"man/math.md\",\n    \"man/contributing.md\",\n]\nDepth = 2","category":"page"}]
}

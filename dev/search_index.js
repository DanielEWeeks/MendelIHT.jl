var documenterSearchIndex = {"docs":
[{"location":"man/math/#Details-of-Parameter-Estimation","page":"Mathematical Details","title":"Details of Parameter Estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"This note is meant to supplement our paper. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For a review on generalized linear models, the following resources are recommended:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"(3rd edition) Chapter 15.3 of Applied regression analysis and generalized linear models by John Fox\n(3rd edition) Chapter 3-5 of An introduction to generalized linear models by Dobson and Barnett","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For review on projected gradient descent, I recommend","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Chapter 5 of MM optimization algorithms by Kenneth Lange (2nd edition is almost out, as of 1/2/2021)","category":"page"},{"location":"man/math/#Generalized-linear-models","page":"Mathematical Details","title":"Generalized linear models","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, phenotypes (bf y) are modeled as a generalized linear model:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    mu_i = E(y_i) = g(bf x_i^t boldsymbol beta)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where bf x is sample i's p-dimensional vector of covariates (genotypes + other fixed effects), boldsymbol beta is a p-dimensional regression coefficients, g is a non-linear inverse-link function, y_i is sample i's phenotype value, and mu_i is the average predicted value of y_i given bf x. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The full design matrix bf X_n times p and phenotypes bf y_n times 1 are observed. The distribution of bf y and the inverse link g are chosen before fitting. The regression coefficients boldsymbol beta are not observed and are estimated by maximum likelihood methods, traditionally via iteratively reweighted least squares (IRLS). For high dimensional problems where n  p, we substitute iterative hard thresholding in place of IRLS. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases under the GLM framework. When g(alpha) = alpha we get standard linear regression used for Gaussian phenotypes. ","category":"page"},{"location":"man/math/#Loglikelihood,-gradient,-and-expected-information","page":"Mathematical Details","title":"Loglikelihood, gradient, and expected information","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In GLM, the distribution of bf y is from the exponential family with density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    f(y mid theta phi) = exp left fracy theta - b(theta)a(phi) + c(y phi) right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Here theta is called the canonical (location) parameter and under the canonical link, theta = g(bf x^t bf beta). phi is the dispersion (scale) parameter. The functions a b c are known functions that vary depending on the distribution of y. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Given n independent observations, the loglikelihood is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    L(bf theta phi bf y) = sum_i=1^n fracy_itheta_i - b(theta_i)a_i(phi) + c(y_i phi)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate the loglikelihood, we evaluate sample i's logpdf using the logpdf function in Distributions.jl.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The perform maximum likelihood estimation, we compute partial derivatives for betas. The jth score component is (eq 4.18 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    fracpartial Lpartial beta_j = sum_i=1^n leftfracy_i - mu_ivar(y_i)x_ijleft(fracpartial mu_ipartial eta_iright)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus the full gradient is","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla L= bf X^tbf W(bf y - boldsymbolmu) quad W_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"and similarly, the expected information is (eq 4.23 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    J = bf X^ttildeWX quad tildeW_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate nabla L and J, note bf y and bf X are known, so we just need to calculate boldsymbolmu fracpartialmu_ipartialeta_i and var(y_i). The first simply uses the inverse link: mu_i = g(bf x_i^t boldsymbol beta). For the second, note fracpartial mu_ipartialeta_i = fracpartial g(bf x_i^t boldsymbol beta)partialbf x_i^t boldsymbol beta is just the derivative of the inverse link function evaluated at the linear predictor eta_i = bf x_i^t boldsymbol beta. This is already implemented for various link functions as mueta in GLM.jl, which we call internally. To compute var(y_i), we note that the exponential family distributions have variance","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    var(y) = a(phi)b(theta) = a(phi)fracpartial^2b(theta)partialtheta^2 = a(phi) var(mu)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"That is, var(y_i) is a product of 2 terms where the first depends solely on phi, and the second solely on mu_i = g(bf x_i^t boldsymbol beta). In our code, we use glmvar implemented in GLM.jl to calculate var(mu). Because phi is unknown, we assume a(phi) = 1 for all models in computing W_ii and tildeW_ii, except for the negative binomial model. For negative binomial model, we discuss how to estimate phi and boldsymbolbeta using alternate block descent below.  ","category":"page"},{"location":"man/math/#Iterative-hard-thresholding","page":"Mathematical Details","title":"Iterative hard thresholding","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, the loglikelihood is maximized using iterative hard thresholding. This is achieved by repeating the following iteration:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    boldsymbolbeta_n+1 = overbraceP_S_k^(3)big(boldsymbolbeta_n + underbraces_n_(2) overbracenabla f(boldsymbolbeta_n)^(1)big)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where f is the loglikelihood to maximize. Step (1) computes the gradient as previously discussed. Step (2) computes the step size s_k. Step (3) evaluates the projection operator P_S_k, which sets all but k largest entries in magnitude to 0. To perform P_S_k, we first partially sort the dense vector beta_n + s_n nabla f(beta_n), and set the smallest k+1  n entries in magnitude to 0. Note the step size s_n is derived in our paper to be","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    s_n = fracnabla f(boldsymbolbeta_n)_2^2nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where J = bf X^ttildeWX is the expected information matrix (derived in the previous section) which should never be explicitly formed. To evaluate the denominator, observe that ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n) = left(nabla f(boldsymbolbeta_n)^tbf X^t sqrt(tildeW)right)left(sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n)right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus one computes bf v = sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n) and calculate its inner product with itself. ","category":"page"},{"location":"man/math/#Nuisance-parameter-estimation","page":"Mathematical Details","title":"Nuisance parameter estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Currently MendelIHT.jl only estimates nuisance parameter for the Negative Binomial model. Estimation of phi and boldsymbol beta can be achieved with alternating block updates. That is, we run 1 IHT iteration to estimate boldsymbol beta_n, followed by 1 iteration of Newton or MM update to estimate phi_n. Below we derive the Newton and MM updates. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note 1: This feature is provided by our 2019 Bruins in Genomics summer student Vivian Garcia and Francis Adusei. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note 2: for Gaussian response, one can use the sample variance formula to estimate phi from the estimated mean hatmu. ","category":"page"},{"location":"man/math/#Parametrization-for-Negative-Binomial-model","page":"Mathematical Details","title":"Parametrization for Negative Binomial model","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The negative binomial distribution has density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tP(Y = y) = binomy+r-1yp^r(1-p)^y\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where y is the number of failures before the rth success and p is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to WOLFRAM is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tmu_i = fracr(1-p_i)r quad\n\tVar(y_i) = fracr(1-p_i)p_i^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note these formula are different than the default on wikipedia because in wiki y is the number of success and r is the number of failure.  Therefore, solving for p_i, we have ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tp_i = fracrmu_i + r = fracre^mathbfx_i^Tbeta + r in (0 1)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"And indeed this this is how we parametrize the negative binomial model. Importantly, we can interpret p_i as a probability, since mathbfx_i^Tbeta can take on any number between -infty and +infty (since beta and mathbfx_i can have positive and negative entries), so exp(mathbfx_i^Tbeta)in(0 infty).","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"We can also try to express Var(y_i) in terms of mu_i and r by doing some algebra:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tVar(y_i)\n\t= fracr(1-p_i)p_i^2 = fracrleft( 1 - fracrmu_i + r right)fracr^2(mu_i + r)^2 = frac1rleft(1 - fracrmu_i + rright)(mu_i + r)^2 \n\t= frac1r left (mu_i + r)^2 - r(mu_r + r) right = frac1r(mu_i + r)mu_i\n\t= mu_i left( fracmu_ir + 1 right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"You can verify in GLM.jl that this is indeed how they compute the variance of a negative binomial distribution. ","category":"page"},{"location":"man/math/#Estimating-nuisance-parameter-using-MM-algorithms","page":"Mathematical Details","title":"Estimating nuisance parameter using MM algorithms","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The MM algorithm is very stable, but converges much slower than Newton's alogorithm below. Thus use MM only if Newton's method fails.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The loglikelihood for n independent samples under a Negative Binomial model is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tL(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left sum_j=0^y_i - 1 ln(r+j) + rln(p_i) - ln(y_i) + y_iln(1-p_i) right\n\tgeq sum_i=1^mleft sum_j=0^y_i-1fracr_nr_n+jln(r) + c_n + rln(p_i) - ln(y_i) + y_i ln(1-p_i) right\n    equiv M(p_1  p_m r)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The last inequality can be seen by applying Jensen's inequality:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfleft sum_iu_i(boldsymboltheta)right leq sum_i fracu_i(boldsymboltheta_n)sum_j u_j(boldsymboltheta_n)f left fracsum_j u_j(boldsymboltheta_n)u_i(boldsymboltheta_n) u_i(boldsymboltheta)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"to the function f(u) = - ln(u) Maximizing M over r (i.e. differentiating with respect to r and setting equal to zero, then solving for r), we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    fracddr M\n\t= sum_i=1^m left sum_j=0^y_i-1 fracr_nr_n + j frac1r + ln(p_i) right \n\t= sum_i=1^msum_j=0^y_i-1 fracr_nr_n + j frac1r + sum_i=1^mln(p_i) \n\tequiv 0\n\tiff r_n+1 = frac-sum_i=1^msum_j=0^y_i-1 fracr_nr_n + jsum_i=1^mln(p_i)  \nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since L ge M (M minorizes L), maximizing M will maximize L. ","category":"page"},{"location":"man/math/#Estimating-Nuisance-parameter-using-Newton's-method","page":"Mathematical Details","title":"Estimating Nuisance parameter using Newton's method","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since we are dealing with 1 parameter optimization, Newton's method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (r), we use maximum likelihood estimates. By p_i = r  (mu_i + r) in above, we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\t L(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left lnleft((y_i+r-1)right) - lnleft(y_iright) - lnleft((r-1)right) + rln(r) - rln(mu_i+r) + y_iln(mu_i) + y_iln(mu_i + r)right\n\t= sum_i=1^mleftlnleft((y_i+r-1)right)-ln(y_i) - lnleft((r-1)right) + rln(r) - (r+y_i)ln(mu_i + r) + y_iln(mu_i)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Recalling the definition of digamma and trigamma functions, the first and second derivative of our last expression with respect to r is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfracddr L(p_1  p_m r) =  sum_i=1^m left operatornamedigamma(y_i+r) - operatornamedigamma(r) + 1 + ln(r) - fracr+y_imu_i+r - ln(mu_i + r) right\n\tfracd^2dr^2 L(p_1  p_m r) =sum_i=1^m left operatornametrigamma(y_i+r) - operatornametrigamma(r) + frac1r - frac2mu_i + r + fracr+y_i(mu_i + r)^2 right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"So the iteration to use is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tr_n+1 = r_n - fracfracddrL(p_1p_mr)fracd^2dr^2L(p_1p_mr)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For stability, we set the denominator equal to 1 if it is less than 0. That is, we use gradient descent if the current iteration has non-positive definite Hessian matrices. ","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Please star our github page, that would be very helpful.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you would like to contribute to this project, we compiled a list of desired features for this project. Developers of any level is welcomed. Do not be shy because it can't hurt to ask. ","category":"page"},{"location":"man/contributing/#Bug-Fixes-and-User-Support","page":"Contributing","title":"Bug Fixes & User Support","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you encounter a bug or you need some user support, please open a new issue here. If you can, provide the error message and, ideally, a reproducible code that generated the error.","category":"page"},{"location":"man/contributing/#Citation","page":"Contributing","title":"Citation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you use MendelIHT.jl in an academic manuscript, please cite:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"@article{mendeliht,\n  title={{Iterative hard thresholding in genome-wide association studies: Generalized linear models, prior weights, and double sparsity}},\n  author={Chu, Benjamin B and Keys, Kevin L and German, Christopher A and Zhou, Hua and Zhou, Jin J and Sobel, Eric M and Sinsheimer, Janet S and Lange, Kenneth},\n  journal={GigaScience},\n  volume={9},\n  number={6},\n  pages={giaa044},\n  year={2020},\n  publisher={Oxford University Press}\n}","category":"page"},{"location":"man/getting_started/#Getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"In this section, we outline the basic procedure to analyze your GWAS data with MendelIHT. ","category":"page"},{"location":"man/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Download and install Julia. Within Julia, copy and paste the following:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\npkg\"add https://github.com/OpenMendel/SnpArrays.jl\"\npkg\"add https://github.com/OpenMendel/MendelIHT.jl\"","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT.jl supports Julia 1.5+ for Mac, Linux, and window machines. A few features are disabled for windows users, and users will be warned when trying to use them.","category":"page"},{"location":"man/getting_started/#Typical-Workflow","page":"Getting Started","title":"Typical Workflow","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Run cross_validate() to determine best sparsity level (k).\nRun iht on optimal k.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"We believe the best way to learn is through examples. Head over to the example section on the left to see these steps in action. ","category":"page"},{"location":"man/getting_started/#Parallel-computing","page":"Getting Started","title":"Parallel computing","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"For large datasets, one can run cross validation in parallel. Assuming you have N cores, one can load N processors by","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Distributed\naddprocs(4) # 4 processors\n@everywhere begin\n    using MendelIHT\n    using LinearAlgebra\n    BLAS.set_num_threads(1)\nend","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Note by default, BLAS runs with multiple threads, so the command BLAS.set_num_threads(1) sets the number of BLAS threads to 1, avoiding oversubscription","category":"page"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we give numerous example analysis of GWAS data with MendelIHT.jl. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Users are highly encouraged to read the source code of our main fit_iht and cv_iht functions, which contain more options than what is described here.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# machine information for reproducibility\nversioninfo()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Julia Version 1.5.0\nCommit 96786e22cc (2020-08-01 23:44 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# add workers needed for parallel computing. Add only as many CPU cores available\nusing Distributed\n# addprocs(4)\n\n#load necessary packages for running all examples below\n@everywhere begin\n    using Revise\n    using MendelIHT\n    using SnpArrays\n    using DataFrames\n    using Distributions\n    using Random\n    using LinearAlgebra\n    using GLM\n    using DelimitedFiles\n    using Statistics\n    using BenchmarkTools\nend","category":"page"},{"location":"man/examples/#Example-1:-GWAS-with-PLINK-files","page":"Examples","title":"Example 1: GWAS with PLINK files","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, our data are stored in binary PLINK files:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"normal.bed\nnormal.bim\nnormal.fam","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"which contains simulated (Gaussian) phenotypes for n=1000 samples and p=10000 SNPs. There are 8 causal variants and 2 causal non-genetic covariates (intercept and sex). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"These data are present under MendelIHT/data directory.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# change directory to where example data is located\ncd(normpath(MendelIHT.datadir()))\n\n# show working directory\n@show pwd() \n\n# show files in current directory\nreaddir()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n\n\n\n\n\n9-element Array{String,1}:\n \".DS_Store\"\n \"README.md\"\n \"covariates.txt\"\n \"normal.bed\"\n \"normal.bim\"\n \"normal.fam\"\n \"normal_true_beta.txt\"\n \"phenotypes.txt\"\n \"simulate.jl\"","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here covariates.txt contains non-genetic covariates, normal.bed/bim/fam are the PLINK files storing genetic covariates, phenotypes.txt are phenotypes for each sample, normal_true_beta.txt is the true statistical model used to generate the phenotypes, and simulate.jl is the script used to generate all the files. ","category":"page"},{"location":"man/examples/#Step-1:-Run-cross-validation-to-determine-best-model-size","page":"Examples","title":"Step 1: Run cross validation to determine best model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"If phenotypes are stored in the .fam file and there are no other covariates (except for the intercept which is automatically included), one can run cross validation as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# test k = 1, 2, ..., 20\nmses = cross_validate(\"normal\", 1:20);\n\n# Alternative syntax\n# mses = cross_validate(\"normal\", [1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n# mses = cross_validate(\"normal\", \"covariates.txt\", 1:20) # include additional covariates in separate file\n# mses = cross_validate(\"phenotypes.txt\", \"normal\", \"covariates.txt\", 1:20) # when phenotypes are stored separately","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:30\u001b[39m\n\n\n\n\nCrossvalidation Results:\n\tk\tMSE\n\t1\t1408.1563764852363\n\t2\t859.2530853501157\n\t3\t675.489438272005\n\t4\t554.241640057255\n\t5\t457.7905801971809\n\t6\t393.4038368630614\n\t7\t340.4572103654293\n\t8\t308.14840394813007\n\t9\t313.19209629368044\n\t10\t315.4842259795911\n\t11\t324.98561588950076\n\t12\t325.9700128835669\n\t13\t328.5339681830119\n\t14\t330.3025490829286\n\t15\t326.8185639742826\n\t16\t343.98518919049127\n\t17\t335.8778323256304\n\t18\t340.69382555553784\n\t19\t345.50179778174197\n\t20\t346.9362994901397\n\nBest k = 8","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Do not be alarmed if you get slightly different MSEs, because cross validation breaks data into training/testing randomly. Set a seed by Random.seed!(1234) if you want reproducibility.","category":"page"},{"location":"man/examples/#Step-2:-Run-IHT-on-best-k","page":"Examples","title":"Step 2: Run IHT on best k","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"According to cross validation, k = 8 achieves the minimum MSE. Thus we run IHT on the full dataset.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"normal\", 8)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.3.3                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse linear regression\nLink functin = IdentityLink()\nSparsity parameter (k) = 8\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001:\n\nIteration 1: loglikelihood = -1632.5890039523172, backtracks = 0, tol = 0.7845860052299409\nIteration 2: loglikelihood = -1627.2942148015939, backtracks = 0, tol = 0.023580968682353067\nIteration 3: loglikelihood = -1627.2793358038934, backtracks = 0, tol = 0.0015500765263876102\nIteration 4: loglikelihood = -1627.2792456558277, backtracks = 0, tol = 0.00010521336604120053\nIteration 5: loglikelihood = -1627.279244876156, backtracks = 0, tol = 8.430366413828275e-6\n\n\n\n\n\n\nIHT estimated 7 nonzero SNP predictors and 1 non-genetic predictors.\n\nCompute time (sec):     0.03294205665588379\nFinal loglikelihood:    -1627.279244876156\nSNP PVE:                0.8276388351471942\nIterations:             5\n\nSelected genetic predictors:\n\u001b[1m7×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │     3137     0.411838\n   2 │     4246     0.572452\n   3 │     4717     0.909215\n   4 │     6290    -0.693302\n   5 │     7755    -0.54482\n   6 │     8375    -0.788884\n   7 │     9415    -2.15858\n\nSelected nongenetic predictors:\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.65223","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Note: we explicitly ran cross_validate and iht only with genetic data. The known non-genetic covariate sex is explicitly not included. They can be included using the alternative syntax (see Example 3)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"mses = cross_validate(\"normal\", \"covariates.txt\", 1:20)\nresult = iht(\"normal\", \"covariates.txt\", argmin(mses))","category":"page"},{"location":"man/examples/#Step-3:-Examine-results","page":"Examples","title":"Step 3: Examine results","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT picked 7 SNPs and the intercept as the 8 most significant predictor. Their position (the order in which the SNP appeared in the PLINK file) and estimated effect size are displayed. To extract more information (for instance to extract rs IDs), we can do","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"snpdata = SnpData(\"normal\")                   # import PLINK information\nsnps_idx = findall(!iszero, result.beta)      # indices of SNPs selected by IHT\nselected_snps = snpdata.snp_info[snps_idx, :] # see which SNPs are selected\n@show selected_snps;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"selected_snps = 7×6 DataFrame\n Row │ chromosome  snpid    genetic_distance  position  allele1  allele2\n     │ String      String   Float64           Int64     String   String\n─────┼───────────────────────────────────────────────────────────────────\n   1 │ 1           snp3137               0.0         1  1        2\n   2 │ 1           snp4246               0.0         1  1        2\n   3 │ 1           snp4717               0.0         1  1        2\n   4 │ 1           snp6290               0.0         1  1        2\n   5 │ 1           snp7755               0.0         1  1        2\n   6 │ 1           snp8375               0.0         1  1        2\n   7 │ 1           snp9415               0.0         1  1        2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The table above displays the SNP information for the selected SNPs. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since data is simulated, the fields chromosome, snpid, genetic_distance, position, allele1, and allele2 are fake. ","category":"page"},{"location":"man/examples/#Example-2:-How-to-simulate-data","page":"Examples","title":"Example 2: How to simulate data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we demonstrate how to use MendelIHT.jl and SnpArrays.jl to simulate data, allowing you to design your own genetic studies. Note:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"For more complex simulation, please use the module TraitSimulations.jl.  \nAll linear algebra routines involving PLINK files are handled by SnpArrays.jl. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate an example PLINK trio (.bim, .bed, .fam) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Bernoulli(mathbfx_i^Tboldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm intercept = 1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm sex = 15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000            # number of samples\np = 10000           # number of SNPs\nk = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\nd = Bernoulli       # Binary (continuous) phenotypes\nl = LogitLink()     # canonical link function\n\n# set random seed\nRandom.seed!(1111)\n\n# simulate `sim.bed` file with no missing data\nx = simulate_random_snparray(\"sim.bed\", n, p)\nxla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n\n# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\nz = ones(n, 2) \nz[:, 2] .= rand(0:1, n)\nstandardize!(@view(z[:, 2:end])) \n\n# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\ntrue_b = zeros(p) \ntrue_b[1:k-2] = randn(k-2)\nshuffle!(true_b)\n\n# find correct position of genetic predictors\ncorrect_position = findall(!iszero, true_b)\n\n# define effect size of non-genetic predictors: intercept & sex\ntrue_c = [1.0; 1.5] \n\n# simulate phenotype using genetic and nongenetic predictors\nprob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # turn y into floating point numbers\n\n# create `sim.bim` and `sim.bam` files using phenotype\nmake_bim_fam_files(x, y, \"sim\")\n\n#save covariates and phenotypes (without header)\nwritedlm(\"sim.covariates.txt\", z, ',')\nwritedlm(\"sim.phenotypes.txt\", y)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"note: Note\nPlease standardize (or at least center) your non-genetic covariates. If you use our iht() or cross_validation() functions, standardization is automatic. For genotype matrix, SnpLinAlg efficiently achieves this standardization. For non-genetic covariates, please use the built-in function standardize!. ","category":"page"},{"location":"man/examples/#Example-3:-Logistic/Poisson/Negative-binomial-GWAS","page":"Examples","title":"Example 3: Logistic/Poisson/Negative-binomial GWAS","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true k = 10. Let's try running a logistic regression on this data. This is specified using keyword arguments. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"sim\", \"sim.covariates.txt\", 10, d=Bernoulli(), l=LogitLink())\n\n# other responses\n# result = iht(\"sim\", 10, d=Bernoulli(), l=ProbitLink())     # Logistic regression using ProbitLink\n# result = iht(\"sim\", 10, d=Poisson(), l=LogLink())          # Poisson regression using canonical link\n# result = iht(\"sim\", 10, d=NegativeBinomial(), l=LogLink()) # Negative Binomial regression using canonical link","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.3.3                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse logistic regression\nLink functin = LogitLink()\nSparsity parameter (k) = 10\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001:\n\nIteration 1: loglikelihood = -403.91876912829696, backtracks = 0, tol = 0.588227446294744\nIteration 2: loglikelihood = -354.2415736389379, backtracks = 0, tol = 0.2825138668458029\nIteration 3: loglikelihood = -347.5483388154266, backtracks = 0, tol = 0.19289827584644767\nIteration 4: loglikelihood = -335.97152474649437, backtracks = 0, tol = 0.14269962283934898\nIteration 5: loglikelihood = -334.49756712078744, backtracks = 1, tol = 0.02283147714926755\nIteration 6: loglikelihood = -333.5432195711081, backtracks = 2, tol = 0.019792429262652955\nIteration 7: loglikelihood = -332.80672688543484, backtracks = 2, tol = 0.01984566493946018\nIteration 8: loglikelihood = -332.5588563458224, backtracks = 3, tol = 0.00765066824120313\nIteration 9: loglikelihood = -332.3619297572996, backtracks = 3, tol = 0.0069136917483499484\nIteration 10: loglikelihood = -332.20642890616097, backtracks = 3, tol = 0.006159757548662302\nIteration 11: loglikelihood = -332.0840431892422, backtracks = 3, tol = 0.0054730856932040705\nIteration 12: loglikelihood = -331.988004167528, backtracks = 3, tol = 0.004854846133978431\nIteration 13: loglikelihood = -331.912840859504, backtracks = 3, tol = 0.004300010671833966\nIteration 14: loglikelihood = -331.8541573752894, backtracks = 3, tol = 0.0038033387186571527\nIteration 15: loglikelihood = -331.8084401845103, backtracks = 3, tol = 0.0033597954021304085\nIteration 16: loglikelihood = -331.7728941748384, backtracks = 3, tol = 0.002964587612723496\nIteration 17: loglikelihood = -331.74530513071767, backtracks = 3, tol = 0.002613181520199698\nIteration 18: loglikelihood = -331.72392566719304, backtracks = 3, tol = 0.0023013180213642273\nIteration 19: loglikelihood = -331.707381516887, backtracks = 3, tol = 0.0020250247294297443\nIteration 20: loglikelihood = -331.69459517397274, backtracks = 3, tol = 0.0017806229156774542\nIteration 21: loglikelihood = -331.6847241325898, backtracks = 3, tol = 0.001564728733016127\nIteration 22: loglikelihood = -331.6771112518333, backtracks = 3, tol = 0.0013742489121423228\nIteration 23: loglikelihood = -331.6712450943089, backtracks = 3, tol = 0.0012063716814260338\nIteration 24: loglikelihood = -331.6667283950377, backtracks = 3, tol = 0.0010585539268262742\nIteration 25: loglikelihood = -331.6632531068335, backtracks = 3, tol = 0.0009285056582307363\nIteration 26: loglikelihood = -331.6605807292455, backtracks = 3, tol = 0.0008141727680124564\nIteration 27: loglikelihood = -331.6585268570941, backtracks = 3, tol = 0.0007137189219975596\nIteration 28: loglikelihood = -331.6569490812431, backtracks = 3, tol = 0.0006255072563945026\nIteration 29: loglikelihood = -331.65573754039684, backtracks = 3, tol = 0.000548082392516716\nIteration 30: loglikelihood = -331.65480756089744, backtracks = 3, tol = 0.0004801531377076392\nIteration 31: loglikelihood = -331.6540939353272, backtracks = 3, tol = 0.00042057612092363887\nIteration 32: loglikelihood = -331.6535464833441, backtracks = 3, tol = 0.0003683405154479384\nIteration 33: loglikelihood = -331.65312661305927, backtracks = 3, tol = 0.0003225539271646608\nIteration 34: loglikelihood = -331.6528046612901, backtracks = 3, tol = 0.00028242947163362354\nIteration 35: loglikelihood = -331.6525578388944, backtracks = 3, tol = 0.0002472740235281776\nIteration 36: loglikelihood = -331.6523686452689, backtracks = 3, tol = 0.00021647759466681237\nIteration 37: loglikelihood = -331.652223646102, backtracks = 3, tol = 0.00018950377910949889\nIteration 38: loglikelihood = -331.6521125319519, backtracks = 3, tol = 0.00016588119325870547\nIteration 39: loglikelihood = -331.65202739366003, backtracks = 3, tol = 0.0001451958337205726\nIteration 40: loglikelihood = -331.65196216503983, backtracks = 3, tol = 0.0001270842743388486\nIteration 41: loglikelihood = -331.65191219446064, backtracks = 3, tol = 0.00011122762514495095\nIteration 42: loglikelihood = -331.65187391567315, backtracks = 3, tol = 9.734617909715456e-5\n\n\n\n\n\n\nIHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n\nCompute time (sec):     0.30141711235046387\nFinal loglikelihood:    -331.65187391567315\nSNP PVE:                0.4798854810844273\nIterations:             42\n\nSelected genetic predictors:\n\u001b[1m8×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │     3137     0.503252\n   2 │     4246     0.590809\n   3 │     4248    -0.37987\n   4 │     4717     1.04006\n   5 │     6290    -0.741734\n   6 │     7755    -0.437585\n   7 │     8375    -0.942293\n   8 │     9415    -2.11206\n\nSelected nongenetic predictors:\n\u001b[1m2×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.03892\n   2 │        2      1.5844","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since data is simulated, we can compare IHT's estimated effect size with the truth. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"8×2 Array{Float64,2}:\n  0.469278    0.503252\n  0.554408    0.590809\n  0.923213    1.04006\n  0.0369732   0.0\n -0.625634   -0.741734\n -0.526553   -0.437585\n -0.815561   -0.942293\n -2.18271    -2.11206","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT found 7/8 genetic predictors, and estimates are reasonably close to truth. IHT missed one SNP with very small effect size (beta = 00369). The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# remove simulated data once they are no longer needed\nrm(\"sim.bed\", force=true)\nrm(\"sim.bim\", force=true)\nrm(\"sim.fam\", force=true)\nrm(\"sim.covariates.txt\", force=true)\nrm(\"sim.phenotypes.txt\", force=true)","category":"page"},{"location":"man/examples/#Example-4:-Running-IHT-on-general-matrices","page":"Examples","title":"Example 4: Running IHT on general matrices","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To run IHT on genotypes in VCF files, or other general data, one must call fit_iht and cv_iht directly. These functions are designed to work on AbstractArray{T, 2} type where T is a Float64 or Float32. Thus, one must first import the data, and then call fit_iht and cv_iht on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"tip: Tip\nCheck out VCFTools.jl to learn how to import VCF data.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate some count response using the model:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Poisson(mathbfx_i^T boldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Normal(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 03)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000             # number of samples\np = 10000            # number of SNPs\nk = 10               # 9 causal predictors + intercept\nd = Poisson          # Response follows Poisson distribution (count data)\nl = LogLink()        # canonical link\n\n# set random seed for reproducibility\nRandom.seed!(2020)\n\n# simulate design matrix\nx = randn(n, p)\n\n# simulate response, true model b, and the correct non-0 positions of b\ntrue_b = zeros(p)\ntrue_b[1:k] .= rand(Normal(0, 0.5), k)\nshuffle!(true_b)\nintercept = 1.0\ncorrect_position = findall(!iszero, true_b)\nprob = GLM.linkinv.(l, intercept .+ x * true_b)\nclamp!(prob, -20, 20) # prevents overflow\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # convert phenotypes to double precision","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Now we have the response y, design matrix x. Let's run IHT and compare with truth.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# first run cross validation \nmses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:15\u001b[39m\n\n\n\n\nCrossvalidation Results:\n\tk\tMSE\n\t1\t1489.8188363695676\n\t2\t707.6175237350031\n\t3\t546.8867981545659\n\t4\t467.2192708681082\n\t5\t440.03761893872735\n\t6\t459.9446241516855\n\t7\t482.3184687223138\n\t8\t504.0229684333779\n\t9\t495.12633308066677\n\t10\t525.4353275609003\n\t11\t534.0267905856207\n\t12\t524.761614819788\n\t13\t558.2726852255062\n\t14\t561.6025100531801\n\t15\t561.3898895087017\n\t16\t555.5897051455378\n\t17\t618.3872529214124\n\t18\t655.395210924614\n\t19\t652.4915677346956\n\t20\t561.4237250226572\n\nBest k = 5","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# run IHT on best k (achieved at k = 5)\nresult = fit_iht(y, x, k=argmin(mses), d=Poisson(), l=LogLink())","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.3.3                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse Poisson regression\nLink functin = LogLink()\nSparsity parameter (k) = 5\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001:\n\nIteration 1: loglikelihood = -2847.082501924986, backtracks = 0, tol = 0.2928574304111579\nIteration 2: loglikelihood = -2465.401434829009, backtracks = 0, tol = 0.05230999409875657\nIteration 3: loglikelihood = -2376.6519599956155, backtracks = 0, tol = 0.07104164424891486\nIteration 4: loglikelihood = -2351.5833133504116, backtracks = 0, tol = 0.026208176849564724\nIteration 5: loglikelihood = -2343.11078282251, backtracks = 0, tol = 0.02023001613423483\nIteration 6: loglikelihood = -2339.0692529047387, backtracks = 0, tol = 0.011080308351803736\nIteration 7: loglikelihood = -2337.149479249759, backtracks = 0, tol = 0.00930914236578197\nIteration 8: loglikelihood = -2336.1781402958745, backtracks = 0, tol = 0.00556416943618412\nIteration 9: loglikelihood = -2335.6908426969235, backtracks = 0, tol = 0.004609772037770406\nIteration 10: loglikelihood = -2335.440388139586, backtracks = 0, tol = 0.002861799512474864\nIteration 11: loglikelihood = -2335.3124548737906, backtracks = 0, tol = 0.002340881298705853\nIteration 12: loglikelihood = -2335.2463824561282, backtracks = 0, tol = 0.001479832987797599\nIteration 13: loglikelihood = -2335.2123851939327, backtracks = 0, tol = 0.0012011066579049358\nIteration 14: loglikelihood = -2335.1947979604856, backtracks = 0, tol = 0.0007661746047595665\nIteration 15: loglikelihood = -2335.1857186752313, backtracks = 0, tol = 0.0006191995770663082\nIteration 16: loglikelihood = -2335.1810189052294, backtracks = 0, tol = 0.00039678836835178535\nIteration 17: loglikelihood = -2335.178588840017, backtracks = 0, tol = 0.00031993814923964465\nIteration 18: loglikelihood = -2335.177330620363, backtracks = 0, tol = 0.00020549845888243352\nIteration 19: loglikelihood = -2335.1766795324024, backtracks = 0, tol = 0.00016549800033345948\nIteration 20: loglikelihood = -2335.176342377269, backtracks = 0, tol = 0.00010642830220001078\nIteration 21: loglikelihood = -2335.176167840737, backtracks = 0, tol = 8.565816720868677e-5\n\n\n\n\n\n\nIHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n\nCompute time (sec):     0.09447383880615234\nFinal loglikelihood:    -2335.176167840737\nSNP PVE:                0.09113449276174615\nIterations:             21\n\nSelected genetic predictors:\n\u001b[1m4×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │       83    -0.809284\n   2 │      989     0.378376\n   3 │     4294    -0.274544\n   4 │     4459     0.169417\n\nSelected nongenetic predictors:\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.26918","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# compare IHT result with truth\n[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"10×2 Array{Float64,2}:\n -1.303      -0.809284\n  0.585809    0.378376\n -0.0700563   0.0\n -0.0901341   0.0\n -0.0620201   0.0\n -0.441452   -0.274544\n  0.271429    0.169417\n -0.164888    0.0\n -0.0790484   0.0\n  0.0829054   0.0","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since many of the true beta are small, we were only able to find 5 true signals (4 predictors + intercept). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: In this example, we ran IHT on count response with a general Array{T, 2} design matrix. ","category":"page"},{"location":"man/examples/#Example-5:-Group-IHT","page":"Examples","title":"Example 5: Group IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most J = 5 groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter k is known. ","category":"page"},{"location":"man/examples/#Data-simulation","page":"Examples","title":"Data simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in our paper. In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; j = 5 distinct groups are each assigned 125 causal SNPs with effect sizes randomly chosen from 0202. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity lambda_g = 125. The remaining groups are assigned lambda_g = 1 (i.e. only 1 active predictor are allowed).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# define problem size\nd = NegativeBinomial\nl = LogLink()\nn = 1000\np = 10000\nblock_size = 20                  #simulation parameter\nnum_blocks = Int(p / block_size) #simulation parameter\n\n# set seed\nRandom.seed!(2019)\n\n# assign group membership\nmembership = collect(1:num_blocks)\ng = zeros(Int64, p + 1)\nfor i in 1:length(membership)\n    for j in 1:block_size\n        cur_row = block_size * (i - 1) + j\n        g[block_size*(i - 1) + j] = membership[i]\n    end\nend\ng[end] = membership[end]\n\n#simulate correlated snparray\nx = simulate_correlated_snparray(\"tmp.bed\", n, p)\nintercept = 0.5\nx_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n\n#simulate true model, where 5 groups each with 1~5 snps contribute\ntrue_b = zeros(p)\ntrue_groups = randperm(num_blocks)[1:5]\nsort!(true_groups)\nwithin_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n                randperm(block_size)[1:3], randperm(block_size)[1:4], \n                randperm(block_size)[1:5]]\ncorrect_position = zeros(Int64, 15)\nfor i in 1:5\n    cur_group = block_size * (true_groups[i] - 1)\n    cur_group_snps = cur_group .+ within_group[i]\n    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n    correct_position[start:last] .= cur_group_snps\nend\nfor i in 1:15\n    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\nend\nsort!(correct_position)\n\n# simulate phenotype\nr = 10 #nuisance parameter\nμ = GLM.linkinv.(l, intercept .+ x_float * true_b)\nclamp!(μ, -20, 20)\nprob = 1 ./ (1 .+ μ ./ r)\ny = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT without groups\nungrouped = fit_iht(y, x_float, k=15, d=NegativeBinomial(), l=LogLink(), verbose=false)\n\n#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\nmax_group_snps = ones(Int, num_blocks)\nmax_group_snps[true_groups] .= collect(1:5)\nvariable_group = fit_iht(y, x_float, d=NegativeBinomial(), l=LogLink(), k=max_group_snps, J=5, group=g, verbose=false);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#check result\ncorrect_position = findall(!iszero, true_b)\ncompare_model = DataFrame(\n    position = correct_position,\n    correct_β = true_b[correct_position],\n    ungrouped_IHT_β = ungrouped.beta[correct_position], \n    grouped_IHT_β = variable_group.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 15×4 DataFrame\n Row │ position  correct_β  ungrouped_IHT_β  grouped_IHT_β\n     │ Int64     Float64    Float64          Float64\n─────┼─────────────────────────────────────────────────────\n   1 │      235       -0.2        -0.218172       0.0\n   2 │     2673       -0.2        -0.171002      -0.178483\n   3 │     2679       -0.2        -0.236793      -0.213098\n   4 │     6383       -0.2        -0.228555      -0.224309\n   5 │     6389       -0.2        -0.190352      -0.192022\n   6 │     6394        0.2         0.215984       0.198447\n   7 │     7862        0.2         0.229254       0.224207\n   8 │     7864       -0.2        -0.184551      -0.19331\n   9 │     7868       -0.2        -0.174773      -0.177359\n  10 │     7870       -0.2        -0.192932      -0.208592\n  11 │     9481       -0.2         0.0            0.0\n  12 │     9491        0.2         0.0            0.0\n  13 │     9493        0.2         0.183659       0.175211\n  14 │     9494        0.2         0.117548       0.112946\n  15 │     9499       -0.2         0.0            0.0","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: Ungroup IHT actually found 1 more SNPs than grouped IHT. ","category":"page"},{"location":"man/examples/#Example-6:-Linear-Regression-with-prior-weights","page":"Examples","title":"Example 6: Linear Regression with prior weights","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include (predetermined) prior weights for each SNP. You can check out our paper for references of why/how to choose these weights. In this case, we mimic our paper and randomly set 10 of all SNPs to have a weight of 20. Other predictors have weight of 10. All causal SNPs have weights of 20. Under this scenario, SNPs with weight 20 is twice as likely to enter the model identified by IHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Our model is simulated as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim mathbfx_i^Tmathbfbeta + epsilon_i","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"epsilon_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 025)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"d = Normal\nl = IdentityLink()\nn = 1000\np = 10000\nk = 10\n\n#random seed\nRandom.seed!(4)\n\n# construct snpmatrix, covariate files, and true model b\nx = simulate_random_snparray(\"tmp.bed\", n, p)\nX = convert(Matrix{Float64}, x, center=true, scale=true)\nintercept = 1.0\n    \n#define true_b \ntrue_b = zeros(p)\ntrue_b[1:10] .= rand(Normal(0, 0.25), k)\nshuffle!(true_b)\ncorrect_position = findall(!iszero, true_b)\n\n#simulate phenotypes (e.g. vector y)\nprob = GLM.linkinv.(l, intercept .+ X * true_b)\nclamp!(prob, -20, 20)\ny = [rand(d(i)) for i in prob]\ny = Float64.(y);\n\n# construct weight vector\nw = ones(p + 1)\nw[correct_position] .= 2.0\none_tenth = round(Int, p/10)\nidx = rand(1:p, one_tenth)\nw[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run weighted and unweighted IHT\nunweighted = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false)\nweighted   = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false, weight=w)\n\n#check result\ncompare_model = DataFrame(\n    position    = correct_position,\n    correct     = true_b[correct_position],\n    unweighted  = unweighted.beta[correct_position], \n    weighted    = weighted.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×4 DataFrame\n Row │ position  correct     unweighted  weighted\n     │ Int64     Float64     Float64     Float64\n─────┼─────────────────────────────────────────────\n   1 │     1264   0.252886     0.270233   0.264713\n   2 │     1506  -0.0939841    0.0       -0.125803\n   3 │     4866  -0.227394    -0.233703  -0.237007\n   4 │     5778  -0.510488    -0.507114  -0.494199\n   5 │     5833  -0.311969    -0.324309  -0.322663\n   6 │     5956  -0.0548168    0.0        0.0\n   7 │     6378  -0.0155173    0.0        0.0\n   8 │     7007  -0.123301     0.0        0.0\n   9 │     7063   0.0183886    0.0        0.0\n  10 │     7995  -0.102122     0.0       -0.142201","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: weighted IHT found 2 extra predictor than non-weighted IHT.","category":"page"},{"location":"man/examples/#Other-examples-and-functionalities","page":"Examples","title":"Other examples and functionalities","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Additional features are available as optional parameters in the fit_iht function, but they should be treated as experimental features. Interested users are encouraged to explore them and please file issues on GitHub if you encounter a problem.","category":"page"},{"location":"#Mendel-Iterative-Hard-Thresholding","page":"Home","title":"Mendel - Iterative Hard Thresholding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A modern approach to analyze data from a Genome Wide Association Studies (GWAS)","category":"page"},{"location":"#Package-Feature","page":"Home","title":"Package Feature","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Analyze large GWAS datasets intuitively.\nBuilt-in support for PLINK binary files via SnpArrays.jl and VCF files via VCFTools.jl.\nOut-of-the-box parallel computing routines for q-fold cross-validation.\nFits a variety of generalized linear models with any choice of link function.\nComputation directly on raw genotype files.\nEfficient handlings for non-genetic covariates.\nOptional acceleration (debias) step to dramatically improve speed.\nAbility to explicitly incorporate weights for predictors.\nAbility to enforce within and between group sparsity. \nNaive genotype imputation. \nEstimates nuisance parameter for negative binomial regression using Newton or MM algorithm. \nExcellent flexibility to handle different data structures and complements well with other Julia packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read our paper for more detail.","category":"page"},{"location":"#Supported-GLM-models-and-Link-functions","page":"Home","title":"Supported GLM models and Link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MendelIHT borrows distribution and link functions implementationed in GLM.jl and Distributions.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distribution Canonical Link Status\nNormal IdentityLink checkmark\nBernoulli LogitLink checkmark\nPoisson LogLink checkmark\nNegativeBinomial LogLink checkmark\nGamma InverseLink experimental\nInverseGaussian InverseSquareLink experimental","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples of these distributions in their default value is visualized in this post.","category":"page"},{"location":"#Available-link-functions","page":"Home","title":"Available link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nProbitLink\nSqrtLink","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/getting_started.md\",\n    \"man/examples.md\",\n    \"man/math.md\",\n    \"man/contributing.md\",\n    \"man/api.md\",\n]\nDepth = 2","category":"page"},{"location":"man/api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"man/api/#Wrapper-Functions","page":"API","title":"Wrapper Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"Most users will use the following wrapper functions. The user only has to specify where the PLINK files (and possibly the phenotype/covariate files) are located. These functions will soon be updated to support VCF and BGEN formats.","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  iht\n  cross_validate","category":"page"},{"location":"man/api/#MendelIHT.iht","page":"API","title":"MendelIHT.iht","text":"iht(plinkfile, k, kwargs...)\n\nRuns IHT with sparsity level k. Example:\n\nresult = iht(\"plinkfile\", 10)\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. Current there should be NO missing phenotypes. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\ncol: Column of .fam file that stores phenotype. Can be integer (for    univariate analysis) or vector of integers (multivariate analysis).    Default is 6. \n\nAll arguments available in fit_iht\n\n\n\n\n\niht(plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, with additional covariates stored separately. Example:\n\nresult = iht(\"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit_iht\n\n\n\n\n\niht(phenotypes, plinkfile, covariates, k, kwargs...)\n\nRuns IHT with sparsity level k, where both phenotypes and additional covariates are stored separately. Example:\n\nresult = iht(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 10)\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\nk: An Int for sparsity parameter = number of none-zero coefficients\n\nOptional Arguments\n\nAll arguments available in fit_iht\n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.cross_validate","page":"API","title":"MendelIHT.cross_validate","text":"cross_validate(plinkfile, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", 1:20)\nmses = cross_validate(\"plinkfile\", [1, 2, 3, 4, 5]) # alternative syntax\n\nPhenotypes and other covariates\n\nWill use 6th column of .fam file for phenotype values and will automatically include an intercept as the only non-genetic covariate. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nWill use 6th column of .fam file for phenotype values. \n\nOther covariates\n\nCovariates are read using readdlm@ref function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\ncross_validate(phenotypes, plinkfile, covariates, path, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Sparsity levels is specified in `path. Example:\n\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", 1:20)\nmses = cross_validate(\"phenotypes.txt\", \"plinkfile\", \"covariates.txt\", [1, 10, 20]) # alternative syntax\n\nPhenotypes\n\nPhenotypes are read using readdlm function in Julia base. We require each  subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK. \n\nOther covariates\n\nCovariates are read using readdlm function in Julia base. We require the covariate file to be comma separated, and not include a header line. Each row should be listed in the same order as in the PLINK. The first column should be all 1s to indicate an intercept. All other columns will be standardized to mean 0 variance 1. \n\nArguments\n\nphenotypes: A String for phenotype file name\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\ncovariates: A String for covariate file name\npath: Different sparsity levels. Can be an integer range (default 1:20) or vector of integers. \n\nOptional Arguments\n\nAll arguments available in cv_iht\n\n\n\n\n\n","category":"function"},{"location":"man/api/#Core-Functions","page":"API","title":"Core Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"For advanced users, one can also run IHT regression or cross-validation directly. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  fit_iht\n  cv_iht","category":"page"},{"location":"man/api/#MendelIHT.fit_iht","page":"API","title":"MendelIHT.fit_iht","text":"fit_iht(y, x, z; kwargs...)\n\nFits a model on design matrix (genotype data) x, response (phenotype) y,  and non-genetic covariates z on a specific sparsity parameter k. If k is  a constant, then each group will have the same sparsity level. To run doubly  sparse IHT, construct k to be a vector where k[i] indicates the max number of predictors for group i. \n\nArguments:\n\ny: Phenotype vector or matrix. Should be an Array{T, 1} (single traits) or   Array{T, 2} (multivariate Gaussian traits). For multivariate traits, each    column of y should be a sample. \nx: Genotype matrix (an Array{T, 2} or SnpLinAlg). For univariate   analysis, samples are rows of x. For multivariate analysis, samples are   columns of x (i.e. input Transpose(x) for SnpLinAlg)\nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}.   For univariate analysis, sample covariates are rows of z. For multivariate   analysis, sample covariates are columns of z. If this is not specified, an   intercept term will be included automatically. If z is specified, make sure   the first column (row) is all 1s to represent the intercept. \n\nOptional Arguments:\n\nk: Number of non-zero predictors. Can be a constant or a vector (for group IHT). \nJ: The number of maximum groups (set as 1 if no group infomation available)\nd: Distribution of your phenotype (e.g. Normal, Bernoulli)\nl: A link function (e.g. IdentityLink, LogitLink, ProbitLink)\ngroup: vector storing group membership\nweight: vector storing vector of weights containing prior knowledge on each SNP\nest_r: Symbol (:MM, :Newton or :None) to estimate nuisance parameters for negative binomial regression\nuse_maf: boolean indicating whether we want to scale projection with minor allele frequencies (see paper)\ndebias: boolean indicating whether we debias at each iteration (only works for univariate models)\nverbose: boolean indicating whether we want to print intermediate results\ntol: used to track convergence\nmax_iter: is the maximum IHT iteration for a model to converge. Defaults to 200, or 100 for cross validation\nmax_step: is the maximum number of backtracking per IHT iteration. Defaults 5\n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.cv_iht","page":"API","title":"MendelIHT.cv_iht","text":"cv_iht(y, x, z; kwargs...)\n\nFor each model specified in path, performs q-fold cross validation and  returns the (averaged) deviance residuals. \n\nThe purpose of this function is to find the best sparsity level k, obtained from selecting the model with the minimum out-of-sample error. Different cross validation folds are cycled through sequentially different paths are fitted in parallel on different CPUs. Currently there are no routines to cross validate different group sizes. \n\nArguments:\n\ny: Phenotype vector or matrix. Should be an Array{T, 1} (single traits) or   Array{T, 2} (multivariate Gaussian traits). For multivariate traits, each    column of y should be a sample. \nx: Genotype matrix (an Array{T, 2} or SnpLinAlg). For univariate   analysis, samples are rows of x. For multivariate analysis, samples are   columns of x (i.e. input Transpose(x) for SnpLinAlg)\nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}.   For univariate analysis, sample covariates are rows of z. For multivariate   analysis, sample covariates are columns of z. If this is not specified, an   intercept term will be included automatically. If z is specified, make sure   the first column (row) is all 1s to represent the intercept. \n\nOptional Arguments:\n\npath: Different model sizes to be tested in cross validation (default 1:20)\nq: Number of cross validation folds. (default 5)\nd: Distribution of your phenotype. (default Normal)\nl: A link function (default IdentityLink)\nest_r: Symbol for whether to estimate nuisance parameters. Only supported distribution is negative binomial and choices include :Newton or :MM.\ngroup: vector storing group membership for each predictor\nweight: vector storing vector of weights containing prior knowledge on each predictor\nfolds: Vector that separates the sample into q disjoint subsets\ndestin: Directory where intermediate files will be generated. Directory name must end with /.\nuse_maf: Boolean indicating we should scale the projection step by a weight vector \ndebias: Boolean indicating whether we should debias at each IHT step\nverbose: Whether we want IHT to print meaningful intermediate steps\nparallel: Whether we want to run cv_iht using multiple CPUs (highly recommended)\n\n\n\n\n\n","category":"function"},{"location":"man/api/#Specifying-Groups-and-Weights","page":"API","title":"Specifying Groups and Weights","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"When you have group and weight information, you input them as optional arguments in fit_iht and cv_iht. The weight vector is a vector of Float64, while the group vector is a vector of Int. For instance,","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"    g = #import group vector\n    w = #import weight vector\n    ng = length(unique(g)) # specify number of non-zero groups\n    result = fit_iht(y, x, z; J=ng, k=10, d=Normal(), l=IdentityLink(), group=g, weight=w)","category":"page"},{"location":"man/api/#Simulation-Utilities","page":"API","title":"Simulation Utilities","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"For complex simulations, please use TraitSimulation.jl. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"MendelIHT provides very naive simulation utilities, which were written before TraitSimulation.jl was developed.","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  simulate_random_snparray\n  simulate_correlated_snparray","category":"page"},{"location":"man/api/#MendelIHT.simulate_random_snparray","page":"API","title":"MendelIHT.simulate_random_snparray","text":"simulate_random_snparray(s::String, n::Integer, p::Integer; \n    [mafs::Vector{Float64}], [min_ma::Integer])\n\nCreates a random SnpArray in the current directory without missing value,  where each SNP has ⫺5 (default) minor alleles. \n\nNote: if supplied minor allele frequency is extremely small, it could take a long time for the simulation to generate samples where at least min_ma (defaults to 5) are present. \n\nArguments:\n\ns: name of SnpArray that will be created in the current directory. To not   create file, use undef.\nn: number of samples\np: number of SNPs\n\nOptional Arguments:\n\nmafs: vector of desired minor allele freuqencies (uniform(0,0.5) by default)\nmin_ma: the minimum number of minor alleles that must be present for each   SNP (defaults to 5)\n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.simulate_correlated_snparray","page":"API","title":"MendelIHT.simulate_correlated_snparray","text":"simulate_correlated_snparray(s, n, p; block_length, hap, prob)\n\nSimulates a SnpArray with correlation. SNPs are divided into blocks where each adjacent SNP is the same with probability prob. There are no correlation between blocks.\n\nArguments:\n\nn: number of samples\np: number of SNPs\ns: name of SnpArray that will be created (memory mapped) in the current directory. To not memory map, use undef.\n\nOptional arguments:\n\nblock_length: length of each LD block\nhap: number of haplotypes to simulate for each block\nprob: with probability prob an adjacent SNP would be the same. \n\n\n\n\n\n","category":"function"},{"location":"man/api/","page":"API","title":"API","text":"note: Note\nSimulating a SnpArray with n subjects and p SNPs requires up to 2np bits of RAM. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  simulate_random_response","category":"page"},{"location":"man/api/#MendelIHT.simulate_random_response","page":"API","title":"MendelIHT.simulate_random_response","text":"simulate_random_response(x, k, d, l; kwargs...)\n\nThis function simulates a random response (trait) vector y. When the  distribution d is from Poisson, Gamma, or Negative Binomial, we simulate  β ∼ N(0, 0.3) to roughly ensure the mean of response y doesn't become too large. For other distributions, we choose β ∼ N(0, 1). \n\nArguments\n\nx: Design matrix\nk: the true number of predictors. \nd: The distribution of the simulated trait (note typeof(d) = UnionAll but typeof(d()) is an actual distribution: e.g. Normal)\nl: The link function. Input canonicallink(d()) if you want to use the canonical link of d.\n\nOptional arguments\n\nr: The number of success until stopping in negative binomial regression, defaults to 10\nα: Shape parameter of the gamma distribution, defaults to 1\nZu: Effect of non-genetic covariates. Zu should have dimension n × 1. \n\n\n\n\n\nsimulate_random_response(x, k, traits)\n\nSimulates a response matrix Y where each row is an independent multivariate Gaussian with length trait. There are k non-zero β over all traits. Each trait shares overlap causal SNPs. The covariance matrix Σ is positive definite and symmetric.\n\nArguments\n\nx: Design matrix of dimension n × p. Each row is a sample. \nk: the total true number of causal SNPs (predictors)\ntraits: Number of traits\n\nOptional arguments\n\nZu: Effect of non-genetic covariates. Zu should have dimension n × traits. \noverlap: Number of causal SNPs shared by all traits. Shared SNPs does not have the same effect size. \n\nOutputs\n\nY: Response matrix where each row is sampled from a multivariate normal with mean μ[i] = X[i, :] * true_b and variance Σ\nΣ: the symmetric, positive definite covariance matrix used\ntrue_b: A sparse matrix containing true beta values.\ncorrect_position: Non-zero indices of true_b\n\n\n\n\n\n","category":"function"},{"location":"man/api/","page":"API","title":"API","text":"note: Note\nFor negative binomial and gamma, the link function must be LogLink. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  make_bim_fam_files","category":"page"},{"location":"man/api/#MendelIHT.make_bim_fam_files","page":"API","title":"MendelIHT.make_bim_fam_files","text":"make_bim_fam_files(x::SnpArray, y, name::String)\n\nCreates .bim and .bed files from a SnpArray. \n\nArguments:\n\nx: A SnpArray (i.e. .bed file on the disk) for which you wish to create corresponding .bim and .fam files.\nname: string that should match the .bed file (Do not include .bim or .fam extensions in name).\ny: Trait vector that will go in to the 6th column of .fam file. \n\n\n\n\n\n","category":"function"},{"location":"man/api/#Other-Useful-Functions","page":"API","title":"Other Useful Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"MendelIHT additionally provides useful utilities that may be of interest to a few advanced users. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  iht_run_many_models\n  pve","category":"page"},{"location":"man/api/#MendelIHT.iht_run_many_models","page":"API","title":"MendelIHT.iht_run_many_models","text":"Runs IHT across many different model sizes specifed in path using the full design matrix. Same as cv_iht but DOES NOT validate in a holdout set, meaning that this will definitely induce overfitting as we increase model size. Use this if you want to quickly estimate a range of feasible model sizes before  engaging in full cross validation. \n\n\n\n\n\n","category":"function"}]
}

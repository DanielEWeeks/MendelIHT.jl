{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples\n",
    "\n",
    "Here we give numerous example analysis of GWAS data with `MendelIHT.jl`. \n",
    "\n",
    "Users are highly encouraged to read the source code of our main [fit_iht](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/fit.jl#L37) and [cv_iht](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/cross_validation.jl#L40) functions, which contain more options than what is described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.5.0\n",
      "Commit 96786e22cc (2020-08-01 23:44 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.7.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "# machine information for reproducibility\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add workers needed for parallel computing. Add only as many CPU cores available\n",
    "using Distributed\n",
    "# addprocs(4)\n",
    "\n",
    "#load necessary packages for running all examples below\n",
    "@everywhere begin\n",
    "    using Revise\n",
    "    using MendelIHT\n",
    "    using SnpArrays\n",
    "    using DataFrames\n",
    "    using Distributions\n",
    "    using Random\n",
    "    using LinearAlgebra\n",
    "    using GLM\n",
    "    using DelimitedFiles\n",
    "    using Statistics\n",
    "    using BenchmarkTools\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: GWAS with PLINK files\n",
    "\n",
    "In this example, our data are stored in binary PLINK files:\n",
    "\n",
    "+ `normal.bed`\n",
    "+ `normal.bim`\n",
    "+ `normal.fam`\n",
    "\n",
    "which contains simulated (Gaussian) phenotypes for $n=1000$ samples and $p=10,000$ SNPs. There are $8$ causal variants and 2 causal non-genetic covariates (intercept and sex). \n",
    "\n",
    "These data are present under `MendelIHT/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9-element Array{String,1}:\n",
       " \".DS_Store\"\n",
       " \"README.md\"\n",
       " \"covariates.txt\"\n",
       " \"normal.bed\"\n",
       " \"normal.bim\"\n",
       " \"normal.fam\"\n",
       " \"normal_true_beta.txt\"\n",
       " \"phenotypes.txt\"\n",
       " \"simulate.jl\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory to where example data is located\n",
    "cd(normpath(MendelIHT.datadir()))\n",
    "\n",
    "# show working directory\n",
    "@show pwd() \n",
    "\n",
    "# show files in current directory\n",
    "readdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `covariates.txt` contains non-genetic covariates, `normal.bed/bim/fam` are the PLINK files storing genetic covariates, `phenotypes.txt` are phenotypes for each sample, `normal_true_beta.txt` is the true statistical model used to generate the phenotypes, and `simulate.jl` is the script used to generate all the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run cross validation to determine best model size\n",
    "\n",
    "If phenotypes are stored in the `.fam` file and there are no other covariates (except for the intercept which is automatically included), one can run cross validation as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:30\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1408.1563764852363\n",
      "\t2\t859.2530853501157\n",
      "\t3\t675.489438272005\n",
      "\t4\t554.241640057255\n",
      "\t5\t457.7905801971809\n",
      "\t6\t393.4038368630614\n",
      "\t7\t340.4572103654293\n",
      "\t8\t308.14840394813007\n",
      "\t9\t313.19209629368044\n",
      "\t10\t315.4842259795911\n",
      "\t11\t324.98561588950076\n",
      "\t12\t325.9700128835669\n",
      "\t13\t328.5339681830119\n",
      "\t14\t330.3025490829286\n",
      "\t15\t326.8185639742826\n",
      "\t16\t343.98518919049127\n",
      "\t17\t335.8778323256304\n",
      "\t18\t340.69382555553784\n",
      "\t19\t345.50179778174197\n",
      "\t20\t346.9362994901397\n",
      "\n",
      "Best k = 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test k = 1, 2, ..., 20\n",
    "mses = cross_validate(\"normal\", 1:20);\n",
    "\n",
    "# Alternative syntax\n",
    "# mses = cross_validate(\"normal\", [1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n",
    "# mses = cross_validate(\"normal\", \"covariates.txt\", 1:20) # include additional covariates in separate file\n",
    "# mses = cross_validate(\"phenotypes.txt\", \"normal\", \"covariates.txt\", 1:20) # when phenotypes are stored separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not be alarmed if you get slightly different MSEs, because cross validation breaks data into training/testing randomly. Set a seed by `Random.seed!(1234)` if you want reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run IHT on best k\n",
    "\n",
    "According to cross validation, `k = 8` achieves the minimum MSE. Thus we run IHT on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.3.3                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse linear regression\n",
      "Link functin = IdentityLink()\n",
      "Sparsity parameter (k) = 8\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -1632.5890039523172, backtracks = 0, tol = 0.7845860052299409\n",
      "Iteration 2: loglikelihood = -1627.2942148015939, backtracks = 0, tol = 0.023580968682353067\n",
      "Iteration 3: loglikelihood = -1627.2793358038934, backtracks = 0, tol = 0.0015500765263876102\n",
      "Iteration 4: loglikelihood = -1627.2792456558277, backtracks = 0, tol = 0.00010521336604120053\n",
      "Iteration 5: loglikelihood = -1627.279244876156, backtracks = 0, tol = 8.430366413828275e-6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 7 nonzero SNP predictors and 1 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.03294205665588379\n",
       "Final loglikelihood:    -1627.279244876156\n",
       "SNP PVE:                0.8276388351471942\n",
       "Iterations:             5\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m7×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.411838\n",
       "   2 │     4246     0.572452\n",
       "   3 │     4717     0.909215\n",
       "   4 │     6290    -0.693302\n",
       "   5 │     7755    -0.54482\n",
       "   6 │     8375    -0.788884\n",
       "   7 │     9415    -2.15858\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m1×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.65223"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"normal\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we explicitly ran `cross_validate` and `iht` only with genetic data. The known non-genetic covariate `sex` is explicitly not included. They can be included using the alternative syntax (see Example 3)\n",
    "```julia\n",
    "mses = cross_validate(\"normal\", \"covariates.txt\", 1:20)\n",
    "result = iht(\"normal\", \"covariates.txt\", argmin(mses))\n",
    "```\n",
    "\n",
    "### Step 3: Examine results\n",
    "\n",
    "IHT picked 7 SNPs and the intercept as the 8 most significant predictor. Their position (the order in which the SNP appeared in the PLINK file) and estimated effect size are displayed. To extract more information (for instance to extract `rs` IDs), we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_snps = 7×6 DataFrame\n",
      " Row │ chromosome  snpid    genetic_distance  position  allele1  allele2\n",
      "     │ String      String   Float64           Int64     String   String\n",
      "─────┼───────────────────────────────────────────────────────────────────\n",
      "   1 │ 1           snp3137               0.0         1  1        2\n",
      "   2 │ 1           snp4246               0.0         1  1        2\n",
      "   3 │ 1           snp4717               0.0         1  1        2\n",
      "   4 │ 1           snp6290               0.0         1  1        2\n",
      "   5 │ 1           snp7755               0.0         1  1        2\n",
      "   6 │ 1           snp8375               0.0         1  1        2\n",
      "   7 │ 1           snp9415               0.0         1  1        2\n"
     ]
    }
   ],
   "source": [
    "snpdata = SnpData(\"normal\")                   # import PLINK information\n",
    "snps_idx = findall(!iszero, result.beta)      # indices of SNPs selected by IHT\n",
    "selected_snps = snpdata.snp_info[snps_idx, :] # see which SNPs are selected\n",
    "@show selected_snps;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above displays the SNP information for the selected SNPs. \n",
    "\n",
    "Since data is simulated, the fields `chromosome`, `snpid`, `genetic_distance`, `position`, `allele1`, and `allele2` are fake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: How to simulate data\n",
    "\n",
    "Here we demonstrate how to use `MendelIHT.jl` and [SnpArrays.jl](https://github.com/OpenMendel/SnpArrays.jl) to simulate data, allowing you to design your own genetic studies. Note:\n",
    "+ For more complex simulation, please use the module [TraitSimulations.jl](https://github.com/OpenMendel/TraitSimulation.jl).  \n",
    "+ All linear algebra routines involving PLINK files are handled by [SnpArrays.jl](https://github.com/OpenMendel/SnpArrays.jl). \n",
    "\n",
    "First we simulate an example PLINK trio (`.bim`, `.bed`, `.fam`) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:\n",
    "\n",
    "$$y_i \\sim \\rm Bernoulli(\\mathbf{x}_i^T\\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_{\\rm intercept} = 1$$\n",
    "$$\\beta_{\\rm sex} = 1.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000            # number of samples\n",
    "p = 10000           # number of SNPs\n",
    "k = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\n",
    "d = Bernoulli       # Binary (continuous) phenotypes\n",
    "l = LogitLink()     # canonical link function\n",
    "\n",
    "# set random seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "# simulate `sim.bed` file with no missing data\n",
    "x = simulate_random_snparray(\"sim.bed\", n, p)\n",
    "xla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n",
    "\n",
    "# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\n",
    "z = ones(n, 2) \n",
    "z[:, 2] .= rand(0:1, n)\n",
    "standardize!(@view(z[:, 2:end])) \n",
    "\n",
    "# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\n",
    "true_b = zeros(p) \n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "\n",
    "# find correct position of genetic predictors\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "# define effect size of non-genetic predictors: intercept & sex\n",
    "true_c = [1.0; 1.5] \n",
    "\n",
    "# simulate phenotype using genetic and nongenetic predictors\n",
    "prob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # turn y into floating point numbers\n",
    "\n",
    "# create `sim.bim` and `sim.bam` files using phenotype\n",
    "make_bim_fam_files(x, y, \"sim\")\n",
    "\n",
    "#save covariates and phenotypes (without header)\n",
    "writedlm(\"sim.covariates.txt\", z, ',')\n",
    "writedlm(\"sim.phenotypes.txt\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    Please **standardize** (or at least center) your non-genetic covariates. If you use our `iht()` or `cross_validation()` functions, standardization is automatic. For genotype matrix, `SnpLinAlg` efficiently achieves this standardization. For non-genetic covariates, please use the built-in function `standardize!`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logistic/Poisson/Negative-binomial GWAS\n",
    "\n",
    "In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true $k = 10$. Let's try running a logistic regression on this data. This is specified using keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.3.3                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse logistic regression\n",
      "Link functin = LogitLink()\n",
      "Sparsity parameter (k) = 10\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -403.91876912829696, backtracks = 0, tol = 0.588227446294744\n",
      "Iteration 2: loglikelihood = -354.2415736389379, backtracks = 0, tol = 0.2825138668458029\n",
      "Iteration 3: loglikelihood = -347.5483388154266, backtracks = 0, tol = 0.19289827584644767\n",
      "Iteration 4: loglikelihood = -335.97152474649437, backtracks = 0, tol = 0.14269962283934898\n",
      "Iteration 5: loglikelihood = -334.49756712078744, backtracks = 1, tol = 0.02283147714926755\n",
      "Iteration 6: loglikelihood = -333.5432195711081, backtracks = 2, tol = 0.019792429262652955\n",
      "Iteration 7: loglikelihood = -332.80672688543484, backtracks = 2, tol = 0.01984566493946018\n",
      "Iteration 8: loglikelihood = -332.5588563458224, backtracks = 3, tol = 0.00765066824120313\n",
      "Iteration 9: loglikelihood = -332.3619297572996, backtracks = 3, tol = 0.0069136917483499484\n",
      "Iteration 10: loglikelihood = -332.20642890616097, backtracks = 3, tol = 0.006159757548662302\n",
      "Iteration 11: loglikelihood = -332.0840431892422, backtracks = 3, tol = 0.0054730856932040705\n",
      "Iteration 12: loglikelihood = -331.988004167528, backtracks = 3, tol = 0.004854846133978431\n",
      "Iteration 13: loglikelihood = -331.912840859504, backtracks = 3, tol = 0.004300010671833966\n",
      "Iteration 14: loglikelihood = -331.8541573752894, backtracks = 3, tol = 0.0038033387186571527\n",
      "Iteration 15: loglikelihood = -331.8084401845103, backtracks = 3, tol = 0.0033597954021304085\n",
      "Iteration 16: loglikelihood = -331.7728941748384, backtracks = 3, tol = 0.002964587612723496\n",
      "Iteration 17: loglikelihood = -331.74530513071767, backtracks = 3, tol = 0.002613181520199698\n",
      "Iteration 18: loglikelihood = -331.72392566719304, backtracks = 3, tol = 0.0023013180213642273\n",
      "Iteration 19: loglikelihood = -331.707381516887, backtracks = 3, tol = 0.0020250247294297443\n",
      "Iteration 20: loglikelihood = -331.69459517397274, backtracks = 3, tol = 0.0017806229156774542\n",
      "Iteration 21: loglikelihood = -331.6847241325898, backtracks = 3, tol = 0.001564728733016127\n",
      "Iteration 22: loglikelihood = -331.6771112518333, backtracks = 3, tol = 0.0013742489121423228\n",
      "Iteration 23: loglikelihood = -331.6712450943089, backtracks = 3, tol = 0.0012063716814260338\n",
      "Iteration 24: loglikelihood = -331.6667283950377, backtracks = 3, tol = 0.0010585539268262742\n",
      "Iteration 25: loglikelihood = -331.6632531068335, backtracks = 3, tol = 0.0009285056582307363\n",
      "Iteration 26: loglikelihood = -331.6605807292455, backtracks = 3, tol = 0.0008141727680124564\n",
      "Iteration 27: loglikelihood = -331.6585268570941, backtracks = 3, tol = 0.0007137189219975596\n",
      "Iteration 28: loglikelihood = -331.6569490812431, backtracks = 3, tol = 0.0006255072563945026\n",
      "Iteration 29: loglikelihood = -331.65573754039684, backtracks = 3, tol = 0.000548082392516716\n",
      "Iteration 30: loglikelihood = -331.65480756089744, backtracks = 3, tol = 0.0004801531377076392\n",
      "Iteration 31: loglikelihood = -331.6540939353272, backtracks = 3, tol = 0.00042057612092363887\n",
      "Iteration 32: loglikelihood = -331.6535464833441, backtracks = 3, tol = 0.0003683405154479384\n",
      "Iteration 33: loglikelihood = -331.65312661305927, backtracks = 3, tol = 0.0003225539271646608\n",
      "Iteration 34: loglikelihood = -331.6528046612901, backtracks = 3, tol = 0.00028242947163362354\n",
      "Iteration 35: loglikelihood = -331.6525578388944, backtracks = 3, tol = 0.0002472740235281776\n",
      "Iteration 36: loglikelihood = -331.6523686452689, backtracks = 3, tol = 0.00021647759466681237\n",
      "Iteration 37: loglikelihood = -331.652223646102, backtracks = 3, tol = 0.00018950377910949889\n",
      "Iteration 38: loglikelihood = -331.6521125319519, backtracks = 3, tol = 0.00016588119325870547\n",
      "Iteration 39: loglikelihood = -331.65202739366003, backtracks = 3, tol = 0.0001451958337205726\n",
      "Iteration 40: loglikelihood = -331.65196216503983, backtracks = 3, tol = 0.0001270842743388486\n",
      "Iteration 41: loglikelihood = -331.65191219446064, backtracks = 3, tol = 0.00011122762514495095\n",
      "Iteration 42: loglikelihood = -331.65187391567315, backtracks = 3, tol = 9.734617909715456e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.30141711235046387\n",
       "Final loglikelihood:    -331.65187391567315\n",
       "SNP PVE:                0.4798854810844273\n",
       "Iterations:             42\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m8×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.503252\n",
       "   2 │     4246     0.590809\n",
       "   3 │     4248    -0.37987\n",
       "   4 │     4717     1.04006\n",
       "   5 │     6290    -0.741734\n",
       "   6 │     7755    -0.437585\n",
       "   7 │     8375    -0.942293\n",
       "   8 │     9415    -2.11206\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m2×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.03892\n",
       "   2 │        2      1.5844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"sim\", \"sim.covariates.txt\", 10, d=Bernoulli(), l=LogitLink())\n",
    "\n",
    "# other responses\n",
    "# result = iht(\"sim\", 10, d=Bernoulli(), l=ProbitLink())     # Logistic regression using ProbitLink\n",
    "# result = iht(\"sim\", 10, d=Poisson(), l=LogLink())          # Poisson regression using canonical link\n",
    "# result = iht(\"sim\", 10, d=NegativeBinomial(), l=LogLink()) # Negative Binomial regression using canonical link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is simulated, we can compare IHT's estimated effect size with the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×2 Array{Float64,2}:\n",
       "  0.469278    0.503252\n",
       "  0.554408    0.590809\n",
       "  0.923213    1.04006\n",
       "  0.0369732   0.0\n",
       " -0.625634   -0.741734\n",
       " -0.526553   -0.437585\n",
       " -0.815561   -0.942293\n",
       " -2.18271    -2.11206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHT found 7/8 genetic predictors, and estimates are reasonably close to truth. IHT missed one SNP with very small effect size ($\\beta = 0.0369$). The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove simulated data once they are no longer needed\n",
    "rm(\"sim.bed\", force=true)\n",
    "rm(\"sim.bim\", force=true)\n",
    "rm(\"sim.fam\", force=true)\n",
    "rm(\"sim.covariates.txt\", force=true)\n",
    "rm(\"sim.phenotypes.txt\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Running IHT on general matrices\n",
    "\n",
    "To run IHT on genotypes in VCF files, or other general data, one must call `fit_iht` and `cv_iht` directly. These functions are designed to work on `AbstractArray{T, 2}` type where `T` is a `Float64` or `Float32`. Thus, one must first import the data, and then call `fit_iht` and `cv_iht` on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    Check out [VCFTools.jl](https://github.com/OpenMendel/VCFTools.jl) to learn how to import VCF data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we simulate some count response using the model:\n",
    "\n",
    "$$y_i \\sim \\rm Poisson(\\mathbf{x}_i^T \\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Normal(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000             # number of samples\n",
    "p = 10000            # number of SNPs\n",
    "k = 10               # 9 causal predictors + intercept\n",
    "d = Poisson          # Response follows Poisson distribution (count data)\n",
    "l = LogLink()        # canonical link\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2020)\n",
    "\n",
    "# simulate design matrix\n",
    "x = randn(n, p)\n",
    "\n",
    "# simulate response, true model b, and the correct non-0 positions of b\n",
    "true_b = zeros(p)\n",
    "true_b[1:k] .= rand(Normal(0, 0.5), k)\n",
    "shuffle!(true_b)\n",
    "intercept = 1.0\n",
    "correct_position = findall(!iszero, true_b)\n",
    "prob = GLM.linkinv.(l, intercept .+ x * true_b)\n",
    "clamp!(prob, -20, 20) # prevents overflow\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # convert phenotypes to double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the response $y$, design matrix $x$. Let's run IHT and compare with truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:15\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1489.8188363695676\n",
      "\t2\t707.6175237350031\n",
      "\t3\t546.8867981545659\n",
      "\t4\t467.2192708681082\n",
      "\t5\t440.03761893872735\n",
      "\t6\t459.9446241516855\n",
      "\t7\t482.3184687223138\n",
      "\t8\t504.0229684333779\n",
      "\t9\t495.12633308066677\n",
      "\t10\t525.4353275609003\n",
      "\t11\t534.0267905856207\n",
      "\t12\t524.761614819788\n",
      "\t13\t558.2726852255062\n",
      "\t14\t561.6025100531801\n",
      "\t15\t561.3898895087017\n",
      "\t16\t555.5897051455378\n",
      "\t17\t618.3872529214124\n",
      "\t18\t655.395210924614\n",
      "\t19\t652.4915677346956\n",
      "\t20\t561.4237250226572\n",
      "\n",
      "Best k = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first run cross validation \n",
    "mses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.3.3                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse Poisson regression\n",
      "Link functin = LogLink()\n",
      "Sparsity parameter (k) = 5\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -2847.082501924986, backtracks = 0, tol = 0.2928574304111579\n",
      "Iteration 2: loglikelihood = -2465.401434829009, backtracks = 0, tol = 0.05230999409875657\n",
      "Iteration 3: loglikelihood = -2376.6519599956155, backtracks = 0, tol = 0.07104164424891486\n",
      "Iteration 4: loglikelihood = -2351.5833133504116, backtracks = 0, tol = 0.026208176849564724\n",
      "Iteration 5: loglikelihood = -2343.11078282251, backtracks = 0, tol = 0.02023001613423483\n",
      "Iteration 6: loglikelihood = -2339.0692529047387, backtracks = 0, tol = 0.011080308351803736\n",
      "Iteration 7: loglikelihood = -2337.149479249759, backtracks = 0, tol = 0.00930914236578197\n",
      "Iteration 8: loglikelihood = -2336.1781402958745, backtracks = 0, tol = 0.00556416943618412\n",
      "Iteration 9: loglikelihood = -2335.6908426969235, backtracks = 0, tol = 0.004609772037770406\n",
      "Iteration 10: loglikelihood = -2335.440388139586, backtracks = 0, tol = 0.002861799512474864\n",
      "Iteration 11: loglikelihood = -2335.3124548737906, backtracks = 0, tol = 0.002340881298705853\n",
      "Iteration 12: loglikelihood = -2335.2463824561282, backtracks = 0, tol = 0.001479832987797599\n",
      "Iteration 13: loglikelihood = -2335.2123851939327, backtracks = 0, tol = 0.0012011066579049358\n",
      "Iteration 14: loglikelihood = -2335.1947979604856, backtracks = 0, tol = 0.0007661746047595665\n",
      "Iteration 15: loglikelihood = -2335.1857186752313, backtracks = 0, tol = 0.0006191995770663082\n",
      "Iteration 16: loglikelihood = -2335.1810189052294, backtracks = 0, tol = 0.00039678836835178535\n",
      "Iteration 17: loglikelihood = -2335.178588840017, backtracks = 0, tol = 0.00031993814923964465\n",
      "Iteration 18: loglikelihood = -2335.177330620363, backtracks = 0, tol = 0.00020549845888243352\n",
      "Iteration 19: loglikelihood = -2335.1766795324024, backtracks = 0, tol = 0.00016549800033345948\n",
      "Iteration 20: loglikelihood = -2335.176342377269, backtracks = 0, tol = 0.00010642830220001078\n",
      "Iteration 21: loglikelihood = -2335.176167840737, backtracks = 0, tol = 8.565816720868677e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.09447383880615234\n",
       "Final loglikelihood:    -2335.176167840737\n",
       "SNP PVE:                0.09113449276174615\n",
       "Iterations:             21\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m4×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │       83    -0.809284\n",
       "   2 │      989     0.378376\n",
       "   3 │     4294    -0.274544\n",
       "   4 │     4459     0.169417\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m1×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.26918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run IHT on best k (achieved at k = 5)\n",
    "result = fit_iht(y, x, k=argmin(mses), d=Poisson(), l=LogLink())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       " -1.303      -0.809284\n",
       "  0.585809    0.378376\n",
       " -0.0700563   0.0\n",
       " -0.0901341   0.0\n",
       " -0.0620201   0.0\n",
       " -0.441452   -0.274544\n",
       "  0.271429    0.169417\n",
       " -0.164888    0.0\n",
       " -0.0790484   0.0\n",
       "  0.0829054   0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare IHT result with truth\n",
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many of the true $\\beta$ are small, we were only able to find 5 true signals (4 predictors + intercept). \n",
    "\n",
    "**Conclusion:** In this example, we ran IHT on count response with a general `Array{T, 2}` design matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Group IHT \n",
    "\n",
    "In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most $J = 5$ groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter $k$ is known. \n",
    "\n",
    "### Data simulation\n",
    "To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf). In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; $j = 5$ distinct groups are each assigned $1,2,...,5$ causal SNPs with effect sizes randomly chosen from $\\{−0.2,0.2\\}$. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity $\\lambda_g = 1,2,...,5$. The remaining groups are assigned $\\lambda_g = 1$ (i.e. only 1 active predictor are allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define problem size\n",
    "d = NegativeBinomial\n",
    "l = LogLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "block_size = 20                  #simulation parameter\n",
    "num_blocks = Int(p / block_size) #simulation parameter\n",
    "\n",
    "# set seed\n",
    "Random.seed!(2019)\n",
    "\n",
    "# assign group membership\n",
    "membership = collect(1:num_blocks)\n",
    "g = zeros(Int64, p + 1)\n",
    "for i in 1:length(membership)\n",
    "    for j in 1:block_size\n",
    "        cur_row = block_size * (i - 1) + j\n",
    "        g[block_size*(i - 1) + j] = membership[i]\n",
    "    end\n",
    "end\n",
    "g[end] = membership[end]\n",
    "\n",
    "#simulate correlated snparray\n",
    "x = simulate_correlated_snparray(\"tmp.bed\", n, p)\n",
    "intercept = 0.5\n",
    "x_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n",
    "\n",
    "#simulate true model, where 5 groups each with 1~5 snps contribute\n",
    "true_b = zeros(p)\n",
    "true_groups = randperm(num_blocks)[1:5]\n",
    "sort!(true_groups)\n",
    "within_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n",
    "                randperm(block_size)[1:3], randperm(block_size)[1:4], \n",
    "                randperm(block_size)[1:5]]\n",
    "correct_position = zeros(Int64, 15)\n",
    "for i in 1:5\n",
    "    cur_group = block_size * (true_groups[i] - 1)\n",
    "    cur_group_snps = cur_group .+ within_group[i]\n",
    "    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n",
    "    correct_position[start:last] .= cur_group_snps\n",
    "end\n",
    "for i in 1:15\n",
    "    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\n",
    "end\n",
    "sort!(correct_position)\n",
    "\n",
    "# simulate phenotype\n",
    "r = 10 #nuisance parameter\n",
    "μ = GLM.linkinv.(l, intercept .+ x_float * true_b)\n",
    "clamp!(μ, -20, 20)\n",
    "prob = 1 ./ (1 .+ μ ./ r)\n",
    "y = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\n",
    "y = Float64.(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run IHT without groups\n",
    "ungrouped = fit_iht(y, x_float, k=15, d=NegativeBinomial(), l=LogLink(), verbose=false)\n",
    "\n",
    "#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\n",
    "max_group_snps = ones(Int, num_blocks)\n",
    "max_group_snps[true_groups] .= collect(1:5)\n",
    "variable_group = fit_iht(y, x_float, d=NegativeBinomial(), l=LogLink(), k=max_group_snps, J=5, group=g, verbose=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 15×4 DataFrame\n",
      " Row │ position  correct_β  ungrouped_IHT_β  grouped_IHT_β\n",
      "     │ Int64     Float64    Float64          Float64\n",
      "─────┼─────────────────────────────────────────────────────\n",
      "   1 │      235       -0.2        -0.218172       0.0\n",
      "   2 │     2673       -0.2        -0.171002      -0.178483\n",
      "   3 │     2679       -0.2        -0.236793      -0.213098\n",
      "   4 │     6383       -0.2        -0.228555      -0.224309\n",
      "   5 │     6389       -0.2        -0.190352      -0.192022\n",
      "   6 │     6394        0.2         0.215984       0.198447\n",
      "   7 │     7862        0.2         0.229254       0.224207\n",
      "   8 │     7864       -0.2        -0.184551      -0.19331\n",
      "   9 │     7868       -0.2        -0.174773      -0.177359\n",
      "  10 │     7870       -0.2        -0.192932      -0.208592\n",
      "  11 │     9481       -0.2         0.0            0.0\n",
      "  12 │     9491        0.2         0.0            0.0\n",
      "  13 │     9493        0.2         0.183659       0.175211\n",
      "  14 │     9494        0.2         0.117548       0.112946\n",
      "  15 │     9499       -0.2         0.0            0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check result\n",
    "correct_position = findall(!iszero, true_b)\n",
    "compare_model = DataFrame(\n",
    "    position = correct_position,\n",
    "    correct_β = true_b[correct_position],\n",
    "    ungrouped_IHT_β = ungrouped.beta[correct_position], \n",
    "    grouped_IHT_β = variable_group.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Ungroup IHT actually found 1 more SNPs than grouped IHT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Linear Regression with prior weights\n",
    "\n",
    "In this example, we show how to include (predetermined) prior weights for each SNP. You can check out [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf) for references of why/how to choose these weights. In this case, we mimic our paper and randomly set $10\\%$ of all SNPs to have a weight of $2.0$. Other predictors have weight of $1.0$. All causal SNPs have weights of $2.0$. Under this scenario, SNPs with weight $2.0$ is twice as likely to enter the model identified by IHT. \n",
    "\n",
    "Our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\epsilon_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.25)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Normal\n",
    "l = IdentityLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "\n",
    "#random seed\n",
    "Random.seed!(4)\n",
    "\n",
    "# construct snpmatrix, covariate files, and true model b\n",
    "x = simulate_random_snparray(\"tmp.bed\", n, p)\n",
    "X = convert(Matrix{Float64}, x, center=true, scale=true)\n",
    "intercept = 1.0\n",
    "    \n",
    "#define true_b \n",
    "true_b = zeros(p)\n",
    "true_b[1:10] .= rand(Normal(0, 0.25), k)\n",
    "shuffle!(true_b)\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "#simulate phenotypes (e.g. vector y)\n",
    "prob = GLM.linkinv.(l, intercept .+ X * true_b)\n",
    "clamp!(prob, -20, 20)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y);\n",
    "\n",
    "# construct weight vector\n",
    "w = ones(p + 1)\n",
    "w[correct_position] .= 2.0\n",
    "one_tenth = round(Int, p/10)\n",
    "idx = rand(1:p, one_tenth)\n",
    "w[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      " Row │ position  correct     unweighted  weighted\n",
      "     │ Int64     Float64     Float64     Float64\n",
      "─────┼─────────────────────────────────────────────\n",
      "   1 │     1264   0.252886     0.270233   0.264713\n",
      "   2 │     1506  -0.0939841    0.0       -0.125803\n",
      "   3 │     4866  -0.227394    -0.233703  -0.237007\n",
      "   4 │     5778  -0.510488    -0.507114  -0.494199\n",
      "   5 │     5833  -0.311969    -0.324309  -0.322663\n",
      "   6 │     5956  -0.0548168    0.0        0.0\n",
      "   7 │     6378  -0.0155173    0.0        0.0\n",
      "   8 │     7007  -0.123301     0.0        0.0\n",
      "   9 │     7063   0.0183886    0.0        0.0\n",
      "  10 │     7995  -0.102122     0.0       -0.142201\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run weighted and unweighted IHT\n",
    "unweighted = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false)\n",
    "weighted   = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false, weight=w)\n",
    "\n",
    "#check result\n",
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    correct     = true_b[correct_position],\n",
    "    unweighted  = unweighted.beta[correct_position], \n",
    "    weighted    = weighted.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: weighted IHT found 2 extra predictor than non-weighted IHT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples and functionalities\n",
    "\n",
    "Additional features are available as optional parameters in the [fit_iht](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/fit.jl#L37) function, but they should be treated as **experimental** features. Interested users are encouraged to explore them and please file issues on GitHub if you encounter a problem."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mathematical Details · MendelIHT</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MendelIHT</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Mathematical Details</a><ul class="internal"><li><a class="tocitem" href="#Generalized-linear-models"><span>Generalized linear models</span></a></li><li><a class="tocitem" href="#Implementation-details-of-loglikelihood,-gradient,-and-expected-information"><span>Implementation details of loglikelihood, gradient, and expected information</span></a></li><li><a class="tocitem" href="#Iterative-hard-thresholding"><span>Iterative hard thresholding</span></a></li><li><a class="tocitem" href="#Nuisance-parameter-estimation"><span>Nuisance parameter estimation</span></a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Mathematical Details</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mathematical Details</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/OpenMendel/MendelIHT.jl/blob/master/docs/src/man/math.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Details-of-Parameter-Estimation"><a class="docs-heading-anchor" href="#Details-of-Parameter-Estimation">Details of Parameter Estimation</a><a id="Details-of-Parameter-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Details-of-Parameter-Estimation" title="Permalink"></a></h1><p>This note is meant to supplement our <a href="https://doi.org/10.1093/gigascience/giaa044">paper</a>. For a review on generalized linear models, I recommend chapter 15.3 of <a href="https://www.amazon.com/Applied-Regression-Analysis-Generalized-Linear/dp/1452205663/ref=sr_1_2?dchild=1&amp;keywords=Applied+Regression+Analysis+and+Generalized+Linear+Models&amp;qid=1609298891&amp;s=books&amp;sr=1-2">Applied regression analysis and generalized linear models</a> by John Fox, or chapter 3-5 of <a href="https://www.amazon.com/Introduction-Generalized-Chapman-Statistical-Science/dp/1138741515/ref=sr_1_2?crid=18BN4MONNYYJH&amp;dchild=1&amp;keywords=an+introduction+to+generalized+linear+models&amp;qid=1609298924&amp;s=books&amp;sprefix=an+introduction+to+ge%2Cstripbooks%2C222&amp;sr=1-2">An introduction to generalized linear models</a> by Dobson and Barnett. </p><h2 id="Generalized-linear-models"><a class="docs-heading-anchor" href="#Generalized-linear-models">Generalized linear models</a><a id="Generalized-linear-models-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-linear-models" title="Permalink"></a></h2><p>In <code>MendelIHT.jl</code>, phenotypes <span>$(\bf y)$</span> are modeled as a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a>: \begin{aligned}     \mu<em>i = E(y</em>i) = g({\bf x}<em>i^t {\boldsymbol \beta}) \end{aligned} where <span>$\bf x$</span> is sample <span>$i$</span>&#39;s <span>$p$</span>-dimensional vector of <em>covariates</em> (genotypes + other fixed effects), <span>$\boldsymbol \beta$</span> is a <span>$p$</span>-dimensional regression coefficients, <span>$g$</span> is a non-linear <em>inverse-link</em> function, y</em>i$ is sample <span>$i$</span>&#39;s phenotype value, and <span>$\mu_i$</span> is the <em>average predicted value</em> of <span>$y_i$</span> given <span>$\bf x$</span>. </p><p>The regression coefficients <span>$\boldsymbol \beta$</span> are not observed and are estimated via <strong>maximum likelihood</strong>. The full design matrix <span>$\bf X$</span> (obtained by stacking each <span>${\bf x}_i^t$</span> row-by-row) and phenotypes <span>$\bf y$</span> are observed. </p><p>GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases. Of course, when <span>$g(\alpha) = \alpha,$</span> we get the standard linear model used for Gaussian phenotypes. </p><h2 id="Implementation-details-of-loglikelihood,-gradient,-and-expected-information"><a class="docs-heading-anchor" href="#Implementation-details-of-loglikelihood,-gradient,-and-expected-information">Implementation details of loglikelihood, gradient, and expected information</a><a id="Implementation-details-of-loglikelihood,-gradient,-and-expected-information-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-details-of-loglikelihood,-gradient,-and-expected-information" title="Permalink"></a></h2><p>In GLM, the distribution of <span>$\bf y$</span> is from the exponential family with density</p><p class="math-container">\[f(y \mid \theta, \phi) = \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right].\]</p><p class="math-container">\[\theta\]</p><p>is called the <strong>canonical (location) parameter</strong> and under the canonical link, <span>$\theta = g(\bf x^t \bf \beta)$</span>. <span>$\phi$</span> is the <strong>dispersion (scale) parameter</strong>. The functions <span>$a, b, c$</span> are known functions that vary depending on the distribution of <span>$y$</span>. </p><p>Given <span>$n$</span> independent observations, the loglikelihood is:</p><p>\begin{aligned}     L({\bf \theta}, \phi; {\bf y}) &amp;= \sum<em>{i=1}^n \frac{y</em>i\theta<em>i - b(\theta</em>i)}{a<em>i(\phi)} + c(y</em>i, \phi). \end{aligned}</p><p>To evaluate the loglikelihood, we use the <a href="https://juliastats.org/Distributions.jl/latest/univariate/#Distributions.logpdf-Tuple{Distribution{Univariate,S}%20where%20S%3C:ValueSupport,Real}">logpdf</a> function in <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>.</p><p>The perform maximum likelihood estimation, we compute partial derivatives for <span>$\beta$</span>s. The <span>$j$</span>th score component is (eq 4.18 in Dobson):</p><p>\begin{aligned}     \frac{\partial L}{\partial \beta<em>j} = \sum</em>{i=1}^n \left[\frac{y<em>i - \mu</em>i}{var(y<em>i)}x</em>{ij}\left(\frac{\partial \mu<em>i}{\partial \eta</em>i}\right)\right]. \end{aligned}</p><p>Thus the full <strong>gradient</strong> is</p><p>\begin{aligned}     \nabla L&amp;= {\bf X}^t{\bf W}({\bf y} - \boldsymbol\mu), \quad {\bf W}<em>{ii} = \frac{1}{var(y</em>i)}\left(\frac{\partial \mu<em>i}{\partial \eta</em>i}\right), \end{aligned}</p><p>and similarly, the <strong>expected information</strong> is (eq 4.23 in Dobson):</p><p>\begin{aligned}     J = {\bf X^t\tilde{W}X}, \quad {\bf \tilde{W}}<em>{ii} = \frac{1}{var(y</em>i)}\left(\frac{\partial \mu<em>i}{\partial \eta</em>i}\right)^2 \end{aligned}</p><p>To evaluate <span>$\nabla L$</span> and <span>$J$</span>, note <span>${\bf y}$</span> and <span>${\bf X}$</span> are known, so we just need to calculate <span>$\boldsymbol\mu, \frac{\partial\mu_i}{\partial\eta_i},$</span> and <span>$var(y_i)$</span>. The first simply uses the inverse link: <span>$\mu_i = g({\bf x}_i^t {\boldsymbol \beta})$</span>. For the second, note <span>$\frac{\partial \mu_i}{\partial\eta_i} = \frac{\partial g({\bf x}_i^t {\boldsymbol \beta})}{\partial{\bf x}_i^t {\boldsymbol \beta}}$</span> is just the derivative of the link function evaluated at the linear predictor <span>$\eta_i = {\bf x}_i^t {\boldsymbol \beta}$</span>. This is already implemented for various link functions as <a href="https://github.com/JuliaStats/GLM.jl/blob/master/src/glmtools.jl#L149">mueta</a> in <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a>, which we call internally. To compute <span>$var(y_i)$</span>, we note that the exponential family distributions have variance</p><p>\begin{aligned}     var(y) &amp;= a(\phi)b&#39;&#39;(\theta) = a(\phi)\frac{\partial^2b(\theta)}{\partial\theta} = a(\phi) var(\mu). \end{aligned}</p><p>That is, <span>$var(y_i)$</span> is a product of 2 terms where the first depends solely on <span>$\phi$</span>, and the second solely on <span>$\mu = g({\bf x}_i^t {\boldsymbol \beta})$</span>. In our code, we use <a href="https://github.com/JuliaStats/GLM.jl/blob/master/src/glmtools.jl#L315">glmvar</a> implemented in <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a> to calculate <span>$var(\mu)$</span>. Because <span>$\phi$</span> is unknown, we assume <span>$a(\phi) = 1$</span> for all models except the negative binomial model. For negative binomial model, we discuss how to estimate <span>$\phi$</span> and <span>$\boldsymbol\beta$</span> using alternate block descent below.  </p><h2 id="Iterative-hard-thresholding"><a class="docs-heading-anchor" href="#Iterative-hard-thresholding">Iterative hard thresholding</a><a id="Iterative-hard-thresholding-1"></a><a class="docs-heading-anchor-permalink" href="#Iterative-hard-thresholding" title="Permalink"></a></h2><p>In <code>MendelIHT.jl</code>, the loglikelihood is maximized using iterative hard thresholding. This is achieved by</p><p>\begin{aligned}     \boldsymbol\beta<em>{n+1} = \overbrace{P</em>{S<em>k}}^{(3)}\big(\boldsymbol\beta</em>n - \underbrace{s<em>n}</em>{(2)} \overbrace{\nabla f(\boldsymbol\beta_n)}^{(1)}\big) \end{aligned}</p><p>where <span>$f$</span> is the function to minimize (i.e. negative loglikelihood), <span>$s_k$</span> is the step size, and <span>$P_{S_k}$</span> is a projection operator that sets all but <span>$k$</span> largest entries in magnitude to <span>$0$</span>. I already discussed above how to compute the gradient of a GLM loglikelihood. To perform <span>$P_{S_k}$</span>, we first partially sort the <em>dense</em> vector <span>$\beta_n - s_n \nabla f(\beta_n)$</span>, and set all <span>$k+1 ... n$</span> entries to <span>$0$</span>. Finally, the step size <span>$s_n$</span> is derived in our paper to be</p><p>\begin{aligned}     s<em>n = \frac{||\nabla f(\boldsymbol\beta</em>n)||<em>2^2}{\nabla f(\boldsymbol\beta</em>n)^t J(\boldsymbol\beta<em>n) \nabla f(\boldsymbol\beta</em>n)} \end{aligned}</p><p>where <span>$J = {\bf X^t\tilde{W}X}$</span> is the expected information matrix (derived in the previous section) <strong>which should never be explicitly formed</strong>. To evaluate the denominator, observe that </p><p>\begin{aligned}     \nabla f(\boldsymbol\beta<em>n)^t J(\boldsymbol\beta</em>n) \nabla f(\boldsymbol\beta<em>n) = \left(\nabla f(\boldsymbol\beta</em>n)^t{\bf X}^t \sqrt(\tilde{W})\right)\left(\sqrt(\tilde{W}){\bf X}\nabla f(\boldsymbol\beta_n)\right). \end{aligned}</p><p>Thus one computes <span>${\bf v} = \sqrt(\tilde{W}){\bf X}\nabla f(\boldsymbol\beta_n)$</span> and calculate its inner product with itself. </p><h2 id="Nuisance-parameter-estimation"><a class="docs-heading-anchor" href="#Nuisance-parameter-estimation">Nuisance parameter estimation</a><a id="Nuisance-parameter-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Nuisance-parameter-estimation" title="Permalink"></a></h2><p>Currently <code>MendelIHT.jl</code> only estimates nuisance parameter for the Negative Binomial model. This feature is provided by our <a href="https://qcb.ucla.edu/big-summer/big2019-2/">2019 Bruins in Genomics</a> summer student <a href="https://github.com/viviangarcia">Vivian Garcia</a> and <a href="https://github.com/fadusei">Francis Adusei</a>. </p><p>Note for Gaussian response, one can use the sample variance formula to estimate <span>$\phi$</span> from the estimated mean <span>$\hat{\mu}$</span>. </p><h3 id="Parametrization-for-Negative-Binomial-model"><a class="docs-heading-anchor" href="#Parametrization-for-Negative-Binomial-model">Parametrization for Negative Binomial model</a><a id="Parametrization-for-Negative-Binomial-model-1"></a><a class="docs-heading-anchor-permalink" href="#Parametrization-for-Negative-Binomial-model" title="Permalink"></a></h3><p>The negative binomial distribution has density  \begin{aligned} 	P(Y = y) = \binom{y+r-1}{y}p^r(1-p)^y \end{aligned} where <span>$y$</span> is the number of failures before the <span>$r$</span>th success and <span>$p$</span> is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to <a href="https://reference.wolfram.com/language/ref/NegativeBinomialDistribution.html">WOLFRAM</a> is  \begin{align<em>} 	\mu<em>i = \frac{r(1-p</em>i)}{r}, \quad 	Var(y<em>i) = \frac{r(1-p</em>i)}{p_i^2}. \end{align</em>} Note these formula are different than the default on <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">wikipedia</a> because in wiki <span>$y$</span> is the number of <em>success</em> and <span>$r$</span> is the number of <em>failure</em>.  Therefore, solving for <span>$p_i$</span>, we have  \begin{aligned} 	p<em>i = \frac{r}{\mu</em>i + r} = \frac{r}{e^{\mathbf{x}<em>i^T\beta} + r} \in (0, 1). \end{aligned} And indeed this this is how we <a href="https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/utilities.jl#L41">parametrize the negative binomial model</a>. **Importantly, we can interpret p</em>i$ as a probability**, since <span>$\mathbf{x}_i^T\beta$</span> can take on any number between <span>$-\infty$</span> and <span>$+\infty$</span> (since <span>$\beta$</span> and <span>$\mathbf{x}_i$</span> can have positive and negative entries), so <span>$exp(\mathbf{x}_i^T\beta)\in(0, \infty)$</span>.</p><p>We can also try to express <span>$Var(y_i)$</span> in terms of <span>$\mu_i$</span> and <span>$r$</span> by doing some algebra: \begin{aligned} 	Var(y<em>i) 	&amp;= \frac{r(1-p</em>i)}{p<em>i^2} = \frac{r\left( 1 - \frac{r}{\mu</em>i + r} \right)}{\frac{r^2}{(\mu<em>i + r)^2}} = \frac{1}{r}\left(1 - \frac{r}{\mu</em>i + r}\right)(\mu<em>i + r)^2 \
	&amp;= \frac{1}{r} \left[ (\mu</em>i + r)^2 - r(\mu<em>r + r) \right] = \frac{1}{r}(\mu</em>i + r)\mu<em>i\
	&amp;= \mu</em>i \left( \frac{\mu_i}{r} + 1 \right) \end{aligned} You can verify <a href="https://github.com/JuliaStats/GLM.jl/blob/ef246bb8fdbfa3f3058435035d0b0cf42abdd06e/src/glmtools.jl#L320">in GLM.jl</a> that this is indeed how they compute the variance of a negative binomial distribution. </p><h3 id="Estimating-nuisance-parameter-using-MM-algorithms"><a class="docs-heading-anchor" href="#Estimating-nuisance-parameter-using-MM-algorithms">Estimating nuisance parameter using MM algorithms</a><a id="Estimating-nuisance-parameter-using-MM-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-nuisance-parameter-using-MM-algorithms" title="Permalink"></a></h3><p>The MM algorithm is very stable, but converges much slower than Newton&#39;s alogorithm below. Thus use MM only if Newton&#39;s method fails.</p><p>The loglikelihood for <span>$n$</span> independent samples under a Negative Binomial model is  \begin{aligned} 	L(p<em>1, ..., p</em>m, r) 	&amp;= \sum<em>{i=1}^m \ln \binom{y</em>i+r-1}{y<em>i} + r\ln(p</em>i) + y<em>i\ln(1-p</em>i)\
	&amp;= \sum<em>{i=1}^m \left[ \sum</em>{j=0}^{y<em>i - 1} \ln(r+j) + r\ln(p</em>i) - \ln(y<em>i!) + y</em>i\ln(1-p<em>i) \right]\
	&amp;\geq \sum</em>{i=1}^m\left[ \sum<em>{j=0}^{y</em>i-1}\frac{r<em>n}{r</em>n+j}\ln(r) + c<em>n + r\ln(p</em>i) - \ln(y<em>i!) + y</em>i \ln(1-p_i) \right] \end{aligned}</p><p>The last inequality can be seen by applying Jensen&#39;s inequality: \begin{align<em>} 	f\left[ \sum<em>{i}u</em>i(\boldsymbol\theta)\right] \leq \sum<em>{i} \frac{u</em>i(\boldsymbol\theta<em>n}{\sum</em>j u<em>j(\boldsymbol\theta</em>n)}f \left[ \frac{\sum<em>j u</em>j(\boldsymbol\theta<em>n)}{u</em>i(\boldsymbol\theta<em>n)} u</em>i(\boldsymbol\theta)\right] \end{align</em>} to the function <span>$f(u) = - \ln(u).$</span> Maximizing <span>$L$</span> over <span>$r$</span> (i.e. differentiating with respect to <span>$r$</span> and setting equal to zero, then solving for <span>$r$</span>), we have \begin{align<em>} 	\frac{d}{dr} L(p<em>1,...,p</em>m,r)  	&amp;= \sum<em>{i=1}^{m} \left[ \sum</em>{j=0}^{y<em>i-1} \frac{r</em>n}{r<em>n + j} \frac{1}{r} + \ln(p</em>i) \right] \
	&amp;= \sum<em>{i=1}^{m}\sum</em>{j=0}^{y<em>i-1} \frac{r</em>n}{r<em>n + j} \frac{1}{r} + \sum</em>{i=1}^{m}\ln(p<em>i)\  	&amp;\equiv 0\
	\iff r</em>{n+1} &amp;= \frac{-\sum<em>{i=1}^{m}\sum</em>{j=0}^{y<em>i-1} \frac{r</em>n}{r<em>n + j}}{\sum</em>{i=1}^{m}\ln(p_i) }  \end{align</em>}</p><h3 id="Estimating-Nuisance-parameter-using-Newton&#39;s-method"><a class="docs-heading-anchor" href="#Estimating-Nuisance-parameter-using-Newton&#39;s-method">Estimating Nuisance parameter using Newton&#39;s method</a><a id="Estimating-Nuisance-parameter-using-Newton&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-Nuisance-parameter-using-Newton&#39;s-method" title="Permalink"></a></h3><p>Since we are dealing with 1 parameter optimization, Newton&#39;s method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (<span>$r$</span>), we use maximum likelihood estimates. By <span>$p_i = r / (\mu_i + r)$</span> in above, we have \begin{aligned} 	&amp; L(p<em>1, ..., p</em>m, r)\
	=&amp; \sum<em>{i=1}^m \ln \binom{y</em>i+r-1}{y<em>i} + r\ln(p</em>i) + y<em>i\ln(1-p</em>i)\
	=&amp; \sum<em>{i=1}^m \left[ \ln\left((y</em>i+r-1)!\right) - \ln\left(y<em>i!\right) - \ln\left((r-1)!\right) + r\ln(r) - r\ln(\mu</em>i+r) + y<em>i\ln(\mu</em>i) + y<em>i\ln(\mu</em>i + r)\right]\
	=&amp; \sum<em>{i=1}^m\left[\ln\left((y</em>i+r-1)!\right)-\ln(y<em>i!) - \ln\left((r-1)\right) + r\ln(r) - (r+y</em>i)\ln(\mu<em>i + r) + y</em>i\ln(\mu<em>i)\right] \end{aligned} Recalling the definition of [digamma and trigamma functions](https://en.wikipedia.org/wiki/Digamma</em>function), the first and second derivative of <span>$L$</span> with respect to <span>$r$</span> is: \begin{aligned} 	\frac{d}{dr} L(p<em>1, ..., p</em>m, r) = &amp; \sum<em>{i=1}^m \left[ \operatorname{digamma}(y</em>i+r) - \operatorname{digamma}(r) + 1 + \ln(r) - \frac{r+y<em>i}{\mu</em>i+r} - \ln(\mu<em>i + r) \right]\
	\frac{d^2}{dr^2} L(p</em>1, ..., p<em>m, r) =&amp;\sum</em>{i=1}^m \left[ \operatorname{trigamma}(y<em>i+r) - \operatorname{trigamma}(r) + \frac{1}{r} - \frac{2}{\mu</em>i + r} + \frac{r+y<em>i}{(\mu</em>i + r)^2} \right] \end{aligned} So the iteration to use is: \begin{aligned} 	r<em>{n+1} = r</em>n - \frac{\frac{d}{dr}L(p<em>1,...,p</em>m,r)}{\frac{d^2}{dr^2}L(p<em>1,...,p</em>m,r)}. \end{aligned} You can verify that this is the same as <a href="https://github.com/JuliaStats/GLM.jl/blob/master/src/negbinfit.jl#L3">the first and second derivative formula in GLM.jl</a>(they used <span>$\theta$</span> in place of <span>$r$</span>). The sign difference is because in GLM.jl, they are minimizing the negative loglikelihood instead of maximizing the loglikelihood. They are equivalent, but in mathematical optimization the standard form is to minimize an objective function. </p><pre><code class="language-julia"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../contributing/">Contributing »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 2 January 2021 01:25">Saturday 2 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

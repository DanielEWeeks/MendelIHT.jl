{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples\n",
    "\n",
    "Here we give numerous example analysis of GWAS data with MendelIHT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.0.3\n",
      "Commit 099e826241 (2018-12-18 01:34 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.0 (ORCJIT, ivybridge)\n"
     ]
    }
   ],
   "source": [
    "# machine information for reproducibility\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first add workers needed for parallel computing. Add only as many CPU cores available\n",
    "using Distributed\n",
    "addprocs(4)\n",
    "\n",
    "#load necessary packages for running all examples below\n",
    "using MendelIHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using GLM\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: How to Import Data\n",
    "\n",
    "We use [SnpArrays.jl](https://openmendel.github.io/SnpArrays.jl/latest/) as backend to process genotype files. Internally, the genotype file is a memory mapped [SnpArray](https://openmendel.github.io/SnpArrays.jl/stable/#SnpArray-1), which will not be loaded into RAM. If you wish to run `L0_reg`, you need to convert a SnpArray into a [SnpBitMatrix](https://openmendel.github.io/SnpArrays.jl/stable/#SnpBitMatrix-1), which consumes $n \\times p \\times 2$ bits of RAM. Non-genetic predictors should be read into Julia in the standard way, and should be stored as an `Array{Float64, 2}`. One should include the intercept (typically in the first column), but an intercept is not required to run IHT. \n",
    "\n",
    "### Reading Genotype data and Non-Genetic Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/biona001/.julia/dev/MendelIHT/docs/src/notebooks\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd() #show current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float64,2}:\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " ⋮       \n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  2.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x   = SnpArray(\"../data/test1.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "z   = readdlm(\"../data/test1_covariates.txt\") # 1st column intercept, 2nd column sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(x) = SnpArray\n",
      "typeof(xbm) = SnpBitMatrix{Float64}\n",
      "typeof(z) = Array{Float64,2}\n"
     ]
    }
   ],
   "source": [
    "@show typeof(x)\n",
    "@show typeof(xbm)\n",
    "@show typeof(z); #non genetic covariates must be Array{Float64, 2} even if only the intercept is included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    (1) MendelIHT.jl assumes there are **NO missing genotypes**, and (2) the trios (`.bim`, `.bed`, `.fam`) must all be present in the same directory. \n",
    "    \n",
    "### Standardizing Non-Genetic Covariates.\n",
    "\n",
    "We recommend standardizing all genetic and non-genetic covarariates (including binary and categorical), except for the intercept. This ensures equal penalization for all predictors. `SnpBitMatrix` efficiently achieves this standardization for genotype data, but this must be done manually for non-genetic covariates prior to using `z` in `L0_reg` or `cv_iht`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float64,2}:\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " ⋮             \n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0   1.01969 \n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706\n",
       " 1.0  -0.979706\n",
       " 1.0   1.01969 \n",
       " 1.0  -0.979706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize all covariates (other than intercept) to mean 0 variance 1\n",
    "standardize!(@view(z[:, 2:end]))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 2: Quantitative Traits\n",
    "\n",
    "Quantitative traits are continuous phenotypes whose distribution can be modeled by the normal distribution. Then using the genotype matrix $\\mathbf{X}$ and phenotype vector $\\mathbf{y}$, we want to recover $\\beta$ such that $\\mathbf{y} \\approx \\mathbf{X}\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Import data\n",
    "\n",
    "In Example 1 we illustrated how to import data into Julia. So here we use simulated data ([code](https://github.com/biona001/MendelIHT.jl/blob/master/src/simulate_utilities.jl#L107)) because, only then, can we compare IHT's result to the true solution. Below we simulate a GWAS data with $n=1000$ patients and $p=10000$ SNPs. Here the quantitative trait vector are affected by $k = 10$ causal SNPs, with no non-genetic confounders. \n",
    "\n",
    "In this example, our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "\n",
    "$$x_{ij} \\sim Binomial(2, p_j)$$\n",
    "\n",
    "$$p_j \\sim Uniform(0, 0.5)$$\n",
    "\n",
    "$$\\epsilon_i \\sim N(0, 1)$$\n",
    "\n",
    "$$\\beta_i \\sim N(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Normal\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019) \n",
    "\n",
    "# simulate SNP matrix, store the result in a file called tmp.bed\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "\n",
    "#construct the SnpBitMatrix type (needed for L0_reg() and simulate_random_response() below)\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "\n",
    "# intercept is the only nongenetic covariate\n",
    "z = ones(n, 1) \n",
    "\n",
    "# simulate response y, true model b, and the correct non-0 positions of b\n",
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run cross validation to determine best model size\n",
    "\n",
    "To run `cv_iht`, you must specify `path` and `num_fold`, defined below:\n",
    "\n",
    "+ `path` are all the model sizes you wish to test, stored in a vector of integers.\n",
    "+ `num_fold` indicates how many disjoint partitions of the samples is requested. \n",
    "\n",
    "By default, we partition the training/testing data randomly, but you can change this by inputing the `fold` vector as optional argument. In this example we tested $k = 1, 2, ..., 20$ across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by `parallel=true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1927.0765190526672\n",
      "\t2\t1443.8788742220866\n",
      "\t3\t1080.041135323195\n",
      "\t4\t862.2385953735205\n",
      "\t5\t705.1014346627649\n",
      "\t6\t507.394935936422\n",
      "\t7\t391.9686876462285\n",
      "\t8\t368.45440222003174\n",
      "\t9\t350.64279409251793\n",
      "\t10\t345.8380848576577\n",
      "\t11\t350.51881472845776\n",
      "\t12\t359.42391568519577\n",
      "\t13\t363.7095696959907\n",
      "\t14\t377.30732985896975\n",
      "\t15\t381.0310879522695\n",
      "\t16\t392.5643923838261\n",
      "\t17\t396.81166049333797\n",
      "\t18\t397.3010019298764\n",
      "\t19\t406.47023764639624\n",
      "\t20\t410.4672260807978\n",
      "\n",
      "The lowest MSE is achieved at k = 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = collect(1:20)\n",
    "num_folds = 3\n",
    "mses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \n",
    "\n",
    "    **DO NOT remove** intermediate files with random filenames as generated by `cv_iht()`. These are necessary auxiliary files that will be automatically removed when cross validation completes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    Because Julia employs a JIT compiler, the first round of any function call run will always take longer and consume extra memory. Therefore it is advised to always run a small \"test example\" (such as this one!) before running cross validation on a large dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run full model on the best estimated model size \n",
    "\n",
    "`cv_iht` finished in less than a minute. \n",
    "\n",
    "According to our cross validation result, the best model size that minimizes deviance residuals (i.e. MSE on the q-th subset of samples) is attained at $k = 10$. That is, cross validation detected that we need 10 SNPs to achieve the best model size. Using this information, one can re-run the IHT algorithm on the *full* dataset to obtain the best estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 10 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.5040078163146973\n",
       "Final loglikelihood:    -1406.8807653835697\n",
       "Iterations:             6\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   10\n",
       "\n",
       "Selected genetic predictors:\n",
       "10×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 853      │ -1.24117    │\n",
       "│ 2   │ 877      │ -0.234676   │\n",
       "│ 3   │ 924      │ 0.82014     │\n",
       "│ 4   │ 2703     │ 0.583403    │\n",
       "│ 5   │ 4241     │ 0.298304    │\n",
       "│ 6   │ 4783     │ -1.14459    │\n",
       "│ 7   │ 5094     │ 0.673012    │\n",
       "│ 8   │ 5284     │ -0.709736   │\n",
       "│ 9   │ 7760     │ 0.16866     │\n",
       "│ 10  │ 8255     │ 1.08117     │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_est = argmin(mses)\n",
    "result = L0_reg(x, xbm, z, y, 1, k_est, d(), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (only for simulated data): Check final model against simulation\n",
    "\n",
    "Since all our data and model was simulated, we can see how well `cv_iht` combined with `L0_reg` estimated the true model. For this example, we find that IHT found every simulated predictor, with 0 false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×2 DataFrame\n",
      "│ Row │ true_β   │ estimated_β │\n",
      "│     │ Float64  │ Float64     │\n",
      "├─────┼──────────┼─────────────┤\n",
      "│ 1   │ -1.29964 │ -1.24117    │\n",
      "│ 2   │ -0.2177  │ -0.234676   │\n",
      "│ 3   │ 0.786217 │ 0.82014     │\n",
      "│ 4   │ 0.599233 │ 0.583403    │\n",
      "│ 5   │ 0.283711 │ 0.298304    │\n",
      "│ 6   │ -1.12537 │ -1.14459    │\n",
      "│ 7   │ 0.693374 │ 0.673012    │\n",
      "│ 8   │ -0.67709 │ -0.709736   │\n",
      "│ 9   │ 0.14727  │ 0.16866     │\n",
      "│ 10  │ 1.03477  │ 1.08117     │\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "@show compare_model\n",
    "\n",
    "#clean up\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logistic Regression Controlling for Sex\n",
    "\n",
    "We show how to use IHT to handle case-control studies, while handling non-genetic covariates. In this example, we fit a logistic regression model with IHT using simulated case-control data, while controling for sex as a nongenetic covariate. \n",
    "\n",
    "### Step 1: Import Data\n",
    "\n",
    "Again we use a simulated model:\n",
    "\n",
    "$$y_i \\sim Bernoulli(\\mathbf{x}_i^T\\mathbf{\\beta})$$\n",
    "\n",
    "$$x_{ij} \\sim Binomial(2, p_j)$$\n",
    "\n",
    "$$p_j \\sim Uniform(0, 0.5)$$\n",
    "\n",
    "$$\\beta_i \\sim N(0, 1)$$\n",
    "\n",
    "$$\\beta_{intercept} = 1$$\n",
    "\n",
    "$$\\beta_{sex} = 1.5$$\n",
    "\n",
    "We assumed there are $k=8$ genetic predictors and 2 non-genetic predictors (intercept and sex) that affects the trait. The simulation code in our package does not yet handle simulations with non-genetic predictors, so we must simulate these phenotypes manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Bernoulli\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019)\n",
    "\n",
    "# construct SnpArray and SnpBitMatrix\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "\n",
    "# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\n",
    "z = ones(n, 2) \n",
    "z[:, 2] .= rand(0:1, n)\n",
    "\n",
    "# randomly set genetic predictors\n",
    "true_b = zeros(p) \n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "\n",
    "# find correct position of genetic predictors\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "# define effect size of non-genetic predictors: intercept & sex\n",
    "true_c = [1.0; 1.5] \n",
    "\n",
    "# simulate phenotype using genetic and nongenetic predictors\n",
    "prob = linkinv.(l, xbm * true_b .+ z * true_c)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # y must be floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run cross validation to determine best model size\n",
    "\n",
    "To run `cv_iht`, you must specify `path` and `num_fold`, defined below:\n",
    "\n",
    "+ `path` are all the model sizes you wish to test, stored in a vector of integers.\n",
    "+ `num_fold` indicates how many disjoint partitions of the samples is requested. \n",
    "\n",
    "By default, we partition the training/testing data randomly, but you can change this by inputing the `fold` vector as optional argument. In this example we tested $k = 1, 2, ..., 20$ across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by `parallel=true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t391.44413742296507\n",
      "\t2\t365.94409558538405\n",
      "\t3\t332.53357480884415\n",
      "\t4\t270.9466574526783\n",
      "\t5\t245.64667604806908\n",
      "\t6\t234.1348150358823\n",
      "\t7\t242.1326570535411\n",
      "\t8\t237.8125739190615\n",
      "\t9\t248.39984663655218\n",
      "\t10\t247.42113174417304\n",
      "\t11\t258.76386322638245\n",
      "\t12\t263.02028089480876\n",
      "\t13\t265.12598433158234\n",
      "\t14\t279.28886892555727\n",
      "\t15\t291.4334160383655\n",
      "\t16\t320.2043997941196\n",
      "\t17\t284.29817104006\n",
      "\t18\t327.9803987249458\n",
      "\t19\t332.2334866227556\n",
      "\t20\t344.79544568090694\n",
      "\n",
      "The lowest MSE is achieved at k = 6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = collect(1:20)\n",
    "num_folds = 3\n",
    "mses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    In our experience, using the `ProbitLink` for logistic regressions deliver better results than `LogitLink` (which is the canonical link). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run full model on the best estimated model size \n",
    "\n",
    "`cv_iht` finished in about a minute. \n",
    "\n",
    "Cross validation have declared that $k_{best} = 8$. Using this information, one can re-run the IHT algorithm on the *full* dataset to obtain the best estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 4 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     3.0592892169952393\n",
       "Final loglikelihood:    -341.107863135428\n",
       "Iterations:             57\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   6\n",
       "\n",
       "Selected genetic predictors:\n",
       "4×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1152     │ 0.745372    │\n",
       "│ 2   │ 1576     │ 1.3801      │\n",
       "│ 3   │ 5765     │ -1.3558     │\n",
       "│ 4   │ 5992     │ -1.47339    │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "2×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1        │ 0.599159    │\n",
       "│ 2   │ 2        │ 1.54466     │"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_est = argmin(mses)\n",
    "result = L0_reg(x, xbm, z, y, 1, k_est, d(), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (only for simulated data): Check final model against simulation\n",
    "\n",
    "Since all our data and model was simulated, we can see how well `cv_iht` combined with `L0_reg` estimated the true model. For this example, we find that IHT found both nongenetic predictor, but missed 2 genetic predictors. The 2 genetic predictors that we missed had much smaller effect size, so given that we only had 1000 samples, this is hardly surprising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model_genetics = 8×2 DataFrame\n",
      "│ Row │ true_β   │ estimated_β │\n",
      "│     │ Float64  │ Float64     │\n",
      "├─────┼──────────┼─────────────┤\n",
      "│ 1   │ 0.961937 │ 0.745372    │\n",
      "│ 2   │ 0.189267 │ 0.0         │\n",
      "│ 3   │ 1.74008  │ 1.3801      │\n",
      "│ 4   │ 0.879004 │ 0.0         │\n",
      "│ 5   │ 0.213066 │ 0.0         │\n",
      "│ 6   │ -1.74663 │ -1.3558     │\n",
      "│ 7   │ -1.93402 │ -1.47339    │\n",
      "│ 8   │ 0.632786 │ 0.0         │\n",
      "compare_model_nongenetics = 2×2 DataFrame\n",
      "│ Row │ true_c  │ estimated_c │\n",
      "│     │ Float64 │ Float64     │\n",
      "├─────┼─────────┼─────────────┤\n",
      "│ 1   │ 1.0     │ 0.599159    │\n",
      "│ 2   │ 1.5     │ 1.54466     │\n"
     ]
    }
   ],
   "source": [
    "compare_model_genetics = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "\n",
    "compare_model_nongenetics = DataFrame(\n",
    "    true_c      = true_c[1:2], \n",
    "    estimated_c = result.c[1:2])\n",
    "\n",
    "@show compare_model_genetics\n",
    "@show compare_model_nongenetics\n",
    "\n",
    "#clean up\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Poisson Regression with Acceleration (i.e. debias)\n",
    "\n",
    "In this example, we show how debiasing can potentially achieve dramatic speedup. Our model is:\n",
    "\n",
    "$$y_i \\sim Poisson(\\mathbf{x}_i^T\\mathbf{\\beta})$$\n",
    "\n",
    "$$x_{ij} \\sim Binomial(2, p_j)$$\n",
    "\n",
    "$$p_j \\sim Uniform(0, 0.5)$$\n",
    "\n",
    "$$\\beta_i \\sim N(0, 0.3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 5000\n",
    "p = 30000\n",
    "k = 10\n",
    "d = Poisson\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019)\n",
    "\n",
    "# construct SnpArray, SnpBitMatrix, and intercept\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "z = ones(n, 1) \n",
    "\n",
    "# simulate response, true model b, and the correct non-0 positions of b\n",
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Compare Reconstruction Result\n",
    "\n",
    "First we show that, with or without debiasing, we obtain comparable results with `L0_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_debias  = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false)\n",
    "yes_debias = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      "│ Row │ position │ true_β     │ no_debias_β │ yes_debias_β │\n",
      "│     │ Int64    │ Float64    │ Float64     │ Float64      │\n",
      "├─────┼──────────┼────────────┼─────────────┼──────────────┤\n",
      "│ 1   │ 2105     │ 0.0155232  │ 0.0         │ 0.0          │\n",
      "│ 2   │ 5852     │ 0.0747323  │ 0.0759218   │ 0.0775061    │\n",
      "│ 3   │ 9219     │ 0.0233952  │ 0.0         │ 0.0          │\n",
      "│ 4   │ 10362    │ -0.241167  │ -0.243655   │ -0.245874    │\n",
      "│ 5   │ 15755    │ 0.278812   │ 0.28015     │ 0.283672     │\n",
      "│ 6   │ 21188    │ 0.0540703  │ 0.0607867   │ 0.0621309    │\n",
      "│ 7   │ 21324    │ -0.216701  │ -0.222548   │ -0.221695    │\n",
      "│ 8   │ 21819    │ -0.0331256 │ -0.0599403  │ -0.0603646   │\n",
      "│ 9   │ 25655    │ 0.0217997  │ 0.0         │ 0.0          │\n",
      "│ 10  │ 29986    │ 0.354062   │ 0.361812    │ 0.361667     │\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    true_β      = true_b[correct_position], \n",
    "    no_debias_β = no_debias.beta[correct_position],\n",
    "    yes_debias_β = yes_debias.beta[correct_position])\n",
    "@show compare_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Speed and Memory Usage\n",
    "\n",
    "Now we illustrate that debiasing may dramatically reduce computational time (in this case 85%), at a cost of increasing the memory usage. In practice, this extra memory usage hardly matters because the matrix size will dominate for larger problems. See [here for complete benchmark figure.](https://github.com/biona001/MendelIHT.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  4.96 MiB\n",
       "  allocs estimate:  531\n",
       "  --------------\n",
       "  minimum time:     7.194 s (0.00% GC)\n",
       "  median time:      7.390 s (0.00% GC)\n",
       "  mean time:        7.393 s (0.01% GC)\n",
       "  maximum time:     7.585 s (0.05% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false) seconds=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  11.83 MiB\n",
       "  allocs estimate:  1320\n",
       "  --------------\n",
       "  minimum time:     6.121 s (0.06% GC)\n",
       "  median time:      6.318 s (0.00% GC)\n",
       "  mean time:        6.307 s (0.03% GC)\n",
       "  maximum time:     6.469 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true) seconds=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean up\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples and functionalities\n",
    "\n",
    "We invite users to experiment with additional functionalities. We explored a significant portion of them in our manuscript, with [reproducible code](https://github.com/biona001/MendelIHT.jl/tree/master/figures). This includes:\n",
    "\n",
    "+ Modeling some exotic distributions and using noncanonical link functions [listed here](https://biona001.github.io/MendelIHT.jl/latest/man/getting_started/#Supported-GLM-models-and-Link-functions-1)\n",
    "+ Modeling SNP-SNP or SNP-environment interaction effects by explicitly including them in the nongenetic covariates `z`.\n",
    "+ Doubly sparse projection (requires group information)\n",
    "+ Weighted projections to favor certain SNPs (requires weight information)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

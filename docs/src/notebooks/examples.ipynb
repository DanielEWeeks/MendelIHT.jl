{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples\n",
    "\n",
    "Here we give numerous example analysis of GWAS data with MendelIHT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.0.3\n",
      "Commit 099e826241 (2018-12-18 01:34 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "# machine information for reproducibility\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first add workers needed for parallel computing. Add only as many CPU cores available\n",
    "using Distributed\n",
    "addprocs(4)\n",
    "\n",
    "#load necessary packages for running all examples below\n",
    "using MendelIHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using GLM\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: How to Import Data\n",
    "\n",
    "We use [SnpArrays.jl](https://openmendel.github.io/SnpArrays.jl/latest/) as backend to process genotype files. Internally, the genotype file is a memory mapped [SnpArray](https://openmendel.github.io/SnpArrays.jl/stable/#SnpArray-1), which will not be loaded into RAM. If you wish to run `L0_reg`, you need to convert a SnpArray into a [SnpBitMatrix](https://openmendel.github.io/SnpArrays.jl/stable/#SnpBitMatrix-1), which consumes $n \\times p \\times 2$ bits of RAM. Non-genetic predictors should be read into Julia in the standard way, and should be stored as an `Array{Float64, 2}`. One should include the intercept (typically in the first column), but an intercept is not required to run IHT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate example data (to be imported later)\n",
    "\n",
    "First we simulate an example PLINK trio (`.bim`, `.bed`, `.fam`) and non-genetic covariates, then we illustrate how to import them. For genotype matrix simulation, we simulate under the model:\n",
    "\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rows and columns\n",
    "n = 1000\n",
    "p = 10000\n",
    "\n",
    "#random seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "# simulate random `.bed` file\n",
    "x = simulate_random_snparray(n, p, \"example.bed\")\n",
    "\n",
    "# create accompanying `.bim`, `.fam` files (randomly generated)\n",
    "make_bim_fam_files(x, ones(n), \"example\")\n",
    "\n",
    "# simulate non-genetic covariates (in this case, we include intercept and sex)\n",
    "z = [ones(n, 1) rand(0:1, n)]\n",
    "writedlm(\"example_nongenetic_covariates.txt\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Genotype data and Non-Genetic Covariates from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float64,2}:\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " ⋮       \n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = SnpArray(\"example.bed\")\n",
    "z = readdlm(\"example_nongenetic_covariates.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    (1) MendelIHT.jl assumes there are **NO missing genotypes**, (2) 1 always encode the minor allele, and (3) the trios (`.bim`, `.bed`, `.fam`) are all be present in the same directory. \n",
    "    \n",
    "### Standardizing Non-Genetic Covariates.\n",
    "\n",
    "We recommend standardizing all genetic and non-genetic covarariates (including binary and categorical), except for the intercept. This ensures equal penalization for all predictors. For genotype matrix, `SnpBitMatrix` efficiently achieves this standardization. For non-genetic covariates, we use the built-in function `standardize!`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float64,2}:\n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " ⋮             \n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0  -0.997503\n",
       " 1.0   1.0015  \n",
       " 1.0  -0.997503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SnpBitMatrix can automatically standardizes .bed file (without extra memory) and behaves like a matrix\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "\n",
    "# using view is important for correctness\n",
    "standardize!(@view(z[:, 2:end])) \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove simulated data once they are no longer needed\n",
    "rm(\"example.bed\", force=true)\n",
    "rm(\"example.bim\", force=true)\n",
    "rm(\"example.fam\", force=true)\n",
    "rm(\"example_nongenetic_covariates.txt\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 2: Running IHT on Quantitative Traits\n",
    "\n",
    "Quantitative traits are continuous phenotypes whose distribution can be modeled by the normal distribution. Then using the genotype matrix $\\mathbf{X}$ and phenotype vector $\\mathbf{y}$, we want to recover $\\beta$ such that $\\mathbf{y} \\approx \\mathbf{X}\\beta$. In this example, we assume we know the true sparsity level `k`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Import data\n",
    "\n",
    "In Example 1 we illustrated how to import data into Julia. So here we use simulated data ([code](https://github.com/biona001/MendelIHT.jl/blob/master/src/simulate_utilities.jl#L107)) because, only then, can we compare IHT's result to the true solution. Below we simulate a GWAS data with $n=1000$ patients and $p=10000$ SNPs. Here the quantitative trait vector are affected by $k = 10$ causal SNPs, with no non-genetic confounders. \n",
    "\n",
    "In this example, our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\epsilon_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Normal\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019) \n",
    "\n",
    "# simulate SNP matrix, store the result in a file called tmp.bed\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "\n",
    "#construct the SnpBitMatrix type (needed for L0_reg() and simulate_random_response() below)\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "\n",
    "# intercept is the only nongenetic covariate\n",
    "z = ones(n, 1) \n",
    "\n",
    "# simulate response y, true model b, and the correct non-0 positions of b\n",
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run cross validation to determine best model size\n",
    "\n",
    "To run `cv_iht`, you must specify `path` and `num_fold`, defined below:\n",
    "\n",
    "+ `path` are all the model sizes you wish to test, stored in a vector of integers.\n",
    "+ `num_fold` indicates how many disjoint partitions of the samples is requested. \n",
    "\n",
    "By default, we partition the training/testing data randomly, but you can change this by inputing the `fold` vector as optional argument. In this example we tested $k = 1, 2, ..., 20$ across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by `parallel=true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1927.0765190526674\n",
      "\t2\t1443.8788742220863\n",
      "\t3\t1080.041135323195\n",
      "\t4\t862.2385953735204\n",
      "\t5\t705.1014346627649\n",
      "\t6\t507.3949359364219\n",
      "\t7\t391.96868764622843\n",
      "\t8\t368.45440222003174\n",
      "\t9\t350.642794092518\n",
      "\t10\t345.8380848576577\n",
      "\t11\t350.5188147284578\n",
      "\t12\t359.42391568519577\n",
      "\t13\t363.70956969599075\n",
      "\t14\t377.30732985896975\n",
      "\t15\t381.0310879522694\n",
      "\t16\t392.56439238382615\n",
      "\t17\t396.81166049333797\n",
      "\t18\t397.3010019298764\n",
      "\t19\t406.47023764639624\n",
      "\t20\t410.4672260807978\n"
     ]
    }
   ],
   "source": [
    "path = collect(1:20)\n",
    "num_folds = 3\n",
    "mses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \n",
    "\n",
    "    **DO NOT remove** intermediate files with random filenames as generated by `cv_iht()`. These are necessary auxiliary files that will be automatically removed when cross validation completes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    Because Julia employs a JIT compiler, the first round of any function call run will always take longer and consume extra memory. Therefore it is advised to always run a small \"test example\" (such as this one!) before running cross validation on a large dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run full model on the best estimated model size \n",
    "\n",
    "`cv_iht` finished in less than a minute. \n",
    "\n",
    "According to our cross validation result, the best model size that minimizes deviance residuals (i.e. MSE on the q-th subset of samples) is attained at $k = 10$. That is, cross validation detected that we need 10 SNPs to achieve the best model size. Using this information, one can re-run the IHT algorithm on the *full* dataset to obtain the best estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 10 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.4135408401489258\n",
       "Final loglikelihood:    -1406.8807653835697\n",
       "Iterations:             6\n",
       "\n",
       "Selected genetic predictors:\n",
       "10×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 853      │ -1.24117    │\n",
       "│ 2   │ 877      │ -0.234676   │\n",
       "│ 3   │ 924      │ 0.82014     │\n",
       "│ 4   │ 2703     │ 0.583403    │\n",
       "│ 5   │ 4241     │ 0.298304    │\n",
       "│ 6   │ 4783     │ -1.14459    │\n",
       "│ 7   │ 5094     │ 0.673012    │\n",
       "│ 8   │ 5284     │ -0.709736   │\n",
       "│ 9   │ 7760     │ 0.16866     │\n",
       "│ 10  │ 8255     │ 1.08117     │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_est = argmin(mses)\n",
    "result = L0_reg(x, xbm, z, y, 1, k_est, d(), l) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (only for simulated data): Check final model against simulation\n",
    "\n",
    "Since all our data and model was simulated, we can see how well `cv_iht` combined with `L0_reg` estimated the true model. For this example, we find that IHT found every simulated predictor, with 0 false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×2 DataFrame\n",
      "│ Row │ true_β   │ estimated_β │\n",
      "│     │ Float64  │ Float64     │\n",
      "├─────┼──────────┼─────────────┤\n",
      "│ 1   │ -1.29964 │ -1.24117    │\n",
      "│ 2   │ -0.2177  │ -0.234676   │\n",
      "│ 3   │ 0.786217 │ 0.82014     │\n",
      "│ 4   │ 0.599233 │ 0.583403    │\n",
      "│ 5   │ 0.283711 │ 0.298304    │\n",
      "│ 6   │ -1.12537 │ -1.14459    │\n",
      "│ 7   │ 0.693374 │ 0.673012    │\n",
      "│ 8   │ -0.67709 │ -0.709736   │\n",
      "│ 9   │ 0.14727  │ 0.16866     │\n",
      "│ 10  │ 1.03477  │ 1.08117     │\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "@show compare_model\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logistic Regression Controlling for Sex\n",
    "\n",
    "We show how to use IHT to handle case-control studies, while handling non-genetic covariates. In this example, we fit a logistic regression model with IHT using simulated case-control data, while controling for sex as a nongenetic covariate. \n",
    "\n",
    "### Step 1: Import Data\n",
    "\n",
    "Again we use a simulated model:\n",
    "\n",
    "$$y_i \\sim \\rm Bernoulli(\\mathbf{x}_i^T\\mathbf{\\beta})$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_{\\rm intercept} = 1$$\n",
    "$$\\beta_{\\rm sex} = 1.5$$\n",
    "\n",
    "We assumed there are $k=8$ genetic predictors and 2 non-genetic predictors (intercept and sex) that affects the trait. The simulation code in our package does not yet handle simulations with non-genetic predictors, so we must simulate these phenotypes manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Bernoulli\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019)\n",
    "\n",
    "# construct SnpArray and SnpBitMatrix\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "\n",
    "# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\n",
    "z = ones(n, 2) \n",
    "z[:, 2] .= rand(0:1, n)\n",
    "\n",
    "# randomly set genetic predictors\n",
    "true_b = zeros(p) \n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "\n",
    "# find correct position of genetic predictors\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "# define effect size of non-genetic predictors: intercept & sex\n",
    "true_c = [1.0; 1.5] \n",
    "\n",
    "# simulate phenotype using genetic and nongenetic predictors\n",
    "prob = GLM.linkinv.(l, xbm * true_b .+ z * true_c)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # y must be floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run cross validation to determine best model size\n",
    "\n",
    "To run `cv_iht`, you must specify `path` and `num_fold`, defined below:\n",
    "\n",
    "+ `path` are all the model sizes you wish to test, stored in a vector of integers.\n",
    "+ `num_fold` indicates how many disjoint partitions of the samples is requested. \n",
    "\n",
    "By default, we partition the training/testing data randomly, but you can change this by inputing the `fold` vector as optional argument. In this example we tested $k = 1, 2, ..., 20$ across 3 fold cross validation. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing (which you specify by `parallel=true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t391.44426892225727\n",
      "\t2\t365.7520496496987\n",
      "\t3\t332.3801926093215\n",
      "\t4\t273.2081512206048\n",
      "\t5\t253.31631985207758\n",
      "\t6\t234.93701288953315\n",
      "\t7\t221.997334181244\n",
      "\t8\t199.01688783420346\n",
      "\t9\t208.31087723164197\n",
      "\t10\t216.00575628120708\n",
      "\t11\t227.91834469184545\n",
      "\t12\t242.42456871743065\n",
      "\t13\t261.46346209331466\n",
      "\t14\t263.5229307137862\n",
      "\t15\t283.30379378951073\n",
      "\t16\t328.2771991160804\n",
      "\t17\t290.4512419547228\n",
      "\t18\t350.3516280657363\n",
      "\t19\t361.61016297810295\n",
      "\t20\t418.2370935226805\n"
     ]
    }
   ],
   "source": [
    "path = collect(1:20)\n",
    "num_folds = 3\n",
    "mses = cv_iht(d(), l, x, z, y, 1, path, num_folds, parallel=true); #here 1 is for number of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    In our experience, using the `ProbitLink` for logistic regressions deliver better results than `LogitLink` (which is the canonical link). But of course, one should choose the link that gives the higher loglikelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run full model on the best estimated model size \n",
    "\n",
    "`cv_iht` finished in about a minute. \n",
    "\n",
    "Cross validation have declared that $k_{best} = 8$. Using this information, one can re-run the IHT algorithm on the *full* dataset to obtain the best estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 6 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     1.6644580364227295\n",
       "Final loglikelihood:    -290.4509381564733\n",
       "Iterations:             37\n",
       "\n",
       "Selected genetic predictors:\n",
       "6×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1152     │ 0.966731    │\n",
       "│ 2   │ 1576     │ 1.56183     │\n",
       "│ 3   │ 3411     │ 0.87674     │\n",
       "│ 4   │ 5765     │ -1.75611    │\n",
       "│ 5   │ 5992     │ -2.04506    │\n",
       "│ 6   │ 8781     │ 0.760213    │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "2×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1        │ 0.709909    │\n",
       "│ 2   │ 2        │ 1.65049     │"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_est = argmin(mses)\n",
    "result = L0_reg(x, xbm, z, y, 1, k_est, d(), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (only for simulated data): Check final model against simulation\n",
    "\n",
    "Since all our data and model was simulated, we can see how well `cv_iht` combined with `L0_reg` estimated the true model. For this example, we find that IHT found both nongenetic predictor, but missed 2 genetic predictors. The 2 genetic predictors that we missed had much smaller effect size, so given that we only had 1000 samples, this is hardly surprising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model_genetics = 8×2 DataFrame\n",
      "│ Row │ true_β   │ estimated_β │\n",
      "│     │ Float64  │ Float64     │\n",
      "├─────┼──────────┼─────────────┤\n",
      "│ 1   │ 0.961937 │ 0.966731    │\n",
      "│ 2   │ 0.189267 │ 0.0         │\n",
      "│ 3   │ 1.74008  │ 1.56183     │\n",
      "│ 4   │ 0.879004 │ 0.87674     │\n",
      "│ 5   │ 0.213066 │ 0.0         │\n",
      "│ 6   │ -1.74663 │ -1.75611    │\n",
      "│ 7   │ -1.93402 │ -2.04506    │\n",
      "│ 8   │ 0.632786 │ 0.760213    │\n",
      "compare_model_nongenetics = 2×2 DataFrame\n",
      "│ Row │ true_c  │ estimated_c │\n",
      "│     │ Float64 │ Float64     │\n",
      "├─────┼─────────┼─────────────┤\n",
      "│ 1   │ 1.0     │ 0.709909    │\n",
      "│ 2   │ 1.5     │ 1.65049     │\n"
     ]
    }
   ],
   "source": [
    "compare_model_genetics = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "\n",
    "compare_model_nongenetics = DataFrame(\n",
    "    true_c      = true_c[1:2], \n",
    "    estimated_c = result.c[1:2])\n",
    "\n",
    "@show compare_model_genetics\n",
    "@show compare_model_nongenetics\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Poisson Regression with Acceleration (i.e. debias)\n",
    "\n",
    "In this example, we show how debiasing can potentially achieve dramatic speedup. Our model is:\n",
    "\n",
    "$$y_i \\sim \\rm Poisson(\\mathbf{x}_i^T \\mathbf{\\beta})$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model dimensions, true model size, distribution, and link functions\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Poisson\n",
    "l = canonicallink(d())\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2019)\n",
    "\n",
    "# construct SnpArray, SnpBitMatrix, and intercept\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true);\n",
    "z = ones(n, 1) \n",
    "\n",
    "# simulate response, true model b, and the correct non-0 positions of b\n",
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Compare Reconstruction Result\n",
    "\n",
    "First we show that, with or without debiasing, we obtain comparable results with `L0_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_debias  = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false)\n",
    "yes_debias = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      "│ Row │ position │ true_β     │ no_debias_β │ yes_debias_β │\n",
      "│     │ Int64    │ Float64    │ Float64     │ Float64      │\n",
      "├─────┼──────────┼────────────┼─────────────┼──────────────┤\n",
      "│ 1   │ 853      │ -0.389892  │ -0.384161   │ -0.38744     │\n",
      "│ 2   │ 877      │ -0.0653099 │ 0.0         │ 0.0          │\n",
      "│ 3   │ 924      │ 0.235865   │ 0.246213    │ 0.240514     │\n",
      "│ 4   │ 2703     │ 0.17977    │ 0.237651    │ 0.225127     │\n",
      "│ 5   │ 4241     │ 0.0851134  │ 0.0         │ 0.0894244    │\n",
      "│ 6   │ 4783     │ -0.33761   │ -0.300663   │ -0.307515    │\n",
      "│ 7   │ 5094     │ 0.208012   │ 0.223384    │ 0.215149     │\n",
      "│ 8   │ 5284     │ -0.203127  │ -0.225593   │ -0.209308    │\n",
      "│ 9   │ 7760     │ 0.0441809  │ 0.0         │ 0.0          │\n",
      "│ 10  │ 8255     │ 0.310431   │ 0.287363    │ 0.301717     │\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    true_β      = true_b[correct_position], \n",
    "    no_debias_β = no_debias.beta[correct_position],\n",
    "    yes_debias_β = yes_debias.beta[correct_position])\n",
    "@show compare_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Speed and Memory Usage\n",
    "\n",
    "Now we illustrate that debiasing may dramatically reduce computational time (in this case ~50%), at a cost of increasing the memory usage. In practice, this extra memory usage hardly matters because the matrix size will dominate for larger problems. See [our paper for complete benchmark figure.](https://www.biorxiv.org/content/10.1101/697755v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.16 MiB\n",
       "  allocs estimate:  738\n",
       "  --------------\n",
       "  minimum time:     673.188 ms (0.00% GC)\n",
       "  median time:      706.368 ms (0.00% GC)\n",
       "  mean time:        708.080 ms (0.04% GC)\n",
       "  maximum time:     741.125 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          22\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false) seconds=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.62 MiB\n",
       "  allocs estimate:  1135\n",
       "  --------------\n",
       "  minimum time:     337.368 ms (0.00% GC)\n",
       "  median time:      350.194 ms (0.00% GC)\n",
       "  mean time:        349.958 ms (0.10% GC)\n",
       "  maximum time:     358.095 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          43\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark L0_reg(x, xbm, z, y, 1, k, d(), l, debias=true) seconds=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Negative Binomial regression with group information \n",
    "\n",
    "In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most $J = 5$ groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter $k$ is known. \n",
    "\n",
    "### Data simulation\n",
    "To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf). In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; $j = 5$ distinct groups are each assigned $1,2,...,5$ causal SNPs with effect sizes randomly chosen from $\\{−0.2,0.2\\}$. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity $\\lambda_g = 1,2,...,5$. The remaining groups are assigned $\\lambda_g = 1$ (i.e. only 1 active predictor are allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define problem size\n",
    "d = NegativeBinomial\n",
    "l = LogLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "block_size = 20                  #simulation parameter\n",
    "num_blocks = Int(p / block_size) #simulation parameter\n",
    "\n",
    "# set seed\n",
    "Random.seed!(2019)\n",
    "\n",
    "# assign group membership\n",
    "membership = collect(1:num_blocks)\n",
    "g = zeros(Int64, p + 1)\n",
    "for i in 1:length(membership)\n",
    "    for j in 1:block_size\n",
    "        cur_row = block_size * (i - 1) + j\n",
    "        g[block_size*(i - 1) + j] = membership[i]\n",
    "    end\n",
    "end\n",
    "g[end] = membership[end]\n",
    "\n",
    "#simulate correlated snparray\n",
    "x = simulate_correlated_snparray(n, p, \"tmp.bed\")\n",
    "z = ones(n, 1) # the intercept\n",
    "x_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n",
    "\n",
    "#simulate true model, where 5 groups each with 1~5 snps contribute\n",
    "true_b = zeros(p)\n",
    "true_groups = randperm(num_blocks)[1:5]\n",
    "sort!(true_groups)\n",
    "within_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n",
    "                randperm(block_size)[1:3], randperm(block_size)[1:4], \n",
    "                randperm(block_size)[1:5]]\n",
    "correct_position = zeros(Int64, 15)\n",
    "for i in 1:5\n",
    "    cur_group = block_size * (true_groups[i] - 1)\n",
    "    cur_group_snps = cur_group .+ within_group[i]\n",
    "    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n",
    "    correct_position[start:last] .= cur_group_snps\n",
    "end\n",
    "for i in 1:15\n",
    "    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\n",
    "end\n",
    "sort!(correct_position)\n",
    "\n",
    "# simulate phenotype\n",
    "r = 10 #nuisance parameter\n",
    "μ = GLM.linkinv.(l, x_float * true_b)\n",
    "clamp!(μ, -20, 20)\n",
    "prob = 1 ./ (1 .+ μ ./ r)\n",
    "y = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\n",
    "y = Float64.(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.11840415000915527\n",
       "Final loglikelihood:    -1441.522293255591\n",
       "Iterations:             27\n",
       "\n",
       "Selected genetic predictors:\n",
       "15×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 3464     │ -0.234958   │\n",
       "│ 2   │ 4383     │ -0.135693   │\n",
       "│ 3   │ 4927     │ 0.158171    │\n",
       "│ 4   │ 4938     │ -0.222613   │\n",
       "│ 5   │ 5001     │ -0.193739   │\n",
       "│ 6   │ 5011     │ -0.162718   │\n",
       "│ 7   │ 5018     │ -0.190532   │\n",
       "│ 8   │ 5090     │ 0.226509    │\n",
       "│ 9   │ 5092     │ -0.17756    │\n",
       "│ 10  │ 5100     │ -0.140337   │\n",
       "│ 11  │ 7004     │ 0.151748    │\n",
       "│ 12  │ 7011     │ 0.206449    │\n",
       "│ 13  │ 7015     │ -0.284706   │\n",
       "│ 14  │ 7016     │ 0.218126    │\n",
       "│ 15  │ 9902     │ 0.119059    │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run IHT without groups\n",
    "k = 15\n",
    "ungrouped = L0_reg(x_float, z, y, 1, k, d(), l, verbose=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 15 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.30719614028930664\n",
       "Final loglikelihood:    -1446.3808810786898\n",
       "Iterations:             16\n",
       "\n",
       "Selected genetic predictors:\n",
       "15×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 3464     │ -0.245853   │\n",
       "│ 2   │ 4927     │ 0.160904    │\n",
       "│ 3   │ 4938     │ -0.213439   │\n",
       "│ 4   │ 5001     │ -0.19624    │\n",
       "│ 5   │ 5011     │ -0.149913   │\n",
       "│ 6   │ 5018     │ -0.181966   │\n",
       "│ 7   │ 5086     │ -0.0560478  │\n",
       "│ 8   │ 5090     │ 0.21164     │\n",
       "│ 9   │ 5092     │ -0.141968   │\n",
       "│ 10  │ 5100     │ -0.157655   │\n",
       "│ 11  │ 7004     │ 0.190224    │\n",
       "│ 12  │ 7011     │ 0.21294     │\n",
       "│ 13  │ 7015     │ -0.256058   │\n",
       "│ 14  │ 7016     │ 0.19746     │\n",
       "│ 15  │ 7020     │ 0.111755    │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\n",
    "J = 5\n",
    "max_group_snps = ones(Int, num_blocks)\n",
    "max_group_snps[true_groups] .= collect(1:5)\n",
    "variable_group = L0_reg(x_float, z, y, J, max_group_snps, d(), l, verbose=false, group=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group IHT found 1 more SNPs than ungrouped IHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 15×4 DataFrame\n",
      "│ Row │ position │ correct_β │ ungrouped_IHT_β │ grouped_IHT_β │\n",
      "│     │ Int64    │ Float64   │ Float64         │ Float64       │\n",
      "├─────┼──────────┼───────────┼─────────────────┼───────────────┤\n",
      "│ 1   │ 3464     │ -0.2      │ -0.234958       │ -0.245853     │\n",
      "│ 2   │ 4927     │ 0.2       │ 0.158171        │ 0.160904      │\n",
      "│ 3   │ 4938     │ -0.2      │ -0.222613       │ -0.213439     │\n",
      "│ 4   │ 5001     │ -0.2      │ -0.193739       │ -0.19624      │\n",
      "│ 5   │ 5011     │ -0.2      │ -0.162718       │ -0.149913     │\n",
      "│ 6   │ 5018     │ -0.2      │ -0.190532       │ -0.181966     │\n",
      "│ 7   │ 5084     │ -0.2      │ 0.0             │ 0.0           │\n",
      "│ 8   │ 5090     │ 0.2       │ 0.226509        │ 0.21164       │\n",
      "│ 9   │ 5098     │ -0.2      │ 0.0             │ 0.0           │\n",
      "│ 10  │ 5100     │ -0.2      │ -0.140337       │ -0.157655     │\n",
      "│ 11  │ 7004     │ 0.2       │ 0.151748        │ 0.190224      │\n",
      "│ 12  │ 7011     │ 0.2       │ 0.206449        │ 0.21294       │\n",
      "│ 13  │ 7015     │ -0.2      │ -0.284706       │ -0.256058     │\n",
      "│ 14  │ 7016     │ 0.2       │ 0.218126        │ 0.19746       │\n",
      "│ 15  │ 7020     │ 0.2       │ 0.0             │ 0.111755      │\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check result\n",
    "correct_position = findall(!iszero, true_b)\n",
    "compare_model = DataFrame(\n",
    "    position = correct_position,\n",
    "    correct_β = true_b[correct_position],\n",
    "    ungrouped_IHT_β = ungrouped.beta[correct_position], \n",
    "    grouped_IHT_β = variable_group.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Linear Regression with prior weights\n",
    "\n",
    "In this example, we show how to include (predetermined) prior weights for each SNP. You can check out [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf) for references of why/how to choose these weights. In this case, we mimic our paper and randomly set $10\\%$ of all SNPs to have a weight of $2.0$. Other predictors have weight of $1.0$. All causal SNPs have weights of $2.0$. Under this scenario, SNPs with weight $2.0$ is twice as likely to enter the model identified by IHT. \n",
    "\n",
    "Our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\epsilon_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random seed\n",
    "Random.seed!(4)\n",
    "\n",
    "d = Normal\n",
    "l = canonicallink(d())\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "\n",
    "# construct snpmatrix, covariate files, and true model b\n",
    "x = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "X = convert(Matrix{Float64}, x, center=true, scale=true)\n",
    "z = ones(n, 1) # the intercept\n",
    "    \n",
    "#define true_b \n",
    "true_b = zeros(p)\n",
    "true_b[1:10] .= collect(0.1:0.1:1.0)\n",
    "shuffle!(true_b)\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "#simulate phenotypes (e.g. vector y)\n",
    "prob = GLM.linkinv.(l, X * true_b)\n",
    "clamp!(prob, -20, 20)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct weight vector\n",
    "w = ones(p + 1)\n",
    "w[correct_position] .= 2.0\n",
    "one_tenth = round(Int, p/10)\n",
    "idx = rand(1:p, one_tenth)\n",
    "w[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      "│ Row │ position │ correct │ unweighted │ weighted │\n",
      "│     │ Int64    │ Float64 │ Float64    │ Float64  │\n",
      "├─────┼──────────┼─────────┼────────────┼──────────┤\n",
      "│ 1   │ 1254     │ 0.4     │ 0.452245   │ 0.450405 │\n",
      "│ 2   │ 1495     │ 0.3     │ 0.306081   │ 0.305738 │\n",
      "│ 3   │ 4856     │ 0.8     │ 0.853536   │ 0.862223 │\n",
      "│ 4   │ 5767     │ 0.1     │ 0.0        │ 0.117286 │\n",
      "│ 5   │ 5822     │ 0.7     │ 0.656213   │ 0.651908 │\n",
      "│ 6   │ 5945     │ 0.9     │ 0.891915   │ 0.894997 │\n",
      "│ 7   │ 6367     │ 0.5     │ 0.469718   │ 0.472524 │\n",
      "│ 8   │ 6996     │ 1.0     │ 0.963236   │ 0.973512 │\n",
      "│ 9   │ 7052     │ 0.6     │ 0.602162   │ 0.600055 │\n",
      "│ 10  │ 7980     │ 0.2     │ 0.231389   │ 0.234094 │\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run IHT\n",
    "unweighted = L0_reg(X, z, y, 1, k, d(), l, verbose=false)\n",
    "weighted   = L0_reg(X, z, y, 1, k, d(), l, verbose=false, weight=w)\n",
    "\n",
    "#check result\n",
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    correct     = true_b[correct_position],\n",
    "    unweighted  = unweighted.beta[correct_position], \n",
    "    weighted    = weighted.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, weighted IHT found an extra predictor than non-weighted IHT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples and functionalities\n",
    "\n",
    "We explored a few more examples in our manuscript, with [reproducible code](https://github.com/biona001/MendelIHT.jl/tree/master/figures). We invite users to experiment with them as well. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

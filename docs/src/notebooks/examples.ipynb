{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples\n",
    "\n",
    "Here we give numerous example analysis of GWAS data with `MendelIHT.jl`. \n",
    "\n",
    "Users are highly encouraged to read the source code of our main [fit](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/fit.jl#L31) and [cv_iht](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/cross_validation.jl#L38) functions, which contain more options than what is described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.5.0\n",
      "Commit 96786e22cc (2020-08-01 23:44 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.7.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "# machine information for reproducibility\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first add workers needed for parallel computing. Add only as many CPU cores available\n",
    "using Distributed\n",
    "addprocs(4)\n",
    "\n",
    "#load necessary packages for running all examples below\n",
    "using Revise\n",
    "using MendelIHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using GLM\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using BenchmarkTools\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: GWAS with PLINK files\n",
    "\n",
    "For PLINK files, users are exposed to a few simple wrapper functions. For demonstration, we use simulated data under the `data` directory, as shown below. This data simulates quantitative (Gaussian) traits using $n=1000$ samples and $p=10,000$ SNPs. There are $8$ causal variants and 2 causal non-genetic covariates (intercept and sex). \n",
    "\n",
    "Start Julia and execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Array{String,1}:\n",
       " \".DS_Store\"\n",
       " \"covariates.txt\"\n",
       " \"example.bed\"\n",
       " \"example.bim\"\n",
       " \"example.fam\"\n",
       " \"example_nongenetic_covariates.txt\"\n",
       " \"normal.bed\"\n",
       " \"normal.bim\"\n",
       " \"normal.fam\"\n",
       " \"normal_true_beta.txt\"\n",
       " \"phenotypes.txt\"\n",
       " \"simulate.jl\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory to where example data is located\n",
    "cd(normpath(MendelIHT.datadir()))\n",
    "\n",
    "# show working directory\n",
    "@show pwd() \n",
    "\n",
    "# show files in current directory\n",
    "readdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `covariates.txt` contains non-genetic covariates, `normal.bed/bim/fam` are the PLINK files storing genetic covariates, `phenotypes.txt` are phenotypes for each sample, `normal_true_beta.txt` is the true statistical model used to generate the phenotypes, and `simulate.jl` is the script used to generate all the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run cross validation to determine best model size\n",
    "\n",
    "If phenotypes are stored in the `.fam` file and there are no other covariates (except for the intercept which is automatically included), one can run cross validation as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1408.4161771885078\n",
      "\t2\t862.714049343596\n",
      "\t3\t683.5762115676305\n",
      "\t4\t562.9030642400235\n",
      "\t5\t461.9271182844219\n",
      "\t6\t399.71508133538737\n",
      "\t7\t350.34847865063654\n",
      "\t8\t318.80715476554786\n",
      "\t9\t323.0559476609656\n",
      "\t10\t331.3640273301743\n",
      "\t11\t336.9865576111173\n",
      "\t12\t341.64939333865465\n",
      "\t13\t347.33123481686835\n",
      "\t14\t353.21600225128464\n",
      "\t15\t361.0692297288225\n",
      "\t16\t352.3514796059428\n",
      "\t17\t357.98125908673916\n",
      "\t18\t360.62269127273447\n",
      "\t19\t366.53839237209183\n",
      "\t20\t376.0279478485556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test k = 1, 2, ..., 20\n",
    "mses = cross_validate(\"normal\", 1:20)\n",
    "argmin(mses)\n",
    "\n",
    "# Alternative syntax\n",
    "# mses = cross_validate(\"normal\", [1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n",
    "# mses = cross_validate(\"normal\", \"covariates.txt\", 1:20) # include additional covariates in separate file\n",
    "# mses = cross_validate(\"phenotypes.txt\", \"normal\", \"covariates.txt\", 1:20) # when phenotypes are stored separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run IHT on best k\n",
    "\n",
    "According to cross validation, `k = 8` achieves the minimum MSE. Thus we run IHT on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.2.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse linear regression\n",
      "Link functin = IdentityLink()\n",
      "Sparsity parameter (k) = 8\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Converging when tol < 0.0001\n",
      "\n",
      "Iteration 1: tol = 0.7845860052299409\n",
      "Iteration 2: tol = 0.02358096868235321\n",
      "Iteration 3: tol = 0.001550076526387469\n",
      "Iteration 4: tol = 0.00010521336604120053\n",
      "Iteration 5: tol = 8.430366413828275e-6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 7 nonzero SNP predictors and 1 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.031874895095825195\n",
       "Final loglikelihood:    -1627.2792448761559\n",
       "Iterations:             5\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m7×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.411838\n",
       "   2 │     4246     0.572452\n",
       "   3 │     4717     0.909215\n",
       "   4 │     6290    -0.693302\n",
       "   5 │     7755    -0.54482\n",
       "   6 │     8375    -0.788884\n",
       "   7 │     9415    -2.15858\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m1×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.65223"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"normal\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examine results\n",
    "\n",
    "Here IHT picked 7 SNPs and the intercept as the 8 most significant predictor. The SNP position is the order in which the SNP appeared in the PLINK file. To extract more information (for instance to extract `rs` IDs), we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_snps = 7×6 DataFrame\n",
      " Row │ chromosome  snpid    genetic_distance  position  allele1  allele2\n",
      "     │ String      String   Float64           Int64     String   String\n",
      "─────┼───────────────────────────────────────────────────────────────────\n",
      "   1 │ 1           snp3137               0.0         1  1        2\n",
      "   2 │ 1           snp4246               0.0         1  1        2\n",
      "   3 │ 1           snp4717               0.0         1  1        2\n",
      "   4 │ 1           snp6290               0.0         1  1        2\n",
      "   5 │ 1           snp7755               0.0         1  1        2\n",
      "   6 │ 1           snp8375               0.0         1  1        2\n",
      "   7 │ 1           snp9415               0.0         1  1        2\n"
     ]
    }
   ],
   "source": [
    "snpdata = SnpData(\"normal\")                   # import PLINK information\n",
    "snps_idx = findall(!iszero, result.beta)      # indices of SNPs selected by IHT\n",
    "selected_snps = snpdata.snp_info[snps_idx, :] # see which SNPs are selected\n",
    "@show selected_snps;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above displays the SNP information for the selected SNPs. \n",
    "\n",
    "Since data is simulated, the fields `chromosome`, `snpid`, `genetic_distance`, `position`, `allele1`, and `allele2` are fake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: How to simulate data\n",
    "\n",
    "Here we demonstrate how to use `MendelIHT.jl` and [SnpArrays.jl](https://github.com/OpenMendel/SnpArrays.jl) to simulate data, allowing you to design your own genetic studies. Note all linear algebra routines involving PLINK files are handled by `SnpArrays.jl`. \n",
    "\n",
    "First we simulate an example PLINK trio (`.bim`, `.bed`, `.fam`) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:\n",
    "\n",
    "$$y_i \\sim \\rm Bernoulli(\\mathbf{x}_i^T\\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_{\\rm intercept} = 1$$\n",
    "$$\\beta_{\\rm sex} = 1.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000            # number of samples\n",
    "p = 10000           # number of SNPs\n",
    "k = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\n",
    "d = Bernoulli       # Binary (continuous) phenotypes\n",
    "l = LogitLink()     # canonical link function\n",
    "\n",
    "# set random seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "# simulate `sim.bed` file with no missing data\n",
    "x = simulate_random_snparray(\"sim.bed\", n, p)\n",
    "xla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n",
    "\n",
    "# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\n",
    "z = ones(n, 2) \n",
    "z[:, 2] .= rand(0:1, n)\n",
    "standardize!(@view(z[:, 2:end])) \n",
    "\n",
    "# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\n",
    "true_b = zeros(p) \n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "\n",
    "# find correct position of genetic predictors\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "# define effect size of non-genetic predictors: intercept & sex\n",
    "true_c = [1.0; 1.5] \n",
    "\n",
    "# simulate phenotype using genetic and nongenetic predictors\n",
    "prob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # turn y into floating point numbers\n",
    "\n",
    "# create `sim.bim` and `sim.bam` files using phenotype\n",
    "make_bim_fam_files(x, y, \"sim\")\n",
    "\n",
    "#save covariates and phenotypes (without header)\n",
    "writedlm(\"sim.covariates.txt\", z, ',')\n",
    "writedlm(\"sim.phenotypes.txt\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    Please **standardize** your non-genetic covariates. If you use our `iht()` or `cross_validation()` functions, standardization is automatic. For genotype matrix, `SnpLinAlg` efficiently achieves this standardization. For non-genetic covariates, please use the built-in function `standardize!`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logistic/Poisson/Negative-binomial GWAS\n",
    "\n",
    "In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true $k = 10$. Let's try running a logistic regression on this data. This is specified using keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.2.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse logistic regression\n",
      "Link functin = LogitLink()\n",
      "Sparsity parameter (k) = 10\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Converging when tol < 0.0001\n",
      "\n",
      "Iteration 1: tol = 0.5882274462947447\n",
      "Iteration 2: tol = 0.2825138668458021\n",
      "Iteration 3: tol = 0.19289827584644756\n",
      "Iteration 4: tol = 0.14269962283934917\n",
      "Iteration 5: tol = 0.022831477149267632\n",
      "Iteration 6: tol = 0.019792429262653115\n",
      "Iteration 7: tol = 0.019845664939460095\n",
      "Iteration 8: tol = 0.00765066824120313\n",
      "Iteration 9: tol = 0.006913691748350025\n",
      "Iteration 10: tol = 0.006159757548662376\n",
      "Iteration 11: tol = 0.0054730856932040705\n",
      "Iteration 12: tol = 0.004854846133978282\n",
      "Iteration 13: tol = 0.004300010671833966\n",
      "Iteration 14: tol = 0.0038033387186573\n",
      "Iteration 15: tol = 0.003359795402130408\n",
      "Iteration 16: tol = 0.0029645876127234955\n",
      "Iteration 17: tol = 0.0026131815201996976\n",
      "Iteration 18: tol = 0.002301318021364227\n",
      "Iteration 19: tol = 0.002025024729429744\n",
      "Iteration 20: tol = 0.001780622915677454\n",
      "Iteration 21: tol = 0.0015647287330161268\n",
      "Iteration 22: tol = 0.0013742489121423226\n",
      "Iteration 23: tol = 0.0012063716814260336\n",
      "Iteration 24: tol = 0.0010585539268262742\n",
      "Iteration 25: tol = 0.0009285056582307361\n",
      "Iteration 26: tol = 0.0008141727680124563\n",
      "Iteration 27: tol = 0.0007137189219975595\n",
      "Iteration 28: tol = 0.0006255072563945025\n",
      "Iteration 29: tol = 0.0005480823925167159\n",
      "Iteration 30: tol = 0.00048015313770763916\n",
      "Iteration 31: tol = 0.0004205761209236388\n",
      "Iteration 32: tol = 0.00036834051544793833\n",
      "Iteration 33: tol = 0.00032255392716466075\n",
      "Iteration 34: tol = 0.00028242947163362354\n",
      "Iteration 35: tol = 0.0002472740235281775\n",
      "Iteration 36: tol = 0.00021647759466681234\n",
      "Iteration 37: tol = 0.00018950377910949886\n",
      "Iteration 38: tol = 0.00016588119325870545\n",
      "Iteration 39: tol = 0.00014519583372057257\n",
      "Iteration 40: tol = 0.0001270842743388486\n",
      "Iteration 41: tol = 0.00011122762514495094\n",
      "Iteration 42: tol = 9.734617909701182e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.3146958351135254\n",
       "Final loglikelihood:    -331.6518739156732\n",
       "Iterations:             42\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m8×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.503252\n",
       "   2 │     4246     0.590809\n",
       "   3 │     4248    -0.37987\n",
       "   4 │     4717     1.04006\n",
       "   5 │     6290    -0.741734\n",
       "   6 │     7755    -0.437585\n",
       "   7 │     8375    -0.942293\n",
       "   8 │     9415    -2.11206\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m2×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.03892\n",
       "   2 │        2      1.5844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"sim\", \"sim.covariates.txt\", 10, d=Bernoulli(), l=LogitLink())\n",
    "\n",
    "# other responses\n",
    "# result = iht(\"sim\", 10, d=Bernoulli(), l=ProbitLink())     # Logistic regression using ProbitLink\n",
    "# result = iht(\"sim\", 10, d=Poisson(), l=LogLink())          # Poisson regression using canonical link\n",
    "# result = iht(\"sim\", 10, d=NegativeBinomial(), l=LogLink()) # Negative Binomial regression using canonical link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is simulated, we can compare IHT's estimated effect size with the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×2 Array{Float64,2}:\n",
       "  0.469278    0.503252\n",
       "  0.554408    0.590809\n",
       "  0.923213    1.04006\n",
       "  0.0369732   0.0\n",
       " -0.625634   -0.741734\n",
       " -0.526553   -0.437585\n",
       " -0.815561   -0.942293\n",
       " -2.18271    -2.11206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHT found 7/8 genetic predictors, and estimates are reasonably close to truth. IHT missed one SNP with very small effect size ($\\beta = 0.0369$). The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove simulated data once they are no longer needed\n",
    "rm(\"sim.bed\", force=true)\n",
    "rm(\"sim.bim\", force=true)\n",
    "rm(\"sim.fam\", force=true)\n",
    "rm(\"sim.covariates.txt\", force=true)\n",
    "rm(\"sim.phenotypes.txt\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Running IHT on general matrices\n",
    "\n",
    "To run IHT on genotypes in VCF files, or other general data, one must call `fit` and `cv_iht` directly. These functions are designed to work on `AbstractArray{T, 2}` type where `T` is a `Float64` or `Float32`. Thus, one must first import the data, and then call `fit` and `cv_iht` on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    Check out [VCFTools.jl](https://github.com/OpenMendel/VCFTools.jl) to learn how to import VCF data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we simulate some count response using the model:\n",
    "\n",
    "$$y_i \\sim \\rm Poisson(\\mathbf{x}_i^T \\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Normal(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000             # number of samples\n",
    "p = 10000            # number of SNPs\n",
    "k = 10               # 9 causal predictors + intercept\n",
    "d = Poisson          # Response follows Poisson distribution (count data)\n",
    "l = LogLink()        # canonical link\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2020)\n",
    "\n",
    "# simulate design matrix\n",
    "x = randn(n, p)\n",
    "\n",
    "# simulate response, true model b, and the correct non-0 positions of b\n",
    "true_b = zeros(p)\n",
    "true_b[1:k] .= rand(Normal(0, 0.5), k)\n",
    "shuffle!(true_b)\n",
    "intercept = 1.0\n",
    "correct_position = findall(!iszero, true_b)\n",
    "prob = GLM.linkinv.(l, intercept .+ x * true_b)\n",
    "clamp!(prob, -20, 20) # prevents overflow\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # convert phenotypes to double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the response $y$, design matrix $x$. Let's run IHT and compare with truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1489.8188363695676\n",
      "\t2\t707.617523735003\n",
      "\t3\t546.8867981545658\n",
      "\t4\t467.2192708681082\n",
      "\t5\t440.0376189387275\n",
      "\t6\t459.9446241516855\n",
      "\t7\t482.3184687223138\n",
      "\t8\t504.0229684333778\n",
      "\t9\t495.1263330806669\n",
      "\t10\t525.4353275609004\n",
      "\t11\t534.0267905856207\n",
      "\t12\t524.7616148197881\n",
      "\t13\t558.2726852255064\n",
      "\t14\t561.6025100531801\n",
      "\t15\t561.3898895087017\n",
      "\t16\t555.5897051455377\n",
      "\t17\t618.3872529214123\n",
      "\t18\t655.3952109246139\n",
      "\t19\t652.4915677346953\n",
      "\t20\t561.4237250226573\n"
     ]
    }
   ],
   "source": [
    "# first run cross validation \n",
    "mses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.2.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse Poisson regression\n",
      "Link functin = LogLink()\n",
      "Sparsity parameter (k) = 5\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Converging when tol < 0.0001\n",
      "\n",
      "Iteration 1: tol = 0.2928574304111577\n",
      "Iteration 2: tol = 0.05230999409875649\n",
      "Iteration 3: tol = 0.07104164424891493\n",
      "Iteration 4: tol = 0.026208176849564724\n",
      "Iteration 5: tol = 0.02023001613423483\n",
      "Iteration 6: tol = 0.011080308351803689\n",
      "Iteration 7: tol = 0.00930914236578197\n",
      "Iteration 8: tol = 0.00556416943618412\n",
      "Iteration 9: tol = 0.004609772037770406\n",
      "Iteration 10: tol = 0.002861799512474864\n",
      "Iteration 11: tol = 0.002340881298705853\n",
      "Iteration 12: tol = 0.001479832987797599\n",
      "Iteration 13: tol = 0.0012011066579049358\n",
      "Iteration 14: tol = 0.0007661746047595665\n",
      "Iteration 15: tol = 0.0006191995770663082\n",
      "Iteration 16: tol = 0.00039678836835178535\n",
      "Iteration 17: tol = 0.00031993814923964465\n",
      "Iteration 18: tol = 0.00020549845888243352\n",
      "Iteration 19: tol = 0.00016549800033345948\n",
      "Iteration 20: tol = 0.00010642830220001078\n",
      "Iteration 21: tol = 8.565816720868677e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.092864990234375\n",
       "Final loglikelihood:    -2335.176167840737\n",
       "Iterations:             21\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m4×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │       83    -0.809284\n",
       "   2 │      989     0.378376\n",
       "   3 │     4294    -0.274544\n",
       "   4 │     4459     0.169417\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m1×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.26918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run IHT on best k (achieved at k = 5)\n",
    "result = fit(y, x, k=argmin(mses), d=Poisson(), l=LogLink())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       " -1.303      -0.809284\n",
       "  0.585809    0.378376\n",
       " -0.0700563   0.0\n",
       " -0.0901341   0.0\n",
       " -0.0620201   0.0\n",
       " -0.441452   -0.274544\n",
       "  0.271429    0.169417\n",
       " -0.164888    0.0\n",
       " -0.0790484   0.0\n",
       "  0.0829054   0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare IHT result with truth\n",
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many of the true $\\beta$ are small, we were only able to find 5 true signals (4 predictors + intercept). \n",
    "\n",
    "**Conclusion:** In this example, we ran IHT on count response with a general `Array{T, 2}` design matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Group IHT \n",
    "\n",
    "In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most $J = 5$ groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter $k$ is known. \n",
    "\n",
    "### Data simulation\n",
    "To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf). In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; $j = 5$ distinct groups are each assigned $1,2,...,5$ causal SNPs with effect sizes randomly chosen from $\\{−0.2,0.2\\}$. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity $\\lambda_g = 1,2,...,5$. The remaining groups are assigned $\\lambda_g = 1$ (i.e. only 1 active predictor are allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define problem size\n",
    "d = NegativeBinomial\n",
    "l = LogLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "block_size = 20                  #simulation parameter\n",
    "num_blocks = Int(p / block_size) #simulation parameter\n",
    "\n",
    "# set seed\n",
    "Random.seed!(2019)\n",
    "\n",
    "# assign group membership\n",
    "membership = collect(1:num_blocks)\n",
    "g = zeros(Int64, p + 1)\n",
    "for i in 1:length(membership)\n",
    "    for j in 1:block_size\n",
    "        cur_row = block_size * (i - 1) + j\n",
    "        g[block_size*(i - 1) + j] = membership[i]\n",
    "    end\n",
    "end\n",
    "g[end] = membership[end]\n",
    "\n",
    "#simulate correlated snparray\n",
    "x = simulate_correlated_snparray(\"tmp.bed\", n, p)\n",
    "intercept = 0.5\n",
    "x_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n",
    "\n",
    "#simulate true model, where 5 groups each with 1~5 snps contribute\n",
    "true_b = zeros(p)\n",
    "true_groups = randperm(num_blocks)[1:5]\n",
    "sort!(true_groups)\n",
    "within_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n",
    "                randperm(block_size)[1:3], randperm(block_size)[1:4], \n",
    "                randperm(block_size)[1:5]]\n",
    "correct_position = zeros(Int64, 15)\n",
    "for i in 1:5\n",
    "    cur_group = block_size * (true_groups[i] - 1)\n",
    "    cur_group_snps = cur_group .+ within_group[i]\n",
    "    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n",
    "    correct_position[start:last] .= cur_group_snps\n",
    "end\n",
    "for i in 1:15\n",
    "    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\n",
    "end\n",
    "sort!(correct_position)\n",
    "\n",
    "# simulate phenotype\n",
    "r = 10 #nuisance parameter\n",
    "μ = GLM.linkinv.(l, intercept .+ x_float * true_b)\n",
    "clamp!(μ, -20, 20)\n",
    "prob = 1 ./ (1 .+ μ ./ r)\n",
    "y = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\n",
    "y = Float64.(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run IHT without groups\n",
    "ungrouped = fit(y, x_float, k=15, d=NegativeBinomial(), l=LogLink(), verbose=false)\n",
    "\n",
    "#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\n",
    "max_group_snps = ones(Int, num_blocks)\n",
    "max_group_snps[true_groups] .= collect(1:5)\n",
    "variable_group = fit(y, x_float, d=NegativeBinomial(), l=LogLink(), k=max_group_snps, J=5, group=g, verbose=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, ungroup IHT found 1 more SNPs than grouped IHT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 15×4 DataFrame\n",
      " Row │ position  correct_β  ungrouped_IHT_β  grouped_IHT_β\n",
      "     │ Int64     Float64    Float64          Float64\n",
      "─────┼─────────────────────────────────────────────────────\n",
      "   1 │      235       -0.2        -0.218172       0.0\n",
      "   2 │     2673       -0.2        -0.171002      -0.178483\n",
      "   3 │     2679       -0.2        -0.236793      -0.213098\n",
      "   4 │     6383       -0.2        -0.228555      -0.224309\n",
      "   5 │     6389       -0.2        -0.190352      -0.192022\n",
      "   6 │     6394        0.2         0.215984       0.198447\n",
      "   7 │     7862        0.2         0.229254       0.224207\n",
      "   8 │     7864       -0.2        -0.184551      -0.19331\n",
      "   9 │     7868       -0.2        -0.174773      -0.177359\n",
      "  10 │     7870       -0.2        -0.192932      -0.208592\n",
      "  11 │     9481       -0.2         0.0            0.0\n",
      "  12 │     9491        0.2         0.0            0.0\n",
      "  13 │     9493        0.2         0.183659       0.175211\n",
      "  14 │     9494        0.2         0.117548       0.112946\n",
      "  15 │     9499       -0.2         0.0            0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check result\n",
    "correct_position = findall(!iszero, true_b)\n",
    "compare_model = DataFrame(\n",
    "    position = correct_position,\n",
    "    correct_β = true_b[correct_position],\n",
    "    ungrouped_IHT_β = ungrouped.beta[correct_position], \n",
    "    grouped_IHT_β = variable_group.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Linear Regression with prior weights\n",
    "\n",
    "In this example, we show how to include (predetermined) prior weights for each SNP. You can check out [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf) for references of why/how to choose these weights. In this case, we mimic our paper and randomly set $10\\%$ of all SNPs to have a weight of $2.0$. Other predictors have weight of $1.0$. All causal SNPs have weights of $2.0$. Under this scenario, SNPs with weight $2.0$ is twice as likely to enter the model identified by IHT. \n",
    "\n",
    "Our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\epsilon_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.25)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Normal\n",
    "l = IdentityLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "\n",
    "#random seed\n",
    "Random.seed!(4)\n",
    "\n",
    "# construct snpmatrix, covariate files, and true model b\n",
    "x = simulate_random_snparray(\"tmp.bed\", n, p)\n",
    "X = convert(Matrix{Float64}, x, center=true, scale=true)\n",
    "intercept = 1.0\n",
    "    \n",
    "#define true_b \n",
    "true_b = zeros(p)\n",
    "true_b[1:10] .= rand(Normal(0, 0.25), k)\n",
    "shuffle!(true_b)\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "#simulate phenotypes (e.g. vector y)\n",
    "prob = GLM.linkinv.(l, intercept .+ X * true_b)\n",
    "clamp!(prob, -20, 20)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y);\n",
    "\n",
    "# construct weight vector\n",
    "w = ones(p + 1)\n",
    "w[correct_position] .= 2.0\n",
    "one_tenth = round(Int, p/10)\n",
    "idx = rand(1:p, one_tenth)\n",
    "w[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      " Row │ position  correct     unweighted  weighted\n",
      "     │ Int64     Float64     Float64     Float64\n",
      "─────┼─────────────────────────────────────────────\n",
      "   1 │     1264   0.252886     0.270233   0.264713\n",
      "   2 │     1506  -0.0939841    0.0       -0.125803\n",
      "   3 │     4866  -0.227394    -0.233703  -0.237007\n",
      "   4 │     5778  -0.510488    -0.507114  -0.494199\n",
      "   5 │     5833  -0.311969    -0.324309  -0.322663\n",
      "   6 │     5956  -0.0548168    0.0        0.0\n",
      "   7 │     6378  -0.0155173    0.0        0.0\n",
      "   8 │     7007  -0.123301     0.0        0.0\n",
      "   9 │     7063   0.0183886    0.0        0.0\n",
      "  10 │     7995  -0.102122     0.0       -0.142201\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run weighted and unweighted IHT\n",
    "unweighted = fit(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false)\n",
    "weighted   = fit(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false, weight=w)\n",
    "\n",
    "#check result\n",
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    correct     = true_b[correct_position],\n",
    "    unweighted  = unweighted.beta[correct_position], \n",
    "    weighted    = weighted.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted IHT found 2 extra predictor than non-weighted IHT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples and functionalities\n",
    "\n",
    "Other examples explored in our manuscript has [reproducible code](https://github.com/biona001/MendelIHT.jl/tree/master/figures). \n",
    "\n",
    "Additional features are available as optional parameters in the [fit](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/fit.jl#L31) function, but they should be treated as **experimental** features. Interested users are encouraged to explore them and please file issues on GitHub if you encounter a problem."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

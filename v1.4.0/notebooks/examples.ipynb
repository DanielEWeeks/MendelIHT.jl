{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples\n",
    "\n",
    "Here we give numerous example analysis of GWAS data with `MendelIHT.jl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.5.0\n",
      "Commit 96786e22cc (2020-08-01 23:44 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.7.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "# machine information for reproducibility\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add workers needed for parallel computing. Add only as many CPU cores available\n",
    "using Distributed\n",
    "addprocs(4)\n",
    "\n",
    "#load necessary packages for running all examples below\n",
    "@everywhere begin\n",
    "    using Revise\n",
    "    using MendelIHT\n",
    "    using SnpArrays\n",
    "    using DataFrames\n",
    "    using Distributions\n",
    "    using Random\n",
    "    using LinearAlgebra\n",
    "    using GLM\n",
    "    using DelimitedFiles\n",
    "    using Statistics\n",
    "    using BenchmarkTools\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MendelIHT.jl\n",
    "\n",
    "Users are exposed to 2 levels of interface:\n",
    "+ Wrapper functions [iht()](https://openmendel.github.io/MendelIHT.jl/latest/man/api/#MendelIHT.iht) and [cross_validate()](https://openmendel.github.io/MendelIHT.jl/latest/man/api/#MendelIHT.cross_validate). These functions are simple scripts that import data, runs IHT, and writes result to output automatically. Since they are very simplistic, they might fail for whatever reason (please file an issue on GitHub). If so, please use:\n",
    "+ Core functions [fit_iht()](https://openmendel.github.io/MendelIHT.jl/latest/man/api/#MendelIHT.fit_iht) and [cv_iht()](https://openmendel.github.io/MendelIHT.jl/latest/man/api/#MendelIHT.cv_iht). Input arguments for these functions must be first imported into Julia by the user manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: GWAS with PLINK files\n",
    "\n",
    "In this example, our data are stored in binary PLINK files:\n",
    "\n",
    "+ `normal.bed`\n",
    "+ `normal.bim`\n",
    "+ `normal.fam`\n",
    "\n",
    "which contains simulated (Gaussian) phenotypes for $n=1000$ samples and $p=10,000$ SNPs. There are $8$ causal variants and 2 causal non-genetic covariates (intercept and sex). \n",
    "\n",
    "These data are present under `MendelIHT/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13-element Array{String,1}:\n",
       " \".DS_Store\"\n",
       " \"README.md\"\n",
       " \"covariates.txt\"\n",
       " \"cviht.summary.txt\"\n",
       " \"iht.beta.txt\"\n",
       " \"iht.summary.txt\"\n",
       " \"normal.bed\"\n",
       " \"normal.bim\"\n",
       " \"normal.fam\"\n",
       " \"normal_true_beta.txt\"\n",
       " \"phenotypes.txt\"\n",
       " \"simulate.jl\"\n",
       " \"univariate\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory to where example data is located\n",
    "cd(normpath(MendelIHT.datadir()))\n",
    "\n",
    "# show working directory\n",
    "@show pwd() \n",
    "\n",
    "# show files in current directory\n",
    "readdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `covariates.txt` contains non-genetic covariates, `normal.bed/bim/fam` are the PLINK files storing genetic covariates, `phenotypes.txt` are phenotypes for each sample, `normal_true_beta.txt` is the true statistical model used to generate the phenotypes, and `simulate.jl` is the script used to generate all the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run cross validation to determine best model size\n",
    "\n",
    "Here phenotypes are stored in the 6th column of `.fam` file. Other covariates are stored separately (which includes a column of 1 as intercept). Here we cross validate $k = 1,2,...20$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:35\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1407.2735970192766\n",
      "\t2\t858.9323667647981\n",
      "\t3\t695.3281033011649\n",
      "\t4\t574.9357159487766\n",
      "\t5\t426.30145951172085\n",
      "\t6\t336.31511946184327\n",
      "\t7\t289.01531777955694\n",
      "\t8\t230.659699154335\n",
      "\t9\t195.438939949433\n",
      "\t10\t199.83469223996426\n",
      "\t11\t201.34513294669145\n",
      "\t12\t203.75379485200406\n",
      "\t13\t208.37926053125014\n",
      "\t14\t213.51428971882075\n",
      "\t15\t221.47325404994524\n",
      "\t16\t219.64716813029995\n",
      "\t17\t221.40881497802621\n",
      "\t18\t227.25440479675385\n",
      "\t19\t235.0540681425773\n",
      "\t20\t236.588333388475\n",
      "\n",
      "Best k = 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=6, path=1:20);\n",
    "\n",
    "# Alternative syntax\n",
    "# mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=6, path=[1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n",
    "# mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=\"phenotypes.txt\", path=1:20) # when phenotypes are stored separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not be alarmed if you get slightly different numbers, because cross validation breaks data into training/testing randomly. Set a seed by `Random.seed!(1234)` if you want reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run IHT on best k\n",
    "\n",
    "According to cross validation, `k = 9` achieves the minimum MSE. Thus we run IHT on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.4.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse linear regression\n",
      "Link functin = IdentityLink()\n",
      "Sparsity parameter (k) = 9\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -1407.0715526824579, backtracks = 0, tol = 0.7903070014734823\n",
      "Iteration 2: loglikelihood = -1397.916093195947, backtracks = 0, tol = 0.025737058258134597\n",
      "Iteration 3: loglikelihood = -1397.881115073396, backtracks = 0, tol = 0.0016722147159225114\n",
      "Iteration 4: loglikelihood = -1397.8807458850451, backtracks = 0, tol = 0.0001440850615835579\n",
      "Iteration 5: loglikelihood = -1397.8807416471927, backtracks = 0, tol = 1.674491317444967e-5\n",
      "result = \n",
      "IHT estimated 7 nonzero SNP predictors and 2 non-genetic predictors.\n",
      "\n",
      "Compute time (sec):     0.3655970096588135\n",
      "Final loglikelihood:    -1397.8807416471927\n",
      "SNP PVE:                0.834374956294435\n",
      "Iterations:             5\n",
      "\n",
      "Selected genetic predictors:\n",
      "7×2 DataFrame\n",
      " Row │ Position  Estimated_β\n",
      "     │ Int64     Float64\n",
      "─────┼───────────────────────\n",
      "   1 │     3137     0.424377\n",
      "   2 │     4246     0.52343\n",
      "   3 │     4717     0.922857\n",
      "   4 │     6290    -0.677832\n",
      "   5 │     7755    -0.542981\n",
      "   6 │     8375    -0.792815\n",
      "   7 │     9415    -2.17998\n",
      "\n",
      "Selected nongenetic predictors:\n",
      "2×2 DataFrame\n",
      " Row │ Position  Estimated_β\n",
      "     │ Int64     Float64\n",
      "─────┼───────────────────────\n",
      "   1 │        1     1.65223\n",
      "   2 │        2     0.749866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 7 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.3655970096588135\n",
       "Final loglikelihood:    -1397.8807416471927\n",
       "SNP PVE:                0.834374956294435\n",
       "Iterations:             5\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m7×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.424377\n",
       "   2 │     4246     0.52343\n",
       "   3 │     4717     0.922857\n",
       "   4 │     6290    -0.677832\n",
       "   5 │     7755    -0.542981\n",
       "   6 │     8375    -0.792815\n",
       "   7 │     9415    -2.17998\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m2×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1     1.65223\n",
       "   2 │        2     0.749866"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"normal\", 9, Normal, covariates=\"covariates.txt\", phenotypes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examine results\n",
    "\n",
    "IHT picked 7 SNPs and 2 non-genetic predictors: intercept and sex. The `Position` argument corresponds to the order in which the SNP appeared in the PLINK file, and the `Estimated_β` argument is the estimated effect size for the selected SNPs. To extract more information (for instance to extract `rs` IDs), we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_snps = 7×6 DataFrame\n",
      " Row │ chromosome  snpid    genetic_distance  position  allele1  allele2\n",
      "     │ String      String   Float64           Int64     String   String\n",
      "─────┼───────────────────────────────────────────────────────────────────\n",
      "   1 │ 1           snp3137               0.0         1  1        2\n",
      "   2 │ 1           snp4246               0.0         1  1        2\n",
      "   3 │ 1           snp4717               0.0         1  1        2\n",
      "   4 │ 1           snp6290               0.0         1  1        2\n",
      "   5 │ 1           snp7755               0.0         1  1        2\n",
      "   6 │ 1           snp8375               0.0         1  1        2\n",
      "   7 │ 1           snp9415               0.0         1  1        2\n"
     ]
    }
   ],
   "source": [
    "snpdata = SnpData(\"normal\")                   # import PLINK information\n",
    "snps_idx = findall(!iszero, result.beta)      # indices of SNPs selected by IHT\n",
    "selected_snps = snpdata.snp_info[snps_idx, :] # see which SNPs are selected\n",
    "@show selected_snps;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above displays the SNP information for the selected SNPs. \n",
    "\n",
    "Since data is simulated, the fields `chromosome`, `snpid`, `genetic_distance`, `position`, `allele1`, and `allele2` are fake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: How to simulate data\n",
    "\n",
    "Here we demonstrate how to use `MendelIHT.jl` and [SnpArrays.jl](https://github.com/OpenMendel/SnpArrays.jl) to simulate data, allowing you to design your own genetic studies. Note:\n",
    "+ For more complex simulation, please use the module [TraitSimulations.jl](https://github.com/OpenMendel/TraitSimulation.jl).  \n",
    "+ All linear algebra routines involving PLINK files are handled by [SnpArrays.jl](https://github.com/OpenMendel/SnpArrays.jl). \n",
    "\n",
    "First we simulate an example PLINK trio (`.bim`, `.bed`, `.fam`) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:\n",
    "\n",
    "$$y_i \\sim \\rm Bernoulli(\\mathbf{x}_i^T\\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_{\\rm intercept} = 1$$\n",
    "$$\\beta_{\\rm sex} = 1.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000            # number of samples\n",
    "p = 10000           # number of SNPs\n",
    "k = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\n",
    "d = Bernoulli       # Binary (continuous) phenotypes\n",
    "l = LogitLink()     # canonical link function\n",
    "\n",
    "# set random seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "# simulate `sim.bed` file with no missing data\n",
    "x = simulate_random_snparray(\"sim.bed\", n, p)\n",
    "xla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n",
    "\n",
    "# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\n",
    "z = ones(n, 2) \n",
    "z[:, 2] .= rand(0:1, n)\n",
    "standardize!(@view(z[:, 2:end])) \n",
    "\n",
    "# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\n",
    "true_b = zeros(p) \n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "\n",
    "# find correct position of genetic predictors\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "# define effect size of non-genetic predictors: intercept & sex\n",
    "true_c = [1.0; 1.5] \n",
    "\n",
    "# simulate phenotype using genetic and nongenetic predictors\n",
    "prob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # turn y into floating point numbers\n",
    "\n",
    "# create `sim.bim` and `sim.bam` files using phenotype\n",
    "make_bim_fam_files(x, y, \"sim\")\n",
    "\n",
    "#save covariates and phenotypes (without header)\n",
    "writedlm(\"sim.covariates.txt\", z, ',')\n",
    "writedlm(\"sim.phenotypes.txt\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    Please **standardize** (or at least center) your non-genetic covariates. If you use our `iht()` or `cross_validation()` functions, standardization is automatic. For genotype matrix, `SnpLinAlg` efficiently achieves this standardization. For non-genetic covariates, please use the built-in function `standardize!`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logistic/Poisson/Negative-binomial GWAS\n",
    "\n",
    "In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true $k = 10$. Let's try running a logistic regression (i.e. phenotype follows the Bernoulli distribution) on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.4.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse logistic regression\n",
      "Link functin = LogitLink()\n",
      "Sparsity parameter (k) = 10\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -403.91876912829684, backtracks = 0, tol = 0.5882274462947444\n",
      "Iteration 2: loglikelihood = -354.24157363893784, backtracks = 0, tol = 0.282513866845802\n",
      "Iteration 3: loglikelihood = -347.5483388154264, backtracks = 0, tol = 0.19289827584644761\n",
      "Iteration 4: loglikelihood = -335.9715247464944, backtracks = 0, tol = 0.1426996228393492\n",
      "Iteration 5: loglikelihood = -334.49756712078744, backtracks = 1, tol = 0.02283147714926763\n",
      "Iteration 6: loglikelihood = -333.543219571108, backtracks = 2, tol = 0.019792429262652955\n",
      "Iteration 7: loglikelihood = -332.8067268854347, backtracks = 2, tol = 0.019845664939460095\n",
      "Iteration 8: loglikelihood = -332.5588563458224, backtracks = 3, tol = 0.00765066824120313\n",
      "Iteration 9: loglikelihood = -332.3619297572997, backtracks = 3, tol = 0.006913691748350025\n",
      "Iteration 10: loglikelihood = -332.2064289061609, backtracks = 3, tol = 0.0061597575486623014\n",
      "Iteration 11: loglikelihood = -332.0840431892422, backtracks = 3, tol = 0.0054730856932040705\n",
      "Iteration 12: loglikelihood = -331.98800416752806, backtracks = 3, tol = 0.0048548461339783565\n",
      "Iteration 13: loglikelihood = -331.9128408595039, backtracks = 3, tol = 0.004300010671833966\n",
      "Iteration 14: loglikelihood = -331.85415737528945, backtracks = 3, tol = 0.0038033387186573\n",
      "Iteration 15: loglikelihood = -331.8084401845103, backtracks = 3, tol = 0.003359795402130408\n",
      "Iteration 16: loglikelihood = -331.77289417483837, backtracks = 3, tol = 0.0029645876127234955\n",
      "Iteration 17: loglikelihood = -331.74530513071767, backtracks = 3, tol = 0.0026131815201996976\n",
      "Iteration 18: loglikelihood = -331.72392566719304, backtracks = 3, tol = 0.002301318021364227\n",
      "Iteration 19: loglikelihood = -331.707381516887, backtracks = 3, tol = 0.002025024729429744\n",
      "Iteration 20: loglikelihood = -331.6945951739729, backtracks = 3, tol = 0.001780622915677454\n",
      "Iteration 21: loglikelihood = -331.6847241325898, backtracks = 3, tol = 0.0015647287330161268\n",
      "Iteration 22: loglikelihood = -331.6771112518333, backtracks = 3, tol = 0.0013742489121423226\n",
      "Iteration 23: loglikelihood = -331.671245094309, backtracks = 3, tol = 0.0012063716814260336\n",
      "Iteration 24: loglikelihood = -331.6667283950377, backtracks = 3, tol = 0.0010585539268262742\n",
      "Iteration 25: loglikelihood = -331.66325310683357, backtracks = 3, tol = 0.0009285056582307361\n",
      "Iteration 26: loglikelihood = -331.66058072924545, backtracks = 3, tol = 0.0008141727680124563\n",
      "Iteration 27: loglikelihood = -331.6585268570943, backtracks = 3, tol = 0.0007137189219975595\n",
      "Iteration 28: loglikelihood = -331.6569490812431, backtracks = 3, tol = 0.0006255072563945025\n",
      "Iteration 29: loglikelihood = -331.65573754039684, backtracks = 3, tol = 0.0005480823925167159\n",
      "Iteration 30: loglikelihood = -331.6548075608974, backtracks = 3, tol = 0.00048015313770763916\n",
      "Iteration 31: loglikelihood = -331.65409393532707, backtracks = 3, tol = 0.0004205761209236388\n",
      "Iteration 32: loglikelihood = -331.6535464833441, backtracks = 3, tol = 0.00036834051544793833\n",
      "Iteration 33: loglikelihood = -331.6531266130592, backtracks = 3, tol = 0.00032255392716466075\n",
      "Iteration 34: loglikelihood = -331.6528046612902, backtracks = 3, tol = 0.00028242947163362354\n",
      "Iteration 35: loglikelihood = -331.65255783889444, backtracks = 3, tol = 0.0002472740235281775\n",
      "Iteration 36: loglikelihood = -331.6523686452689, backtracks = 3, tol = 0.00021647759466681234\n",
      "Iteration 37: loglikelihood = -331.652223646102, backtracks = 3, tol = 0.00018950377910949886\n",
      "Iteration 38: loglikelihood = -331.6521125319518, backtracks = 3, tol = 0.00016588119325870545\n",
      "Iteration 39: loglikelihood = -331.6520273936599, backtracks = 3, tol = 0.00014519583372057257\n",
      "Iteration 40: loglikelihood = -331.65196216503995, backtracks = 3, tol = 0.0001270842743388486\n",
      "Iteration 41: loglikelihood = -331.65191219446064, backtracks = 3, tol = 0.00011122762514495094\n",
      "Iteration 42: loglikelihood = -331.6518739156732, backtracks = 3, tol = 9.734617909701182e-5\n",
      "result = \n",
      "IHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n",
      "\n",
      "Compute time (sec):     2.5963549613952637\n",
      "Final loglikelihood:    -331.6518739156732\n",
      "SNP PVE:                0.4798854810844273\n",
      "Iterations:             42\n",
      "\n",
      "Selected genetic predictors:\n",
      "8×2 DataFrame\n",
      " Row │ Position  Estimated_β\n",
      "     │ Int64     Float64\n",
      "─────┼───────────────────────\n",
      "   1 │     3137     0.503252\n",
      "   2 │     4246     0.590809\n",
      "   3 │     4248    -0.37987\n",
      "   4 │     4717     1.04006\n",
      "   5 │     6290    -0.741734\n",
      "   6 │     7755    -0.437585\n",
      "   7 │     8375    -0.942293\n",
      "   8 │     9415    -2.11206\n",
      "\n",
      "Selected nongenetic predictors:\n",
      "2×2 DataFrame\n",
      " Row │ Position  Estimated_β\n",
      "     │ Int64     Float64\n",
      "─────┼───────────────────────\n",
      "   1 │        1      1.03892\n",
      "   2 │        2      1.5844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     2.5963549613952637\n",
       "Final loglikelihood:    -331.6518739156732\n",
       "SNP PVE:                0.4798854810844273\n",
       "Iterations:             42\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m8×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │     3137     0.503252\n",
       "   2 │     4246     0.590809\n",
       "   3 │     4248    -0.37987\n",
       "   4 │     4717     1.04006\n",
       "   5 │     6290    -0.741734\n",
       "   6 │     7755    -0.437585\n",
       "   7 │     8375    -0.942293\n",
       "   8 │     9415    -2.11206\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m2×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.03892\n",
       "   2 │        2      1.5844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = iht(\"sim\", 10, Bernoulli, covariates=\"sim.covariates.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is simulated, we can compare IHT's estimated effect size with the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×2 Array{Float64,2}:\n",
       "  0.469278    0.503252\n",
       "  0.554408    0.590809\n",
       "  0.923213    1.04006\n",
       "  0.0369732   0.0\n",
       " -0.625634   -0.741734\n",
       " -0.526553   -0.437585\n",
       " -0.815561   -0.942293\n",
       " -2.18271    -2.11206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st column are the true beta values, and the 2nd column is the estimated values. IHT found 7/8 genetic predictors, and estimates are reasonably close to truth. IHT missed one SNP with very small effect size ($\\beta = 0.0369$). The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove simulated data once they are no longer needed\n",
    "rm(\"sim.bed\", force=true)\n",
    "rm(\"sim.bim\", force=true)\n",
    "rm(\"sim.fam\", force=true)\n",
    "rm(\"sim.covariates.txt\", force=true)\n",
    "rm(\"sim.phenotypes.txt\", force=true)\n",
    "rm(\"iht.beta.txt\", force=true)\n",
    "rm(\"iht.summary.txt\", force=true)\n",
    "rm(\"cviht.summary.txt\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Running IHT on general matrices\n",
    "\n",
    "To run IHT on genotypes in VCF files, or other general data, one must call `fit_iht` and `cv_iht` directly. These functions are designed to work on `AbstractArray{T, 2}` type where `T` is a `Float64` or `Float32`. Thus, one must first import the data, and then call `fit_iht` and `cv_iht` on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "\n",
    "    Check out [VCFTools.jl](https://github.com/OpenMendel/VCFTools.jl) to learn how to import VCF data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we simulate some count response using the model:\n",
    "\n",
    "$$y_i \\sim \\rm Poisson(\\mathbf{x}_i^T \\boldsymbol\\beta)$$\n",
    "$$x_{ij} \\sim \\rm Normal(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000             # number of samples\n",
    "p = 10000            # number of SNPs\n",
    "k = 10               # 9 causal predictors + intercept\n",
    "d = Poisson          # Response follows Poisson distribution (count data)\n",
    "l = LogLink()        # canonical link\n",
    "\n",
    "# set random seed for reproducibility\n",
    "Random.seed!(2020)\n",
    "\n",
    "# simulate design matrix\n",
    "x = randn(n, p)\n",
    "\n",
    "# simulate response, true model b, and the correct non-0 positions of b\n",
    "true_b = zeros(p)\n",
    "true_b[1:k] .= rand(Normal(0, 0.5), k)\n",
    "shuffle!(true_b)\n",
    "intercept = 1.0\n",
    "correct_position = findall(!iszero, true_b)\n",
    "prob = GLM.linkinv.(l, intercept .+ x * true_b)\n",
    "clamp!(prob, -20, 20) # prevents overflow\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y); # convert phenotypes to double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the response $y$, design matrix $x$. Let's run IHT and compare with truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:12\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t1489.8188363695676\n",
      "\t2\t707.6175237350031\n",
      "\t3\t546.8867981545659\n",
      "\t4\t467.2192708681082\n",
      "\t5\t440.03761893872735\n",
      "\t6\t459.9446241516855\n",
      "\t7\t482.3184687223138\n",
      "\t8\t504.0229684333779\n",
      "\t9\t495.12633308066677\n",
      "\t10\t525.4353275609003\n",
      "\t11\t534.0267905856207\n",
      "\t12\t524.761614819788\n",
      "\t13\t558.2726852255062\n",
      "\t14\t561.6025100531801\n",
      "\t15\t561.3898895087017\n",
      "\t16\t555.5897051455378\n",
      "\t17\t618.3872529214121\n",
      "\t18\t655.395210924614\n",
      "\t19\t652.4915677346956\n",
      "\t20\t561.4237250226572\n",
      "\n",
      "Best k = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first run cross validation \n",
    "mses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run IHT on the full dataset using the best k (achieved at k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****                   MendelIHT Version 1.4.0                  ****\n",
      "****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n",
      "****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n",
      "****                                                            ****\n",
      "****                 Please cite our paper!                     ****\n",
      "****         https://doi.org/10.1093/gigascience/giaa044        ****\n",
      "\n",
      "Running sparse Poisson regression\n",
      "Link functin = LogLink()\n",
      "Sparsity parameter (k) = 5\n",
      "Prior weight scaling = off\n",
      "Doubly sparse projection = off\n",
      "Debias = off\n",
      "Max IHT iterations = 200\n",
      "Converging when tol < 0.0001:\n",
      "\n",
      "Iteration 1: loglikelihood = -2847.082501924986, backtracks = 0, tol = 0.2928574304111579\n",
      "Iteration 2: loglikelihood = -2465.401434829009, backtracks = 0, tol = 0.05230999409875657\n",
      "Iteration 3: loglikelihood = -2376.6519599956155, backtracks = 0, tol = 0.07104164424891486\n",
      "Iteration 4: loglikelihood = -2351.5833133504116, backtracks = 0, tol = 0.026208176849564724\n",
      "Iteration 5: loglikelihood = -2343.11078282251, backtracks = 0, tol = 0.02023001613423483\n",
      "Iteration 6: loglikelihood = -2339.0692529047387, backtracks = 0, tol = 0.011080308351803736\n",
      "Iteration 7: loglikelihood = -2337.149479249759, backtracks = 0, tol = 0.00930914236578197\n",
      "Iteration 8: loglikelihood = -2336.1781402958745, backtracks = 0, tol = 0.00556416943618412\n",
      "Iteration 9: loglikelihood = -2335.6908426969235, backtracks = 0, tol = 0.004609772037770406\n",
      "Iteration 10: loglikelihood = -2335.440388139586, backtracks = 0, tol = 0.002861799512474864\n",
      "Iteration 11: loglikelihood = -2335.3124548737906, backtracks = 0, tol = 0.002340881298705853\n",
      "Iteration 12: loglikelihood = -2335.2463824561282, backtracks = 0, tol = 0.001479832987797599\n",
      "Iteration 13: loglikelihood = -2335.2123851939327, backtracks = 0, tol = 0.0012011066579049358\n",
      "Iteration 14: loglikelihood = -2335.1947979604856, backtracks = 0, tol = 0.0007661746047595665\n",
      "Iteration 15: loglikelihood = -2335.1857186752313, backtracks = 0, tol = 0.0006191995770663082\n",
      "Iteration 16: loglikelihood = -2335.1810189052294, backtracks = 0, tol = 0.00039678836835178535\n",
      "Iteration 17: loglikelihood = -2335.178588840017, backtracks = 0, tol = 0.00031993814923964465\n",
      "Iteration 18: loglikelihood = -2335.177330620363, backtracks = 0, tol = 0.00020549845888243352\n",
      "Iteration 19: loglikelihood = -2335.1766795324024, backtracks = 0, tol = 0.00016549800033345948\n",
      "Iteration 20: loglikelihood = -2335.176342377269, backtracks = 0, tol = 0.00010642830220001078\n",
      "Iteration 21: loglikelihood = -2335.176167840737, backtracks = 0, tol = 8.565816720868677e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "IHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n",
       "\n",
       "Compute time (sec):     0.11967015266418457\n",
       "Final loglikelihood:    -2335.176167840737\n",
       "SNP PVE:                0.09113449276174615\n",
       "Iterations:             21\n",
       "\n",
       "Selected genetic predictors:\n",
       "\u001b[1m4×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │       83    -0.809284\n",
       "   2 │      989     0.378376\n",
       "   3 │     4294    -0.274544\n",
       "   4 │     4459     0.169417\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "\u001b[1m1×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼───────────────────────\n",
       "   1 │        1      1.26918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fit_iht(y, x, k=argmin(mses), d=Poisson(), l=LogLink())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       " -1.303      -0.809284\n",
       "  0.585809    0.378376\n",
       " -0.0700563   0.0\n",
       " -0.0901341   0.0\n",
       " -0.0620201   0.0\n",
       " -0.441452   -0.274544\n",
       "  0.271429    0.169417\n",
       " -0.164888    0.0\n",
       " -0.0790484   0.0\n",
       "  0.0829054   0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare IHT result with truth\n",
    "[true_b[correct_position] result.beta[correct_position]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many of the true $\\beta$ are small, we were only able to find 5 true signals (4 predictors + intercept). \n",
    "\n",
    "**Conclusion:** In this example, we ran IHT on count response with a general `Array{T, 2}` design matrix. Since we used simulated data, we could compare IHT's estimates with the truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Group IHT \n",
    "\n",
    "In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most $J = 5$ groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter $k$ is known. \n",
    "\n",
    "### Data simulation\n",
    "To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf). In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; $j = 5$ distinct groups are each assigned $1,2,...,5$ causal SNPs with effect sizes randomly chosen from $\\{−0.2,0.2\\}$. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity $\\lambda_g = 1,2,...,5$. The remaining groups are assigned $\\lambda_g = 1$ (i.e. only 1 active predictor are allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define problem size\n",
    "d = NegativeBinomial\n",
    "l = LogLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "block_size = 20                  #simulation parameter\n",
    "num_blocks = Int(p / block_size) #simulation parameter\n",
    "\n",
    "# set seed\n",
    "Random.seed!(2019)\n",
    "\n",
    "# assign group membership\n",
    "membership = collect(1:num_blocks)\n",
    "g = zeros(Int64, p + 1)\n",
    "for i in 1:length(membership)\n",
    "    for j in 1:block_size\n",
    "        cur_row = block_size * (i - 1) + j\n",
    "        g[block_size*(i - 1) + j] = membership[i]\n",
    "    end\n",
    "end\n",
    "g[end] = membership[end]\n",
    "\n",
    "#simulate correlated snparray\n",
    "x = simulate_correlated_snparray(\"tmp.bed\", n, p)\n",
    "intercept = 0.5\n",
    "x_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n",
    "\n",
    "#simulate true model, where 5 groups each with 1~5 snps contribute\n",
    "true_b = zeros(p)\n",
    "true_groups = randperm(num_blocks)[1:5]\n",
    "sort!(true_groups)\n",
    "within_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n",
    "                randperm(block_size)[1:3], randperm(block_size)[1:4], \n",
    "                randperm(block_size)[1:5]]\n",
    "correct_position = zeros(Int64, 15)\n",
    "for i in 1:5\n",
    "    cur_group = block_size * (true_groups[i] - 1)\n",
    "    cur_group_snps = cur_group .+ within_group[i]\n",
    "    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n",
    "    correct_position[start:last] .= cur_group_snps\n",
    "end\n",
    "for i in 1:15\n",
    "    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\n",
    "end\n",
    "sort!(correct_position)\n",
    "\n",
    "# simulate phenotype\n",
    "r = 10 #nuisance parameter\n",
    "μ = GLM.linkinv.(l, intercept .+ x_float * true_b)\n",
    "clamp!(μ, -20, 20)\n",
    "prob = 1 ./ (1 .+ μ ./ r)\n",
    "y = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\n",
    "y = Float64.(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run IHT without groups\n",
    "ungrouped = fit_iht(y, x_float, k=15, d=NegativeBinomial(), l=LogLink(), verbose=false)\n",
    "\n",
    "#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\n",
    "max_group_snps = ones(Int, num_blocks)\n",
    "max_group_snps[true_groups] .= collect(1:5)\n",
    "variable_group = fit_iht(y, x_float, d=NegativeBinomial(), l=LogLink(), k=max_group_snps, J=5, group=g, verbose=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 15×4 DataFrame\n",
      " Row │ position  correct_β  ungrouped_IHT_β  grouped_IHT_β\n",
      "     │ Int64     Float64    Float64          Float64\n",
      "─────┼─────────────────────────────────────────────────────\n",
      "   1 │      235       -0.2        -0.218172       0.0\n",
      "   2 │     2673       -0.2        -0.171002      -0.178483\n",
      "   3 │     2679       -0.2        -0.236793      -0.213098\n",
      "   4 │     6383       -0.2        -0.228555      -0.224309\n",
      "   5 │     6389       -0.2        -0.190352      -0.192022\n",
      "   6 │     6394        0.2         0.215984       0.198447\n",
      "   7 │     7862        0.2         0.229254       0.224207\n",
      "   8 │     7864       -0.2        -0.184551      -0.19331\n",
      "   9 │     7868       -0.2        -0.174773      -0.177359\n",
      "  10 │     7870       -0.2        -0.192932      -0.208592\n",
      "  11 │     9481       -0.2         0.0            0.0\n",
      "  12 │     9491        0.2         0.0            0.0\n",
      "  13 │     9493        0.2         0.183659       0.175211\n",
      "  14 │     9494        0.2         0.117548       0.112946\n",
      "  15 │     9499       -0.2         0.0            0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check result\n",
    "correct_position = findall(!iszero, true_b)\n",
    "compare_model = DataFrame(\n",
    "    position = correct_position,\n",
    "    correct_β = true_b[correct_position],\n",
    "    ungrouped_IHT_β = ungrouped.beta[correct_position], \n",
    "    grouped_IHT_β = variable_group.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Ungroup IHT actually found 1 more SNPs than grouped IHT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Linear Regression with prior weights\n",
    "\n",
    "In this example, we show how to include (predetermined) prior weights for each SNP. You can check out [our paper](https://www.biorxiv.org/content/biorxiv/early/2019/11/19/697755.full.pdf) for references of why/how to choose these weights. In this case, we mimic our paper and randomly set $10\\%$ of all SNPs to have a weight of $2.0$. Other predictors have weight of $1.0$. All causal SNPs have weights of $2.0$. Under this scenario, SNPs with weight $2.0$ is twice as likely to enter the model identified by IHT. \n",
    "\n",
    "Our model is simulated as:\n",
    "\n",
    "$$y_i \\sim \\mathbf{x}_i^T\\mathbf{\\beta} + \\epsilon_i$$\n",
    "$$x_{ij} \\sim \\rm Binomial(2, \\rho_j)$$\n",
    "$$\\rho_j \\sim \\rm Uniform(0, 0.5)$$\n",
    "$$\\epsilon_i \\sim \\rm N(0, 1)$$\n",
    "$$\\beta_i \\sim \\rm N(0, 0.25)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Normal\n",
    "l = IdentityLink()\n",
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "\n",
    "#random seed\n",
    "Random.seed!(4)\n",
    "\n",
    "# construct snpmatrix, covariate files, and true model b\n",
    "x = simulate_random_snparray(\"tmp.bed\", n, p)\n",
    "X = convert(Matrix{Float64}, x, center=true, scale=true)\n",
    "intercept = 1.0\n",
    "    \n",
    "#define true_b \n",
    "true_b = zeros(p)\n",
    "true_b[1:10] .= rand(Normal(0, 0.25), k)\n",
    "shuffle!(true_b)\n",
    "correct_position = findall(!iszero, true_b)\n",
    "\n",
    "#simulate phenotypes (e.g. vector y)\n",
    "prob = GLM.linkinv.(l, intercept .+ X * true_b)\n",
    "clamp!(prob, -20, 20)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y);\n",
    "\n",
    "# construct weight vector\n",
    "w = ones(p + 1)\n",
    "w[correct_position] .= 2.0\n",
    "one_tenth = round(Int, p/10)\n",
    "idx = rand(1:p, one_tenth)\n",
    "w[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×4 DataFrame\n",
      " Row │ position  correct     unweighted  weighted\n",
      "     │ Int64     Float64     Float64     Float64\n",
      "─────┼─────────────────────────────────────────────\n",
      "   1 │     1264   0.252886     0.270233   0.264713\n",
      "   2 │     1506  -0.0939841    0.0       -0.125803\n",
      "   3 │     4866  -0.227394    -0.233703  -0.237007\n",
      "   4 │     5778  -0.510488    -0.507114  -0.494199\n",
      "   5 │     5833  -0.311969    -0.324309  -0.322663\n",
      "   6 │     5956  -0.0548168    0.0        0.0\n",
      "   7 │     6378  -0.0155173    0.0        0.0\n",
      "   8 │     7007  -0.123301     0.0        0.0\n",
      "   9 │     7063   0.0183886    0.0        0.0\n",
      "  10 │     7995  -0.102122     0.0       -0.142201\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run weighted and unweighted IHT\n",
    "unweighted = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false)\n",
    "weighted   = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false, weight=w)\n",
    "\n",
    "#check result\n",
    "compare_model = DataFrame(\n",
    "    position    = correct_position,\n",
    "    correct     = true_b[correct_position],\n",
    "    unweighted  = unweighted.beta[correct_position], \n",
    "    weighted    = weighted.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"\\n\")\n",
    "\n",
    "#clean up. Windows user must do this step manually (outside notebook/REPL)\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: weighted IHT found 2 extra predictor than non-weighted IHT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples and functionalities\n",
    "\n",
    "Additional features are available as optional parameters in the [fit_iht](https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/fit.jl#L37) function, but they should be treated as **experimental** features. Interested users are encouraged to explore them and please file issues on GitHub if you encounter a problem."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

var documenterSearchIndex = {"docs":
[{"location":"man/FAQ/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"man/FAQ/","page":"FAQ","title":"FAQ","text":"If you do not find your problem here, or the provided solution does not solve your problem, please file an issue on GitHub. ","category":"page"},{"location":"man/FAQ/#Precompilation-error","page":"FAQ","title":"Precompilation error","text":"","category":"section"},{"location":"man/FAQ/","page":"FAQ","title":"FAQ","text":"The first time one runs using MendelIHT, please do so without doing using Distributed. Sometimes precompilation can fail in a distributed environment. \nOn cluster environments, sometimes Julia crashes randomly causing core dumps. If this happens, try running Julia on intel nodes. ","category":"page"},{"location":"man/FAQ/#Parallel-computing","page":"FAQ","title":"Parallel computing","text":"","category":"section"},{"location":"man/FAQ/","page":"FAQ","title":"FAQ","text":"How to start Julia with multiple threads.\nExecute Threads.nthreads() to check if multiple thread is enabled","category":"page"},{"location":"man/math/#Details-of-Parameter-Estimation","page":"Mathematical Details","title":"Details of Parameter Estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"This note is meant to supplement our paper. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For a review on generalized linear models, the following resources are recommended:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"(3rd edition) Chapter 15.3 of Applied regression analysis and generalized linear models by John Fox\n(3rd edition) Chapter 3-5 of An introduction to generalized linear models by Dobson and Barnett","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For review on projected gradient descent, I recommend","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Chapter 5 of MM optimization algorithms by Kenneth Lange (2nd edition is almost out, as of 1/2/2021)","category":"page"},{"location":"man/math/#Generalized-linear-models","page":"Mathematical Details","title":"Generalized linear models","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, phenotypes (bf y) are modeled as a generalized linear model:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    mu_i = E(y_i) = g(bf x_i^t boldsymbol beta)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where bf x is sample i's p-dimensional vector of covariates (genotypes + other fixed effects), boldsymbol beta is a p-dimensional regression coefficients, g is a non-linear inverse-link function, y_i is sample i's phenotype value, and mu_i is the average predicted value of y_i given bf x. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The full design matrix bf X_n times p and phenotypes bf y_n times 1 are observed. The distribution of bf y and the inverse link g are chosen before fitting. The regression coefficients boldsymbol beta are not observed and are estimated by maximum likelihood methods, traditionally via iteratively reweighted least squares (IRLS). For high dimensional problems where n  p, we substitute iterative hard thresholding in place of IRLS. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases under the GLM framework. When g(alpha) = alpha we get standard linear regression used for Gaussian phenotypes. ","category":"page"},{"location":"man/math/#Loglikelihood,-gradient,-and-expected-information","page":"Mathematical Details","title":"Loglikelihood, gradient, and expected information","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In GLM, the distribution of bf y is from the exponential family with density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    f(y mid theta phi) = exp left fracy theta - b(theta)a(phi) + c(y phi) right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Here theta is called the canonical (location) parameter and under the canonical link, theta = g(bf x^t bf beta). phi is the dispersion (scale) parameter. The functions a b c are known functions that vary depending on the distribution of y. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Given n independent observations, the loglikelihood is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    L(bf theta phi bf y) = sum_i=1^n fracy_itheta_i - b(theta_i)a_i(phi) + c(y_i phi)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate the loglikelihood, we evaluate sample i's logpdf using the logpdf function in Distributions.jl.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The perform maximum likelihood estimation, we compute partial derivatives for betas. The jth score component is (eq 4.18 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    fracpartial Lpartial beta_j = sum_i=1^n leftfracy_i - mu_ivar(y_i)x_ijleft(fracpartial mu_ipartial eta_iright)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus the full gradient is","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla L= bf X^tbf W(bf y - boldsymbolmu) quad W_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"and similarly, the expected information is (eq 4.23 in Dobson):","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    J = bf X^ttildeWX quad tildeW_ii = frac1var(y_i)left(fracpartial mu_ipartial eta_iright)^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"To evaluate nabla L and J, note bf y and bf X are known, so we just need to calculate boldsymbolmu fracpartialmu_ipartialeta_i and var(y_i). The first simply uses the inverse link: mu_i = g(bf x_i^t boldsymbol beta). For the second, note fracpartial mu_ipartialeta_i = fracpartial g(bf x_i^t boldsymbol beta)partialbf x_i^t boldsymbol beta is just the derivative of the inverse link function evaluated at the linear predictor eta_i = bf x_i^t boldsymbol beta. This is already implemented for various link functions as mueta in GLM.jl, which we call internally. To compute var(y_i), we note that the exponential family distributions have variance","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    var(y) = a(phi)b(theta) = a(phi)fracpartial^2b(theta)partialtheta^2 = a(phi) var(mu)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"That is, var(y_i) is a product of 2 terms where the first depends solely on phi, and the second solely on mu_i = g(bf x_i^t boldsymbol beta). In our code, we use glmvar implemented in GLM.jl to calculate var(mu). Because phi is unknown, we assume a(phi) = 1 for all models in computing W_ii and tildeW_ii, except for the negative binomial model. For negative binomial model, we discuss how to estimate phi and boldsymbolbeta using alternate block descent below.  ","category":"page"},{"location":"man/math/#Iterative-hard-thresholding","page":"Mathematical Details","title":"Iterative hard thresholding","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"In MendelIHT.jl, the loglikelihood is maximized using iterative hard thresholding. This is achieved by repeating the following iteration:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    boldsymbolbeta_n+1 = overbraceP_S_k^(3)big(boldsymbolbeta_n + underbraces_n_(2) overbracenabla f(boldsymbolbeta_n)^(1)big)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where f is the loglikelihood to maximize. Step (1) computes the gradient as previously discussed. Step (2) computes the step size s_k. Step (3) evaluates the projection operator P_S_k, which sets all but k largest entries in magnitude to 0. To perform P_S_k, we first partially sort the dense vector beta_n + s_n nabla f(beta_n), and set the smallest k+1  n entries in magnitude to 0. Note the step size s_n is derived in our paper to be","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    s_n = fracnabla f(boldsymbolbeta_n)_2^2nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where J = bf X^ttildeWX is the expected information matrix (derived in the previous section) which should never be explicitly formed. To evaluate the denominator, observe that ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    nabla f(boldsymbolbeta_n)^t J(boldsymbolbeta_n) nabla f(boldsymbolbeta_n) = left(nabla f(boldsymbolbeta_n)^tbf X^t sqrt(tildeW)right)left(sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n)right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Thus one computes bf v = sqrt(tildeW)bf Xnabla f(boldsymbolbeta_n) and calculate its inner product with itself. ","category":"page"},{"location":"man/math/#Nuisance-parameter-estimation","page":"Mathematical Details","title":"Nuisance parameter estimation","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Currently MendelIHT.jl only estimates nuisance parameter for the Negative Binomial model. Estimation of phi and boldsymbol beta can be achieved with alternating block updates. That is, we run 1 IHT iteration to estimate boldsymbol beta_n, followed by 1 iteration of Newton or MM update to estimate phi_n. Below we derive the Newton and MM updates. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note 1: This feature is provided by our 2019 Bruins in Genomics summer student Vivian Garcia and Francis Adusei. ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note 2: for Gaussian response, one can use the sample variance formula to estimate phi from the estimated mean hatmu. ","category":"page"},{"location":"man/math/#Parametrization-for-Negative-Binomial-model","page":"Mathematical Details","title":"Parametrization for Negative Binomial model","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The negative binomial distribution has density","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tP(Y = y) = binomy+r-1yp^r(1-p)^y\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"where y is the number of failures before the rth success and p is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to WOLFRAM is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tmu_i = fracr(1-p_i)r quad\n\tVar(y_i) = fracr(1-p_i)p_i^2\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Note these formula are different than the default on wikipedia because in wiki y is the number of success and r is the number of failure.  Therefore, solving for p_i, we have ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tp_i = fracrmu_i + r = fracre^mathbfx_i^Tbeta + r in (0 1)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"And indeed this this is how we parametrize the negative binomial model. Importantly, we can interpret p_i as a probability, since mathbfx_i^Tbeta can take on any number between -infty and +infty (since beta and mathbfx_i can have positive and negative entries), so exp(mathbfx_i^Tbeta)in(0 infty).","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"We can also try to express Var(y_i) in terms of mu_i and r by doing some algebra:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tVar(y_i)\n\t= fracr(1-p_i)p_i^2 = fracrleft( 1 - fracrmu_i + r right)fracr^2(mu_i + r)^2 = frac1rleft(1 - fracrmu_i + rright)(mu_i + r)^2 \n\t= frac1r left (mu_i + r)^2 - r(mu_r + r) right = frac1r(mu_i + r)mu_i\n\t= mu_i left( fracmu_ir + 1 right)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"You can verify in GLM.jl that this is indeed how they compute the variance of a negative binomial distribution. ","category":"page"},{"location":"man/math/#Estimating-nuisance-parameter-using-MM-algorithms","page":"Mathematical Details","title":"Estimating nuisance parameter using MM algorithms","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The MM algorithm is very stable, but converges much slower than Newton's alogorithm below. Thus use MM only if Newton's method fails.","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The loglikelihood for n independent samples under a Negative Binomial model is ","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tL(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left sum_j=0^y_i - 1 ln(r+j) + rln(p_i) - ln(y_i) + y_iln(1-p_i) right\n\tgeq sum_i=1^mleft sum_j=0^y_i-1fracr_nr_n+jln(r) + c_n + rln(p_i) - ln(y_i) + y_i ln(1-p_i) right\n    equiv M(p_1  p_m r)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"The last inequality can be seen by applying Jensen's inequality:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfleft sum_iu_i(boldsymboltheta)right leq sum_i fracu_i(boldsymboltheta_n)sum_j u_j(boldsymboltheta_n)f left fracsum_j u_j(boldsymboltheta_n)u_i(boldsymboltheta_n) u_i(boldsymboltheta)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"to the function f(u) = - ln(u) Maximizing M over r (i.e. differentiating with respect to r and setting equal to zero, then solving for r), we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n    fracddr M\n\t= sum_i=1^m left sum_j=0^y_i-1 fracr_nr_n + j frac1r + ln(p_i) right \n\t= sum_i=1^msum_j=0^y_i-1 fracr_nr_n + j frac1r + sum_i=1^mln(p_i) \n\tequiv 0\n\tiff r_n+1 = frac-sum_i=1^msum_j=0^y_i-1 fracr_nr_n + jsum_i=1^mln(p_i)  \nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since L ge M (M minorizes L), maximizing M will maximize L. ","category":"page"},{"location":"man/math/#Estimating-Nuisance-parameter-using-Newton's-method","page":"Mathematical Details","title":"Estimating Nuisance parameter using Newton's method","text":"","category":"section"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Since we are dealing with 1 parameter optimization, Newton's method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (r), we use maximum likelihood estimates. By p_i = r  (mu_i + r) in above, we have","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\t L(p_1  p_m r)\n\t= sum_i=1^m ln binomy_i+r-1y_i + rln(p_i) + y_iln(1-p_i)\n\t= sum_i=1^m left lnleft((y_i+r-1)right) - lnleft(y_iright) - lnleft((r-1)right) + rln(r) - rln(mu_i+r) + y_iln(mu_i) + y_iln(mu_i + r)right\n\t= sum_i=1^mleftlnleft((y_i+r-1)right)-ln(y_i) - lnleft((r-1)right) + rln(r) - (r+y_i)ln(mu_i + r) + y_iln(mu_i)right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"Recalling the definition of digamma and trigamma functions, the first and second derivative of our last expression with respect to r is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tfracddr L(p_1  p_m r) =  sum_i=1^m left operatornamedigamma(y_i+r) - operatornamedigamma(r) + 1 + ln(r) - fracr+y_imu_i+r - ln(mu_i + r) right\n\tfracd^2dr^2 L(p_1  p_m r) =sum_i=1^m left operatornametrigamma(y_i+r) - operatornametrigamma(r) + frac1r - frac2mu_i + r + fracr+y_i(mu_i + r)^2 right\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"So the iteration to use is:","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"beginaligned\n\tr_n+1 = r_n - fracfracddrL(p_1p_mr)fracd^2dr^2L(p_1p_mr)\nendaligned","category":"page"},{"location":"man/math/","page":"Mathematical Details","title":"Mathematical Details","text":"For stability, we set the denominator equal to 1 if it is less than 0. That is, we use gradient descent if the current iteration has non-positive definite Hessian matrices. ","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Please star our github page, that would be very helpful.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you would like to contribute to this project, we compiled a list of desired features for this project. Developers of any level is welcomed. Do not be shy because it can't hurt to ask. ","category":"page"},{"location":"man/contributing/#Bug-Fixes-and-User-Support","page":"Contributing","title":"Bug Fixes & User Support","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you encounter a bug or you need some user support, please open a new issue here. If you can, provide the error message and, ideally, a reproducible code that generated the error.","category":"page"},{"location":"man/contributing/#Citation","page":"Contributing","title":"Citation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you use MendelIHT.jl in an academic manuscript, please cite:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"@article{mendeliht,\n  title={{Iterative hard thresholding in genome-wide association studies: Generalized linear models, prior weights, and double sparsity}},\n  author={Chu, Benjamin B and Keys, Kevin L and German, Christopher A and Zhou, Hua and Zhou, Jin J and Sobel, Eric M and Sinsheimer, Janet S and Lange, Kenneth},\n  journal={GigaScience},\n  volume={9},\n  number={6},\n  pages={giaa044},\n  year={2020},\n  publisher={Oxford University Press}\n}","category":"page"},{"location":"man/getting_started/#Getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"In this section, we outline the basic procedure to analyze your GWAS data with MendelIHT. ","category":"page"},{"location":"man/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Download and install Julia. Within Julia, copy and paste the following:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\npkg\"add https://github.com/OpenMendel/SnpArrays.jl\"\npkg\"add https://github.com/OpenMendel/MendelIHT.jl\"","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"MendelIHT.jl supports Julia 1.5+ for Mac, Linux, and window machines. A few features are disabled for windows users, and users will be warned when trying to use them.","category":"page"},{"location":"man/getting_started/#Typical-Workflow","page":"Getting Started","title":"Typical Workflow","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Run cross_validate or cv_iht to determine best sparsity level (k).\nRun iht or fit_iht on optimal k determined from cross validation. ","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"We believe the best way to learn is through examples. Head over to the example section on the left to see these steps in action. ","category":"page"},{"location":"man/getting_started/#Parallel-computing","page":"Getting Started","title":"Parallel computing","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Assuming you have 4 cores, one can load 4 processors by","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nIf you prefer to use the environment variable you can set it as follows in Bash (Linux/macOS):export JULIA_NUM_THREADS=4C shell on Linux/macOS, CMD on Windows:set JULIA_NUM_THREADS=4Powershell on Windows:$env:JULIA_NUM_THREADS=4Note that this must be done before starting Julia.","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Also, the command BLAS.set_num_threads(1) is generally recommended to set the number of BLAS threads to 1, avoiding oversubscription","category":"page"},{"location":"man/getting_started/#Running-from-command-line-as-script","page":"Getting Started","title":"Running from command line as script","text":"","category":"section"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"If you don't want to run MendelIHT.jl in a Julia session (e.g. you want to run batch jobs on a cluster), you can do so by putting the code below in a Julia file. For example, in order to run with 8 cores, create a file called iht.jl which contains:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"# place these code in a file called iht.jl\nusing MendelIHT\n\n# setup code goes here\nplinkfile = ARGS[1]     # 1st command line argument (plink file location)\ncovariates = ARGS[2]    # 2nd command line argument (covariate file location)\npath = 5:5:100          # test k = 5, 10, 15, ... 100\n\n# run MendelIHT: first cross validate for best k, then run IHT using best k\nmses = cross_validate(plinkfile, Normal, covariates=covariates, path=path)\niht_result = iht(plinkfile, Normal, k=path[argmin(mses)])","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"Then in the terminal you can do:","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"julia iht.jl plinkfile covariates.txt","category":"page"},{"location":"man/getting_started/","page":"Getting Started","title":"Getting Started","text":"You should get progress printed to your terminal and have cviht.summary.txt, iht.summary.txt, and iht.beta.txt files saved to your local directory","category":"page"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we give numerous example analysis of GWAS data with MendelIHT.jl. For exact function input/output descriptions, see the manuel's API.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# machine information for reproducibility\nversioninfo()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Julia Version 1.6.0\nCommit f9720dc2eb (2021-03-24 12:55 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin19.6.0)\n  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-11.0.1 (ORCJIT, skylake)\nEnvironment:\n  JULIA_NUM_THREADS = 8","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# load necessary packages for running all examples below\nusing MendelIHT\nusing SnpArrays\nusing DataFrames\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing GLM\nusing DelimitedFiles\nusing Statistics\nusing BenchmarkTools\n\nBLAS.set_num_threads(1) # prevent over subscription with multithreading & BLAS\nRandom.seed!(1111) # set seed for reproducibility","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"MersenneTwister(1111)","category":"page"},{"location":"man/examples/#Using-MendelIHT.jl","page":"Examples","title":"Using MendelIHT.jl","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Users are exposed to 2 levels of interface:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Wrapper functions iht() and cross_validate(). These functions are simple scripts that import data, runs IHT, and writes result to output automatically. Since they are very simplistic, they might fail for whatever reason (please file an issue on GitHub). If so, please use:\nCore functions fit_iht() and cv_iht(). Input arguments for these functions must be first imported into Julia by the user manually.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Below we use numerous examples to illustrate how to use these functions separately. ","category":"page"},{"location":"man/examples/#Parallel-computing","page":"Examples","title":"Parallel computing","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To exploit MendelIHT.jl's parallel processing, start Julia with multiple threads. Two levels of shared-memory parallelism is supported.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"(genotype-matrix)-(vector or matrix) multiplication\ncross validation","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Note: If one is running IHT on Matrix{Float64}, BLAS should NOT run with multiple threads (execute BLAS.set_num_threads(1) before running IHT). This prevents oversubscription. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Threads.nthreads() # show number of threads","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"8","category":"page"},{"location":"man/examples/#Example-1:-GWAS-with-PLINK-files","page":"Examples","title":"Example 1: GWAS with PLINK files","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, our data are stored in binary PLINK files:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"normal.bed\nnormal.bim\nnormal.fam","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"which contains simulated (Gaussian) phenotypes for n=1000 samples and p=10000 SNPs. There are 8 causal variants and 2 causal non-genetic covariates (intercept and sex). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"These data are present under MendelIHT/data directory.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# change directory to where example data is located\ncd(normpath(MendelIHT.datadir()))\n\n# show working directory\n@show pwd() \n\n# show files in current directory\nreaddir()","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"pwd() = \"/Users/biona001/.julia/dev/MendelIHT/data\"\n\n\n\n\n\n17-element Vector{String}:\n \".DS_Store\"\n \"README.md\"\n \"covariates.txt\"\n \"cviht.summary.txt\"\n \"iht.beta.txt\"\n \"iht.cov.txt\"\n \"iht.summary.txt\"\n \"multivariate.bed\"\n \"multivariate.bim\"\n \"multivariate.fam\"\n \"multivariate.phen\"\n \"normal.bed\"\n \"normal.bim\"\n \"normal.fam\"\n \"normal_true_beta.txt\"\n \"phenotypes.txt\"\n \"simulate.jl\"","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here covariates.txt contains non-genetic covariates (intercept + sex), normal.bed/bim/fam are the PLINK files storing genetic covariates, phenotypes.txt are phenotypes for each sample, normal_true_beta.txt is the true statistical model used to generate the phenotypes, and simulate.jl is the script used to generate all the files. ","category":"page"},{"location":"man/examples/#Step-1:-Run-cross-validation-to-determine-best-model-size","page":"Examples","title":"Step 1: Run cross validation to determine best model size","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here phenotypes are stored in the 6th column of .fam file. Other covariates are stored separately (which includes a column of 1 as intercept). Here we cross validate k = 1220. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Note the first run might take awhile because Julia needs to compile the code. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=6, path=1:20,);\n\n# Alternative syntax\n# mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=6, path=[1, 5, 10, 15, 20]) # test k = 1, 5, 10, 15, 20\n# mses = cross_validate(\"normal\", Normal, covariates=\"covariates.txt\", phenotypes=\"phenotypes.txt\", path=1:20) # when phenotypes are stored separately","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\n\n\n\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:17\u001b[39m\n\n\n\n\nCrossvalidation Results:\n\tk\tMSE\n\t1\t1383.3887888333788\n\t2\t714.3108200438915\n\t3\t630.5562933019421\n\t4\t599.6952113619866\n\t5\t438.42328589185536\n\t6\t327.08547676403236\n\t7\t304.1177395732532\n\t8\t297.41346319671766\n\t9\t227.5681066123959\n\t10\t202.69232180491358\n\t11\t206.35218981854467\n\t12\t209.76930769385618\n\t13\t213.48278746710366\n\t14\t215.43361702847125\n\t15\t215.6684475575618\n\t16\t223.24291123616155\n\t17\t221.4060678342564\n\t18\t276.6464367814522\n\t19\t230.976587123502\n\t20\t268.32185480128885\n\nBest k = 10","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Do not be alarmed if you get slightly different numbers, because cross validation breaks data into training/testing randomly. Set a seed by Random.seed!(1234) if you want reproducibility.","category":"page"},{"location":"man/examples/#Step-2:-Run-IHT-on-best-k","page":"Examples","title":"Step 2: Run IHT on best k","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"According to cross validation, k = 10 achieves the minimum MSE. Thus we run IHT on the full dataset.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"normal\", 10, Normal, covariates=\"covariates.txt\", phenotypes=6);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse linear regression\nNumber of threads = 8\nLink functin = IdentityLink()\nSparsity parameter (k) = 10\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001 and iteration ≥ 5:\n\nIteration 1: loglikelihood = -1457.8431531776794, backtracks = 0, tol = 0.7683976715709564\nIteration 2: loglikelihood = -1391.9907038720667, backtracks = 0, tol = 0.11226800885813881\nIteration 3: loglikelihood = -1390.7113887489165, backtracks = 0, tol = 0.011581483394882052\nIteration 4: loglikelihood = -1390.6980736000305, backtracks = 0, tol = 0.0010566984279122389\nIteration 5: loglikelihood = -1390.6978716629346, backtracks = 0, tol = 0.00010752965216198347\nIteration 6: loglikelihood = -1390.4000085563305, backtracks = 0, tol = 0.03716090961691858\nIteration 7: loglikelihood = -1390.3012534352063, backtracks = 0, tol = 0.0032563210092611864\nIteration 8: loglikelihood = -1390.300372706346, backtracks = 0, tol = 0.0003148875004358726\nIteration 9: loglikelihood = -1390.3003586111483, backtracks = 0, tol = 3.673233513687601e-5","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The convergence criteria can be tuned by keywords tol and min_iter. ","category":"page"},{"location":"man/examples/#Step-3:-Examine-results","page":"Examples","title":"Step 3: Examine results","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT picked 8 SNPs and 2 non-genetic predictors: intercept and sex. The Position argument corresponds to the order in which the SNP appeared in the PLINK file, and the Estimated_β argument is the estimated effect size for the selected SNPs. To extract more information (for instance to extract rs IDs), we can do","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"snpdata = SnpData(\"normal\")                   # import PLINK information\nsnps_idx = findall(!iszero, result.beta)      # indices of SNPs selected by IHT\nselected_snps = snpdata.snp_info[snps_idx, :] # see which SNPs are selected\n@show selected_snps;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"selected_snps = 8×6 DataFrame\n Row │ chromosome  snpid    genetic_distance  position  allele1  allele2\n     │ String      String   Float64           Int64     String   String\n─────┼───────────────────────────────────────────────────────────────────\n   1 │ 1           snp3136               0.0         1  1        2\n   2 │ 1           snp3137               0.0         1  1        2\n   3 │ 1           snp4246               0.0         1  1        2\n   4 │ 1           snp4717               0.0         1  1        2\n   5 │ 1           snp6290               0.0         1  1        2\n   6 │ 1           snp7755               0.0         1  1        2\n   7 │ 1           snp8375               0.0         1  1        2\n   8 │ 1           snp9415               0.0         1  1        2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The table above displays the SNP information for the selected SNPs. Because there's only 7 causal SNPs, we have 1 false positive. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since data is simulated, the fields chromosome, snpid, genetic_distance, position, allele1, and allele2 are fake. ","category":"page"},{"location":"man/examples/#Example-2:-How-to-simulate-data","page":"Examples","title":"Example 2: How to simulate data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here we demonstrate how to use MendelIHT.jl and SnpArrays.jl to simulate data, allowing you to design your own genetic studies. Note:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"For more complex simulation, please use the module TraitSimulations.jl.  \nAll linear algebra routines involving PLINK files are handled by SnpArrays.jl. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate an example PLINK trio (.bim, .bed, .fam) and non-genetic covariates, then we illustrate how to import them. For simplicity, let us simulated indepent SNPs with binary phenotypes. Explicitly, our model is:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Bernoulli(mathbfx_i^Tboldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm intercept = 1","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_rm sex = 15","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000            # number of samples\np = 10000           # number of SNPs\nk = 10              # 8 causal SNPs and 2 causal covariates (intercept + sex)\nd = Bernoulli       # Binary (continuous) phenotypes\nl = LogitLink()     # canonical link function\n\n# set random seed\nRandom.seed!(0)\n\n# simulate `sim.bed` file with no missing data\nx = simulate_random_snparray(\"sim.bed\", n, p)\nxla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true, impute=true) \n\n# nongenetic covariate: first column is the intercept, second column is sex: 0 = male 1 = female\nz = ones(n, 2) \nz[:, 2] .= rand(0:1, n)\nstandardize!(@view(z[:, 2:end])) \n\n# randomly set genetic predictors where causal βᵢ ~ N(0, 1)\ntrue_b = zeros(p) \ntrue_b[1:k-2] = randn(k-2)\nshuffle!(true_b)\n\n# find correct position of genetic predictors\ncorrect_position = findall(!iszero, true_b)\n\n# define effect size of non-genetic predictors: intercept & sex\ntrue_c = [1.0; 1.5] \n\n# simulate phenotype using genetic and nongenetic predictors\nprob = GLM.linkinv.(l, xla * true_b .+ z * true_c) # note genotype-vector multiplication is done with `xla`\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # turn y into floating point numbers\n\n# create `sim.bim` and `sim.bam` files using phenotype\nmake_bim_fam_files(x, y, \"sim\")\n\n#save covariates and phenotypes (without header)\nwritedlm(\"sim.covariates.txt\", z, ',')\nwritedlm(\"sim.phenotypes.txt\", y)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"note: Note\nPlease standardize (or at least center) your non-genetic covariates. If you use our iht() or cross_validation() functions, standardization is automatic. For genotype matrix, SnpLinAlg efficiently achieves this standardization. For non-genetic covariates, please use the built-in function standardize!. ","category":"page"},{"location":"man/examples/#Example-3:-Logistic/Poisson/Negative-binomial-GWAS","page":"Examples","title":"Example 3: Logistic/Poisson/Negative-binomial GWAS","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In Example 2, we simulated binary phenotypes, genotypes, non-genetic covariates, and we know true k = 10. Let's try running a logistic regression (i.e. phenotype follows the Bernoulli distribution) on this data. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = iht(\"sim\", 10, Bernoulli, covariates=\"sim.covariates.txt\")","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse logistic regression\nNumber of threads = 8\nLink functin = LogitLink()\nSparsity parameter (k) = 10\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001 and iteration ≥ 5:\n\nIteration 1: loglikelihood = -388.8910083252736, backtracks = 0, tol = 0.6418034752978493\nIteration 2: loglikelihood = -334.5278475827583, backtracks = 0, tol = 0.3170396015883161\nIteration 3: loglikelihood = -327.9633012046245, backtracks = 0, tol = 0.2025866533020331\nIteration 4: loglikelihood = -321.78128260006685, backtracks = 0, tol = 0.1860359951551827\nIteration 5: loglikelihood = -320.82291695265127, backtracks = 1, tol = 0.019411649026600777\nIteration 6: loglikelihood = -320.4507210246635, backtracks = 2, tol = 0.009984009889427738\nIteration 7: loglikelihood = -317.5178129459779, backtracks = 2, tol = 0.10991749703912314\nIteration 8: loglikelihood = -317.02211222442554, backtracks = 1, tol = 0.013593158611626565\nIteration 9: loglikelihood = -316.84609828412295, backtracks = 2, tol = 0.007169021739514612\nIteration 10: loglikelihood = -316.70442301799153, backtracks = 2, tol = 0.007932900581304757\nIteration 11: loglikelihood = -316.6123017144071, backtracks = 2, tol = 0.006753634000554562\nIteration 12: loglikelihood = -316.55859748055383, backtracks = 2, tol = 0.005166907901710646\nIteration 13: loglikelihood = -316.5276537804478, backtracks = 2, tol = 0.00394060866454871\nIteration 14: loglikelihood = -316.50992605296796, backtracks = 2, tol = 0.0029918249852994103\nIteration 15: loglikelihood = -316.4998143291673, backtracks = 2, tol = 0.002265270160216051\nIteration 16: loglikelihood = -316.49406613214404, backtracks = 2, tol = 0.0017110615239077592\nIteration 17: loglikelihood = -316.49080687297084, backtracks = 2, tol = 0.0012902536156348425\nIteration 18: loglikelihood = -316.4889624723853, backtracks = 2, tol = 0.0009716277317779244\nIteration 19: loglikelihood = -316.4879202825298, backtracks = 2, tol = 0.0007309600514247552\nIteration 20: loglikelihood = -316.48733204721407, backtracks = 2, tol = 0.000549484718269684\nIteration 21: loglikelihood = -316.4870003149547, backtracks = 2, tol = 0.0004128291474231582\nIteration 22: loglikelihood = -316.4868133555257, backtracks = 2, tol = 0.00031002536936769156\nIteration 23: loglikelihood = -316.4867080384853, backtracks = 2, tol = 0.00023274674474752565\nIteration 24: loglikelihood = -316.4866487332225, backtracks = 2, tol = 0.00017468833019519464\nIteration 25: loglikelihood = -316.486615346784, backtracks = 2, tol = 0.0001310885323519644\nIteration 26: loglikelihood = -316.48659655541053, backtracks = 2, tol = 9.835709203914658e-5\n\n\n\n\n\n\nIHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n\nCompute time (sec):     0.5662021636962891\nFinal loglikelihood:    -316.48659655541053\nSNP PVE:                0.5592583156030205\nIterations:             26\n\nSelected genetic predictors:\n\u001b[1m8×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │      714    -0.7238\n   2 │      777    -0.625216\n   3 │     1356     1.01887\n   4 │     2426     0.392727\n   5 │     5080     0.395473\n   6 │     5490    -0.720477\n   7 │     6299    -2.09706\n   8 │     7057     0.661431\n\nSelected nongenetic predictors:\n\u001b[1m2×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1     0.919708\n   2 │        2     1.48015","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since data is simulated, we can compare IHT's estimated effect size with the truth. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"8×2 Matrix{Float64}:\n -0.787272  -0.7238\n -0.456783  -0.625216\n  1.12735    1.01887\n  0.185925   0.0\n -0.891023  -0.720477\n -2.15515   -2.09706\n  0.166931   0.0\n  0.82265    0.661431","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The 1st column are the true beta values, and the 2nd column is the estimated values. IHT found 6/8 genetic predictors, and estimates are reasonably close to truth. IHT missed SNPs with small effect size. With increased sample size, these small effects can be detected. The estimated non-genetic effect size is also very close to the truth (1.0 and 1.5). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# remove simulated data once they are no longer needed\nrm(\"sim.bed\", force=true)\nrm(\"sim.bim\", force=true)\nrm(\"sim.fam\", force=true)\nrm(\"sim.covariates.txt\", force=true)\nrm(\"sim.phenotypes.txt\", force=true)\nrm(\"iht.beta.txt\", force=true)\nrm(\"iht.summary.txt\", force=true)\nrm(\"cviht.summary.txt\", force=true)","category":"page"},{"location":"man/examples/#Example-4:-Running-IHT-on-general-matrices","page":"Examples","title":"Example 4: Running IHT on general matrices","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To run IHT on genotypes in VCF files, or other general data, one must call fit_iht and cv_iht directly. These functions are designed to work on AbstractArray{T, 2} type where T is a Float64 or Float32. Thus, one must first import the data, and then call fit_iht and cv_iht on it. Note the vector of 1s (intercept) shouldn't be included in the design matrix itself, as it will be automatically included.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"tip: Tip\nCheck out VCFTools.jl to learn how to import VCF data.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"First we simulate some count response using the model:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim rm Poisson(mathbfx_i^T boldsymbolbeta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Normal(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 03)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000             # number of samples\np = 10000            # number of SNPs\nk = 10               # 9 causal predictors + intercept\nd = Poisson          # Response distribution (count data)\nl = LogLink()        # canonical link\n\n# set random seed for reproducibility\nRandom.seed!(2020)\n\n# simulate design matrix\nx = randn(n, p)\n\n# simulate response, true model b, and the correct non-0 positions of b\ntrue_b = zeros(p)\ntrue_b[1:k] .= rand(Normal(0, 0.5), k)\nshuffle!(true_b)\nintercept = 1.0\ncorrect_position = findall(!iszero, true_b)\nprob = GLM.linkinv.(l, intercept .+ x * true_b)\nclamp!(prob, -20, 20) # prevents overflow\ny = [rand(d(i)) for i in prob]\ny = Float64.(y); # convert phenotypes to double precision","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Now we have the response y, design matrix x. Let's run IHT and compare with truth.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# first run cross validation \nmses = cv_iht(y, x, path=1:20, d=Poisson(), l=LogLink());","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\n\n\n\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:25\u001b[39m\n\n\n\n\nCrossvalidation Results:\n\tk\tMSE\n\t1\t1294.4117918373186\n\t2\t669.4092214451908\n\t3\t543.3989940052342\n\t4\t473.50239409691363\n\t5\t449.5280013487634\n\t6\t469.726620661539\n\t7\t476.40540372697865\n\t8\t541.741342787916\n\t9\t539.3863798082896\n\t10\t523.5971500146672\n\t11\t517.4445374386203\n\t12\t558.3509823150954\n\t13\t600.125510865708\n\t14\t597.7008166652079\n\t15\t569.2744078914006\n\t16\t603.7206875133627\n\t17\t639.488936399919\n\t18\t643.1761683767324\n\t19\t646.566741795878\n\t20\t642.7739157309286\n\nBest k = 5","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Now run IHT on the full dataset using the best k (achieved at k = 5)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result = fit_iht(y, x, k=argmin(mses), d=Poisson(), l=LogLink())","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse Poisson regression\nNumber of threads = 8\nLink functin = LogLink()\nSparsity parameter (k) = 5\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001 and iteration ≥ 5:\n\nIteration 1: loglikelihood = -2847.082501924986, backtracks = 0, tol = 0.2928574304111579\nIteration 2: loglikelihood = -2465.401434829008, backtracks = 0, tol = 0.052309994098756654\nIteration 3: loglikelihood = -2376.6519599956146, backtracks = 0, tol = 0.07104164424891495\nIteration 4: loglikelihood = -2351.583313350411, backtracks = 0, tol = 0.02620817684956463\nIteration 5: loglikelihood = -2343.1107828225104, backtracks = 0, tol = 0.020230016134234835\nIteration 6: loglikelihood = -2339.0692529047383, backtracks = 0, tol = 0.01108030835180364\nIteration 7: loglikelihood = -2337.149479249759, backtracks = 0, tol = 0.009309142365782066\nIteration 8: loglikelihood = -2336.178140295875, backtracks = 0, tol = 0.005564169436184072\nIteration 9: loglikelihood = -2335.6908426969235, backtracks = 0, tol = 0.004609772037770407\nIteration 10: loglikelihood = -2335.440388139586, backtracks = 0, tol = 0.0028617995124748164\nIteration 11: loglikelihood = -2335.3124548737906, backtracks = 0, tol = 0.002340881298705853\nIteration 12: loglikelihood = -2335.2463824561282, backtracks = 0, tol = 0.0014798329877975507\nIteration 13: loglikelihood = -2335.2123851939327, backtracks = 0, tol = 0.0012011066579049358\nIteration 14: loglikelihood = -2335.1947979604856, backtracks = 0, tol = 0.0007661746047595179\nIteration 15: loglikelihood = -2335.1857186752313, backtracks = 0, tol = 0.0006191995770663082\nIteration 16: loglikelihood = -2335.181018905229, backtracks = 0, tol = 0.00039678836835178535\nIteration 17: loglikelihood = -2335.1785888400173, backtracks = 0, tol = 0.0003199381492395469\nIteration 18: loglikelihood = -2335.1773306203627, backtracks = 0, tol = 0.00020549845888248242\nIteration 19: loglikelihood = -2335.1766795324024, backtracks = 0, tol = 0.00016549800033336165\nIteration 20: loglikelihood = -2335.176342377269, backtracks = 0, tol = 0.0001064283022001086\nIteration 21: loglikelihood = -2335.176167840737, backtracks = 0, tol = 8.565816720858893e-5\n\n\n\n\n\n\nIHT estimated 4 nonzero SNP predictors and 1 non-genetic predictors.\n\nCompute time (sec):     0.2328169345855713\nFinal loglikelihood:    -2335.176167840737\nSNP PVE:                0.09113449276174614\nIterations:             21\n\nSelected genetic predictors:\n\u001b[1m4×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │       83    -0.809284\n   2 │      989     0.378376\n   3 │     4294    -0.274544\n   4 │     4459     0.169417\n\nSelected nongenetic predictors:\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1      1.26918","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# compare IHT result with truth\n[true_b[correct_position] result.beta[correct_position]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"10×2 Matrix{Float64}:\n -1.303      -0.809284\n  0.585809    0.378376\n -0.0700563   0.0\n -0.0901341   0.0\n -0.0620201   0.0\n -0.441452   -0.274544\n  0.271429    0.169417\n -0.164888    0.0\n -0.0790484   0.0\n  0.0829054   0.0","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Since many of the true beta are small, we were only able to find 5 true signals (4 predictors + intercept). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: In this example, we ran IHT on count response with a general Matrix{Float64} design matrix. Since we used simulated data, we could compare IHT's estimates with the truth. ","category":"page"},{"location":"man/examples/#Example-5:-Group-IHT","page":"Examples","title":"Example 5: Group IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include group information to perform doubly sparse projections. Here the final model would contain at most J = 5 groups where each group contains limited number of (prespecified) SNPs. For simplicity, we assume the sparsity parameter k is known. ","category":"page"},{"location":"man/examples/#Data-simulation","page":"Examples","title":"Data simulation","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"To illustrate the effect of group information and prior weights, we generated correlated genotype matrix according to the procedure outlined in our paper. In this example, each SNP belongs to 1 of 500 disjoint groups containing 20 SNPs each; j = 5 distinct groups are each assigned 125 causal SNPs with effect sizes randomly chosen from 0202. In all there 15 causal SNPs.  For grouped-IHT, we assume perfect group information. That is, the selected groups containing 1∼5 causative SNPs are assigned maximum within-group sparsity lambda_g = 125. The remaining groups are assigned lambda_g = 1 (i.e. only 1 active predictor are allowed).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# define problem size\nd = NegativeBinomial\nl = LogLink()\nn = 1000\np = 10000\nblock_size = 20                  #simulation parameter\nnum_blocks = Int(p / block_size) #simulation parameter\n\n# set seed\nRandom.seed!(2019)\n\n# assign group membership\nmembership = collect(1:num_blocks)\ng = zeros(Int64, p + 1)\nfor i in 1:length(membership)\n    for j in 1:block_size\n        cur_row = block_size * (i - 1) + j\n        g[block_size*(i - 1) + j] = membership[i]\n    end\nend\ng[end] = membership[end]\n\n#simulate correlated snparray\nx = simulate_correlated_snparray(\"tmp.bed\", n, p)\nintercept = 0.5\nx_float = convert(Matrix{Float64}, x, model=ADDITIVE_MODEL, center=true, scale=true)\n\n#simulate true model, where 5 groups each with 1~5 snps contribute\ntrue_b = zeros(p)\ntrue_groups = randperm(num_blocks)[1:5]\nsort!(true_groups)\nwithin_group = [randperm(block_size)[1:1], randperm(block_size)[1:2], \n                randperm(block_size)[1:3], randperm(block_size)[1:4], \n                randperm(block_size)[1:5]]\ncorrect_position = zeros(Int64, 15)\nfor i in 1:5\n    cur_group = block_size * (true_groups[i] - 1)\n    cur_group_snps = cur_group .+ within_group[i]\n    start, last = Int(i*(i-1)/2 + 1), Int(i*(i+1)/2)\n    correct_position[start:last] .= cur_group_snps\nend\nfor i in 1:15\n    true_b[correct_position[i]] = rand(-1:2:1) * 0.2\nend\nsort!(correct_position)\n\n# simulate phenotype\nr = 10 #nuisance parameter\nμ = GLM.linkinv.(l, intercept .+ x_float * true_b)\nclamp!(μ, -20, 20)\nprob = 1 ./ (1 .+ μ ./ r)\ny = [rand(d(r, i)) for i in prob] #number of failures before r success occurs\ny = Float64.(y);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run IHT without groups\nungrouped = fit_iht(y, x_float, k=15, d=NegativeBinomial(), l=LogLink(), verbose=false)\n\n#run doubly sparse (group) IHT by specifying maximum number of SNPs for each group (in order)\nmax_group_snps = ones(Int, num_blocks)\nmax_group_snps[true_groups] .= collect(1:5)\nvariable_group = fit_iht(y, x_float, d=NegativeBinomial(), l=LogLink(), k=max_group_snps, J=5, group=g, verbose=false);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#check result\ncorrect_position = findall(!iszero, true_b)\ncompare_model = DataFrame(\n    position = correct_position,\n    correct_β = true_b[correct_position],\n    ungrouped_IHT_β = ungrouped.beta[correct_position], \n    grouped_IHT_β = variable_group.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 15×4 DataFrame\n Row │ position  correct_β  ungrouped_IHT_β  grouped_IHT_β\n     │ Int64     Float64    Float64          Float64\n─────┼─────────────────────────────────────────────────────\n   1 │      963       -0.2        -0.21403        0.0\n   2 │     3485       -0.2         0.0           -0.13032\n   3 │     3487       -0.2        -0.323509      -0.225267\n   4 │     7405        0.2         0.254196       0.260726\n   5 │     7407       -0.2        -0.186084      -0.202747\n   6 │     7417       -0.2        -0.190491      -0.201521\n   7 │     9104       -0.2        -0.189195      -0.201113\n   8 │     9110        0.2         0.192222       0.177787\n   9 │     9118       -0.2        -0.196494      -0.189983\n  10 │     9120        0.2         0.253254       0.248832\n  11 │     9206       -0.2        -0.236861      -0.217945\n  12 │     9209       -0.2        -0.198633      -0.177085\n  13 │     9210       -0.2        -0.172682      -0.186602\n  14 │     9211       -0.2        -0.234481      -0.23977\n  15 │     9217        0.2         0.227397       0.217969","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: Ungroup and grouped IHT each found 1 SNP that the other didn't find.  ","category":"page"},{"location":"man/examples/#Example-6:-Linear-Regression-with-prior-weights","page":"Examples","title":"Example 6: Linear Regression with prior weights","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"In this example, we show how to include (predetermined) prior weights for each SNP. You can check out our paper for references of why/how to choose these weights. In this case, we mimic our paper and randomly set 10 of all SNPs to have a weight of 20. Other predictors have weight of 10. All causal SNPs have weights of 20. Under this scenario, SNPs with weight 20 is twice as likely to enter the model identified by IHT. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Our model is simulated as:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"y_i sim mathbfx_i^Tmathbfbeta + epsilon_i","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"x_ij sim rm Binomial(2 rho_j)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"rho_j sim rm Uniform(0 05)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"epsilon_i sim rm N(0 1)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"beta_i sim rm N(0 025)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"d = Normal\nl = IdentityLink()\nn = 1000\np = 10000\nk = 10\n\n#random seed\nRandom.seed!(4)\n\n# construct snpmatrix, covariate files, and true model b\nx = simulate_random_snparray(\"tmp.bed\", n, p)\nX = convert(Matrix{Float64}, x, center=true, scale=true)\nintercept = 1.0\n    \n#define true_b \ntrue_b = zeros(p)\ntrue_b[1:10] .= rand(Normal(0, 0.25), k)\nshuffle!(true_b)\ncorrect_position = findall(!iszero, true_b)\n\n#simulate phenotypes (e.g. vector y)\nprob = GLM.linkinv.(l, intercept .+ X * true_b)\nclamp!(prob, -20, 20)\ny = [rand(d(i)) for i in prob]\ny = Float64.(y);\n\n# construct weight vector\nw = ones(p + 1)\nw[correct_position] .= 2.0\none_tenth = round(Int, p/10)\nidx = rand(1:p, one_tenth)\nw[idx] .= 2.0; #randomly set ~1/10 of all predictors to 2","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"#run weighted and unweighted IHT\nunweighted = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false)\nweighted   = fit_iht(y, X, k=10, d=Normal(), l=IdentityLink(), verbose=false, weight=w)\n\n#check result\ncompare_model = DataFrame(\n    position    = correct_position,\n    correct     = true_b[correct_position],\n    unweighted  = unweighted.beta[correct_position], \n    weighted    = weighted.beta[correct_position])\n@show compare_model\nprintln(\"\\n\")\n\n#clean up. Windows user must do this step manually (outside notebook/REPL)\nrm(\"tmp.bed\", force=true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"compare_model = 10×4 DataFrame\n Row │ position  correct     unweighted  weighted\n     │ Int64     Float64     Float64     Float64\n─────┼─────────────────────────────────────────────\n   1 │     1264   0.252886     0.270233   0.280652\n   2 │     1506  -0.0939841    0.0       -0.118611\n   3 │     4866  -0.227394    -0.233703  -0.232989\n   4 │     5778  -0.510488    -0.507114  -0.501733\n   5 │     5833  -0.311969    -0.324309  -0.319763\n   6 │     5956  -0.0548168    0.0        0.0\n   7 │     6378  -0.0155173    0.0        0.0\n   8 │     7007  -0.123301     0.0        0.0\n   9 │     7063   0.0183886    0.0        0.0\n  10 │     7995  -0.102122     0.0       -0.134898","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: weighted IHT found 2 extra predictor than non-weighted IHT.","category":"page"},{"location":"man/examples/#Example-7:-Multivariate-IHT","page":"Examples","title":"Example 7: Multivariate IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"When there is multiple quantitative traits, analyzing them jointly is known to be superior than conducting multiple univariate-GWAS (ref1, ref2). When MendelIHT.jl performs a multivariate analysis, ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT estimates effect of every SNP (covariate) conditioned on every other SNP across traits\nIHT outputs an estimated covariate matrix among traits\nIHT estimates proportion of trait variance explained by the genetic predictors","category":"page"},{"location":"man/examples/#First-simulate-data","page":"Examples","title":"First simulate data","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"With r traits, each sample's phenotype mathbfy_i in mathbbR^n times 1 is simulated under","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"mathbfy_i^r times 1 sim N(mathbfB^r times pmathbfx_i^p times 1   Sigma_r times r)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"This model assumes each sample is independent. The covariance among traits is specified by Sigma.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"n = 1000  # number of samples\np = 10000 # number of SNPs\nk = 10    # number of causal SNPs\nr = 2     # number of traits\n\n# set random seed for reproducibility\nRandom.seed!(2021)\n\n# simulate `.bed` file with no missing data\nx = simulate_random_snparray(\"multivariate.bed\", n, p)\nxla = SnpLinAlg{Float64}(x, model=ADDITIVE_MODEL, impute=false, center=true, scale=true) \n\n# intercept is the only nongenetic covariate\nz = ones(n, 1)\nintercepts = randn(r)' # each trait have different intercept\n\n# simulate response y, true model b, and the correct non-0 positions of b\nY, true_Σ, true_b, correct_position = simulate_random_response(xla, k, r, Zu=z*intercepts, overlap=0)\nwritedlm(\"multivariate.trait.cov\", true_Σ, ',')\n\n# create `.bim` and `.bam` files using phenotype\nmake_bim_fam_files(x, Y, \"multivariate\")\n\n# also save phenotypes in separate file\nopen(\"multivariate.phen\", \"w\") do io\n    for i in 1:n\n        println(io, Y[i, 1], \",\", Y[i, 2])\n    end\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"For multivariate IHT, one can store multiple phenotpyes as extra columns in the .fam file. The first 10 rows of such a file is visualized below:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":";head multivariate.fam","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"1\t1\t0\t0\t1\t-1.4101566028647934\t-0.4675708866010868\n2\t1\t0\t0\t1\t1.519406122085042\t-0.1521105344879844\n3\t1\t0\t0\t1\t-5.121683111513246\t1.4417764126708223\n4\t1\t0\t0\t1\t2.4188275607309677\t2.5303163340220953\n5\t1\t0\t0\t1\t2.6214639873372234\t1.005904479060761\n6\t1\t0\t0\t1\t1.0918272785956382\t2.8773472639961106\n7\t1\t0\t0\t1\t1.6444938174059964\t-0.3561578100979898\n8\t1\t0\t0\t1\t-1.3607927771748423\t0.049522727283193846\n9\t1\t0\t0\t1\t-3.9917926508357624\t1.8333328019574022\n10\t1\t0\t0\t1\t-2.494886509291137\t2.518184222337186","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Phenotypes can also be stored in a separate file. In this case, we require each subject's phenotype to occupy a different row. The file should not include a header line. Each row should be listed in the same order as in the PLINK and (for multivariate analysis) be comma separated. For example, the first 10 rows of such a file looks like:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":";head multivariate.phen","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"-1.4101566028647934,-0.4675708866010868\n1.519406122085042,-0.1521105344879844\n-5.121683111513246,1.4417764126708223\n2.4188275607309677,2.5303163340220953\n2.6214639873372234,1.005904479060761\n1.0918272785956382,2.8773472639961106\n1.6444938174059964,-0.3561578100979898\n-1.3607927771748423,0.049522727283193846\n-3.9917926508357624,1.8333328019574022\n-2.494886509291137,2.518184222337186","category":"page"},{"location":"man/examples/#Run-multivariate-IHT","page":"Examples","title":"Run multivariate IHT","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The values specified in path corresponds to the total number of non-zero k to be tested in cross validation. Since we simulated 10 true genetic predictors and 2 non-genetic predictors (an intercept term for each trait), k_true = 12. Because non-genetic covariates are not specified, an intercept with automatically be included. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# genotypes stored in multivariate.bed and phenotypes in multivariate.phen\nmses = cross_validate(\"multivariate\", MvNormal, phenotypes=\"multivariate.phen\", path=1:20);\n\n# use columns 6 and 7 of .fam as phenotypes\n# mses = cross_validate(\"multivariate\", MvNormal, phenotypes=[6, 7], path=1:20)\n\n# run directly with xla and Y (note: transpose is necessary to make samples into columns)\n# mses = cv_iht(Matrix(Y'), Transpose(xla), path=1:20)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\n\n\n\u001b[32mCross validating...100%|████████████████████████████████| Time: 0:00:08\u001b[39m\n\n\n\n\nCrossvalidation Results:\n\tk\tMSE\n\t1\t2066.2949539850247\n\t2\t2065.448718184275\n\t3\t1028.1142776401573\n\t4\t891.2895081829226\n\t5\t761.9356018442186\n\t6\t615.9435020075197\n\t7\t548.8271710694912\n\t8\t538.4145437051037\n\t9\t526.7503996446849\n\t10\t528.2000153966353\n\t11\t528.4124971317248\n\t12\t523.3419138092179\n\t13\t538.1305969330168\n\t14\t527.5197299749807\n\t15\t527.7948630499403\n\t16\t531.8215074425368\n\t17\t533.44640197685\n\t18\t535.7714663457024\n\t19\t540.7820708871902\n\t20\t548.1997323593387\n\nBest k = 12","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The best MSE is achieved at k=12. Let's run IHT with this estimate of k. Similarly, there are multiple ways to do so:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# genotypes stored in multivariate.bed and phenotypes in multivariate.phen\nresult = iht(\"multivariate\", 12, MvNormal, phenotypes=\"multivariate.phen\")\n\n# genotypes stored in multivariate.bed use columns 6 and 7 of .fam as phenotypes\n# result = iht(\"multivariate\", 12, MvNormal, phenotypes=[6, 7])\n\n# run cross validation directly with xla and Y (note: transpose is necessary to make samples into columns)\n# result = fit_iht(Matrix(Y'), Transpose(xla), k=12)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"****                   MendelIHT Version 1.4.1                  ****\n****     Benjamin Chu, Kevin Keys, Chris German, Hua Zhou       ****\n****   Jin Zhou, Eric Sobel, Janet Sinsheimer, Kenneth Lange    ****\n****                                                            ****\n****                 Please cite our paper!                     ****\n****         https://doi.org/10.1093/gigascience/giaa044        ****\n\nRunning sparse Multivariate Gaussian regression\nNumber of threads = 8\nLink functin = IdentityLink()\nSparsity parameter (k) = 12\nPrior weight scaling = off\nDoubly sparse projection = off\nDebias = off\nMax IHT iterations = 200\nConverging when tol < 0.0001 and iteration ≥ 5:\n\nIteration 1: loglikelihood = -2376.4820296189982, backtracks = 0, tol = 0.0\nIteration 2: loglikelihood = -2376.479923292264, backtracks = 0, tol = 1.1257263586193733e-6\nIteration 3: loglikelihood = -1957.3455984022858, backtracks = 0, tol = 0.34272673055481445\nIteration 4: loglikelihood = -1492.3671161424047, backtracks = 0, tol = 0.25917873408698855\nIteration 5: loglikelihood = -1182.1735594370286, backtracks = 0, tol = 0.23675182454701588\nIteration 6: loglikelihood = -1173.136551480164, backtracks = 0, tol = 0.07796393235528722\nIteration 7: loglikelihood = -1172.0113774449553, backtracks = 1, tol = 0.011552571250170897\nIteration 8: loglikelihood = -1171.889593655075, backtracks = 1, tol = 0.00348271732377267\nIteration 9: loglikelihood = -1171.8731432400346, backtracks = 1, tol = 0.0009816062764585914\nIteration 10: loglikelihood = -1171.8696778652588, backtracks = 1, tol = 0.0004151796336081729\nIteration 11: loglikelihood = -1171.868629414193, backtracks = 1, tol = 0.00023372232580958058\nIteration 12: loglikelihood = -1171.868259797147, backtracks = 1, tol = 0.0001366081371933243\nIteration 13: loglikelihood = -1171.8681230933576, backtracks = 1, tol = 8.144197684245368e-5\n\n\n\n\n\n\nCompute time (sec):     0.17782807350158691\nFinal loglikelihood:    -1171.8681230933576\nIterations:             13\nTrait 1's SNP PVE:      0.8882386104054829\nTrait 2's SNP PVE:      0.1797217149389984\n\nEstimated trait covariance:\n\u001b[1m2×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m trait1    \u001b[0m\u001b[1m trait2    \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\n─────┼──────────────────────\n   1 │  0.907309  -0.131274\n   2 │ -0.131274   1.57327\n\nTrait 1: IHT estimated 6 nonzero SNP predictors\n\u001b[1m6×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │      134    -0.412059\n   2 │      442    -1.22405\n   3 │      450    -1.5129\n   4 │     1891    -1.45489\n   5 │     2557     0.780163\n   6 │     3243    -0.833766\n\nTrait 1: IHT estimated 1 non-genetic predictors\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1     -0.14915\n\nTrait 2: IHT estimated 4 nonzero SNP predictors\n\u001b[1m4×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │     1014    -0.381763\n   2 │     1570     0.183475\n   3 │     5214     0.346505\n   4 │     9385    -0.18681\n\nTrait 2: IHT estimated 1 non-genetic predictors\n\u001b[1m1×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m Position \u001b[0m\u001b[1m Estimated_β \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼───────────────────────\n   1 │        1     0.812977","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The convergence criteria can be tuned by keywords tol and min_iter. ","category":"page"},{"location":"man/examples/#Check-answers","page":"Examples","title":"Check answers","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# estimated vs true first beta\nβ1 = result.beta[1, :]\ntrue_b1_idx = findall(!iszero, true_b[:, 1])\n[β1[true_b1_idx] true_b[true_b1_idx, 1]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"7×2 Matrix{Float64}:\n -0.412059  -0.388067\n -1.22405   -1.24972\n -1.5129    -1.53835\n  0.0       -0.0034339\n -1.45489   -1.47163\n  0.780163   0.758756\n -0.833766  -0.847906","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# estimated vs true second beta\nβ2 = result.beta[2, :]\ntrue_b2_idx = findall(!iszero, true_b[:, 2])\n[β2[true_b2_idx] true_b[true_b2_idx, 2]]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"3×2 Matrix{Float64}:\n -0.381763  -0.402269\n  0.346505   0.296183\n  0.0        0.125965","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# estimated vs true non genetic covariates (intercept)\n[result.c intercepts']","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"2×2 Matrix{Float64}:\n -0.14915   -0.172668\n  0.812977   0.729135","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# estimated vs true covariance matrix\n[vec(result.Σ) vec(true_Σ)]","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"4×2 Matrix{Float64}:\n  0.907309   0.955563\n -0.131274  -0.0884466\n -0.131274  -0.0884466\n  1.57327    1.62573","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Conclusion: ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"IHT found 9 true positives: 6/7 causal SNPs for trait 1, 2/3 causal SNPs for trait 2, and 1/2 intercept\nBecause we ran IHT with k=12, there are 3 false positives. \nEstimated trait covariance matrix closely match the true covariance\nThe proportion of phenotypic trait variances explained by genotypes are 0.88 and 0.15.","category":"page"},{"location":"man/examples/#Other-examples-and-functionalities","page":"Examples","title":"Other examples and functionalities","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Additional features are available as optional parameters in the fit_iht function, but they should be treated as experimental features. Interested users are encouraged to explore them and please file issues on GitHub if you encounter a problem.","category":"page"},{"location":"#Mendel-Iterative-Hard-Thresholding","page":"Home","title":"Mendel - Iterative Hard Thresholding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A modern approach to analyze data from a Genome Wide Association Studies (GWAS)","category":"page"},{"location":"#Package-Feature","page":"Home","title":"Package Feature","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Analyze large GWAS datasets intuitively.\nBuilt-in support for PLINK binary files via SnpArrays.jl and VCF files via VCFTools.jl.\nOut-of-the-box parallel computing routines for q-fold cross-validation.\nFits a variety of generalized linear models with any choice of link function.\nComputation directly on raw genotype files.\nEfficient handlings for non-genetic covariates.\nOptional acceleration (debias) step to dramatically improve speed.\nAbility to explicitly incorporate weights for predictors.\nAbility to enforce within and between group sparsity. \nNaive genotype imputation. \nEstimates nuisance parameter for negative binomial regression using Newton or MM algorithm. \nExcellent flexibility to handle different data structures and complements well with other Julia packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read our paper for more detail.","category":"page"},{"location":"#Supported-GLM-models-and-Link-functions","page":"Home","title":"Supported GLM models and Link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MendelIHT borrows distribution and link functions implementationed in GLM.jl and Distributions.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distribution Canonical Link Status\nNormal IdentityLink checkmark\nBernoulli LogitLink checkmark\nPoisson LogLink checkmark\nNegativeBinomial LogLink checkmark\nGamma InverseLink experimental\nInverseGaussian InverseSquareLink experimental","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples of these distributions in their default value is visualized in this post.","category":"page"},{"location":"#Available-link-functions","page":"Home","title":"Available link functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nProbitLink\nSqrtLink","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/getting_started.md\",\n    \"man/examples.md\",\n    \"man/math.md\",\n    \"man/contributing.md\",\n    \"man/api.md\",\n]\nDepth = 2","category":"page"},{"location":"man/api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"Here is a list of available function calls. A detailed description can be found below. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"man/api/#Wrapper-Functions","page":"API","title":"Wrapper Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"Most users will use the following wrapper functions. Users specify location of PLINK files and possibly the phenotype/covariate files. These functions will soon be updated to support VCF and BGEN formats.","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  iht\n  cross_validate","category":"page"},{"location":"man/api/#MendelIHT.iht","page":"API","title":"MendelIHT.iht","text":"iht(plinkfile, k, d, phenotypes=6, covariates=\"\", summaryfile=\"iht.summary.txt\",\n    betafile=\"iht.beta.txt\", kwargs...)\n\nRuns IHT with sparsity level k. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\nk: An Int for sparsity parameter = number of none-zero coefficients\nd: Distribution of phenotypes. Specify Normal for quantitative traits,   Bernoulli for binary traits, Poisson or NegativeBinomial for   count traits, and MvNormal for multiple quantitative traits. \n\nOptional Arguments\n\nphenotypes: Phenotype file name (String), an integer, or vector of integer. Integer(s)   coresponds to the column(s) of .fam file that stores phenotypes (default phenotypes=6).    Enter multiple integers for multivariate analysis (e.g. phenotypes=[6, 7]).   We recognize missing phenotypes as NA or -9. For quantitative traits   (univariate or multivariate), missing phenotypes are imputed with the mean. Binary   and count phenotypes cannot be imputed. Phenotype files are read using readdlm function   in Julia base. We require each subject's phenotype to occupy a different row. The file   should not include a header line. Each row should be listed in the same order as in   the PLINK and (for multivariate analysis) be comma separated. \ncovariates: Covariate file name. Default covariates=\"\" (in which case an intercept   term will be automatically included). If covariates file specified, it will be    read using readdlm function in Julia base. We require the covariate file to be   comma separated, and not include a header line. Each row should be listed in the   same order as in the PLINK. The first column should be all 1s to indicate an   intercept. All other columns not specified in exclude_std_idx will be standardized   to mean 0 variance 1. \nsummaryfile: Output file name for saving IHT's summary statistics. Default   summaryfile=\"iht.summary.txt\".\nbetafile: Output file name for saving IHT's estimated genotype effect sizes.    Default betafile=\"iht.beta.txt\". \ncovariancefile: Output file name for saving IHT's estimated trait covariance   matrix for multivariate analysis. Default covariancefile=\"iht.cov.txt\". \nexclude_std_idx: Indices of non-genetic covariates that should be excluded from   standardization. \nAll optional arguments available in fit_iht\n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.cross_validate","page":"API","title":"MendelIHT.cross_validate","text":"cross_validate(plinkfile, d, path=1:20, phenotypes=6, covariates=\"\", \n    cv_summaryfile=\"cviht.summary.txt\", q=5, kwargs...)\n\nRuns cross-validation to determinal optimal sparsity level k. Different sparsity levels are specified in path. \n\nArguments\n\nplinkfile: A String for input PLINK file name (without .bim/.bed/.fam suffixes)\nd: Distribution of phenotypes. Specify Normal for quantitative traits,   Bernoulli for binary traits, Poisson or NegativeBinomial for   count traits, and MvNormal for multiple quantitative traits. \n\nOptional Arguments\n\npath: Different values of k that should be tested. One can input a vector of    Int (e.g. path=[5, 10, 15, 20]) or a range (default path=1:20).\nphenotypes: Phenotype file name (String), an integer, or vector of integer. Integer(s)   coresponds to the column(s) of .fam file that stores phenotypes (default 6).    We recognize missing phenotypes as NA or -9. For quantitative traits   (univariate or multivariate), missing phenotypes are imputed with the mean. Binary   and count phenotypes cannot be imputed. Phenotype files are read using readdlm function   in Julia base. We require each subject's phenotype to occupy a different row. The file   should not include a header line. Each row should be listed in the same order as in   the PLINK. \ncovariates: Covariate file name. Default covariates=\"\" (in which case an intercept   term will be automatically included). If covariates file specified, it will be    read using readdlm function in Julia base. We require the covariate file to be   comma separated, and not include a header line. Each row should be listed in the   same order as in the PLINK. The first column should be all 1s to indicate an   intercept. All other columns not specified in exclude_std_idx will be standardized   to mean 0 variance 1\ncv_summaryfile: Output file name for saving IHT's cross validation summary statistics.   Default cv_summaryfile=\"cviht.summary.txt\".\nq: Number of cross validation folds. Larger means more accurate and more computationally   intensive. Should be larger 2 and smaller than 10. Default q=5. \nAll optional arguments available in cv_iht\n\n\n\n\n\n","category":"function"},{"location":"man/api/#Core-Functions","page":"API","title":"Core Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"Users can also use the fit_iht and cv_iht functions directly. One must import genotypes via SnpArrays.jl (and use SnpLinAlg type for x argument) or VCFTools.jl. Phenotypes/covariates must also be imported using Julia's standard routine (typically with Base.readdlm). ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  fit_iht\n  cv_iht","category":"page"},{"location":"man/api/#MendelIHT.fit_iht","page":"API","title":"MendelIHT.fit_iht","text":"fit_iht(y, x, z; k=10, J=1, d = Normal(), l=IdentityLink(), group=Int[], \n    weight=Float64[], est_r=:None, debias=false, verbose=true, tol=1e-4,\n    max_iter=200, max_step=3, io=stdout)\n\nFits a model on design matrix (genotype data) x, response (phenotype) y,  and non-genetic covariates z on a specific sparsity parameter k. Variables in  x and z will both be subject to sparsity constraint. \n\nIf k is a constant, then each group will have the same sparsity level. To run doubly  sparse IHT, construct k to be a vector where k[i] indicates the max number of predictors for group i. \n\nArguments:\n\ny: Phenotype vector or matrix. Should be an Array{T, 1} (single traits) or   Array{T, 2} (multivariate Gaussian traits). For multivariate traits, each    column of y should be a sample. \nx: Genotype matrix (an Array{T, 2} or SnpLinAlg). For univariate   analysis, samples are rows of x. For multivariate analysis, samples are   columns of x (i.e. input Transpose(x) for SnpLinAlg)\nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}.   For univariate analysis, sample covariates are rows of z. For multivariate   analysis, sample covariates are columns of z. If this is not specified, an   intercept term will be included automatically. If z is specified, make sure   the first column (row) is all 1s to represent the intercept. \n\nOptional Arguments:\n\nk: Number of non-zero predictors. Can be a constant or a vector (for group IHT). \nJ: The number of maximum groups (set as 1 if no group infomation available)\nd: Distribution of phenotypes. Specify Normal() for quantitative traits,   Bernoulli() for binary traits, Poisson() or NegativeBinomial() for   count traits, and MvNormal() for multiple quantitative traits. \nl: A link function. The recommended link functions are l=IdentityLink() for   quantitative traits, l=LogitLink() for binary traits, l=LogLink() for Poisson   distribution, and l=Loglink() for NegativeBinomial distribution. \ngroup: vector storing (non-overlapping) group membership\nweight: vector storing vector of weights containing prior knowledge on each SNP\nest_r: Symbol (:MM, :Newton or :None) to estimate nuisance parameters for negative binomial regression\nuse_maf: boolean indicating whether we want to scale projection with minor allele frequencies (see paper)\ndebias: boolean indicating whether we debias at each iteration\nverbose: boolean indicating whether we want to print intermediate results\ntol: used to track convergence\nmax_iter: is the maximum IHT iteration for a model to converge. Defaults to 200, or 100 for cross validation\nmin_iter: is the minimum IHT iteration before checking for convergence. Defaults to 5.\nmax_step: is the maximum number of backtracking per IHT iteration. Defaults 3\nio: An IO object for displaying intermediate results. Default stdout.\ninit_beta: Whether to initialize beta values to univariate regression values.    Currently only Gaussian traits can be initialized. Default false. \n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.cv_iht","page":"API","title":"MendelIHT.cv_iht","text":"cv_iht(y, x, z; path=1:20, q=5, d=Normal(), l=IdentityLink(), est_r=:None,\n    group=Int[], weight=Float64[], folds=rand(1:q, is_multivariate(y) ?\n    size(x, 2) : size(x, 1)), debias=false, verbose=true,\n    max_iter=100, min_iter=20, init_beta=true)\n\nFor each model specified in path, performs q-fold cross validation and  returns the (averaged) deviance residuals. The purpose of this function is to find the best sparsity level k, obtained from selecting the model with the minimum out-of-sample error. \n\nTo check if multithreading is enabled, check output of Threads.nthreads().\n\nArguments:\n\ny: Phenotype vector or matrix. Should be an Array{T, 1} (single traits) or   Array{T, 2} (multivariate Gaussian traits). For multivariate traits, each    column of y should be a sample. \nx: Genotype matrix (an Array{T, 2} or SnpLinAlg). For univariate   analysis, samples are rows of x. For multivariate analysis, samples are   columns of x (i.e. input Transpose(x) for SnpLinAlg)\nz: Matrix of non-genetic covariates of type Array{T, 2} or Array{T, 1}.   For univariate analysis, sample covariates are rows of z. For multivariate   analysis, sample covariates are columns of z. If this is not specified, an   intercept term will be included automatically. If z is specified, make sure   the first column (row) is all 1s to represent the intercept. \n\nOptional Arguments:\n\npath: Different values of k that should be tested. One can input a vector of    Int (e.g. path=[5, 10, 15, 20]) or a range (default path=1:20).\nq: Number of cross validation folds. Larger means more accurate and more computationally   intensive. Should be larger 2 and smaller than 10. Default q=5.\nd: Distribution of phenotypes. Specify Normal() for quantitative traits,   Bernoulli() for binary traits, Poisson() or NegativeBinomial() for   count traits, and MvNormal() for multiple quantitative traits. \nl: A link function. The recommended link functions are l=IdentityLink() for   quantitative traits, l=LogitLink() for binary traits, l=LogLink() for Poisson   distribution, and l=Loglink() for NegativeBinomial distribution. \nest_r: Symbol (:MM, :Newton or :None) to estimate nuisance parameters for negative binomial regression\ngroup: vector storing group membership for each predictor\nweight: vector storing vector of weights containing prior knowledge on each predictor\nfolds: Vector that separates the sample into q disjoint subsets\ndebias: Boolean indicating whether we should debias at each IHT step. Defaults false\nverbose: Boolean indicating whether to print mean squared error for each k in path. Defaults true\nmax_iter: is the maximum IHT iteration for a model to converge. Defaults to 100 \nmin_iter: is the minimum IHT iteration before checking for convergence. Defaults to 5.\ninit_beta: Whether to initialize beta values to univariate regression values.    Currently only Gaussian traits can be initialized. Default false. \n\n\n\n\n\n","category":"function"},{"location":"man/api/#Specifying-Groups-and-Weights","page":"API","title":"Specifying Groups and Weights","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"When you have group and weight information, you input them as optional arguments in fit_iht and cv_iht. The weight vector is a vector of Float64, while the group vector is a vector of Int. For instance,","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"    g = #import group vector\n    w = #import weight vector\n    ng = length(unique(g)) # specify number of non-zero groups\n    result = fit_iht(y, x, z; J=ng, k=10, d=Normal(), l=IdentityLink(), group=g, weight=w)","category":"page"},{"location":"man/api/#Simulation-Utilities","page":"API","title":"Simulation Utilities","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"For complex simulations, please use TraitSimulation.jl. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"MendelIHT provides very naive simulation utilities, which were written before TraitSimulation.jl was developed.","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  simulate_random_snparray\n  simulate_correlated_snparray","category":"page"},{"location":"man/api/#MendelIHT.simulate_random_snparray","page":"API","title":"MendelIHT.simulate_random_snparray","text":"simulate_random_snparray(s::String, n::Integer, p::Integer; \n    [mafs::Vector{Float64}], [min_ma::Integer])\n\nCreates a random SnpArray in the current directory without missing value,  where each SNP has ⫺5 (default) minor alleles. \n\nNote: if supplied minor allele frequency is extremely small, it could take a long time for the simulation to generate samples where at least min_ma (defaults to 5) are present. \n\nArguments:\n\ns: name of SnpArray that will be created in the current directory. To not   create file, use undef.\nn: number of samples\np: number of SNPs\n\nOptional Arguments:\n\nmafs: vector of desired minor allele freuqencies (uniform(0,0.5) by default)\nmin_ma: the minimum number of minor alleles that must be present for each   SNP (defaults to 5)\n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.simulate_correlated_snparray","page":"API","title":"MendelIHT.simulate_correlated_snparray","text":"simulate_correlated_snparray(s, n, p; block_length, hap, prob)\n\nSimulates a SnpArray with correlation. SNPs are divided into blocks where each adjacent SNP is the same with probability prob. There are no correlation between blocks.\n\nArguments:\n\nn: number of samples\np: number of SNPs\ns: name of SnpArray that will be created (memory mapped) in the current directory. To not memory map, use undef.\n\nOptional arguments:\n\nblock_length: length of each LD block\nhap: number of haplotypes to simulate for each block\nprob: with probability prob an adjacent SNP would be the same. \n\n\n\n\n\n","category":"function"},{"location":"man/api/","page":"API","title":"API","text":"note: Note\nSimulating a SnpArray with n subjects and p SNPs requires up to 2np bits of RAM. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  simulate_random_response","category":"page"},{"location":"man/api/#MendelIHT.simulate_random_response","page":"API","title":"MendelIHT.simulate_random_response","text":"simulate_random_response(x, k, d, l; kwargs...)\n\nThis function simulates a random response (trait) vector y. When the  distribution d is from Poisson, Gamma, or Negative Binomial, we simulate  β ∼ N(0, 0.3) to roughly ensure the mean of response y doesn't become too large. For other distributions, we choose β ∼ N(0, 1). \n\nArguments\n\nx: Design matrix\nk: the true number of predictors. \nd: The distribution of the simulated trait (note typeof(d) = UnionAll but typeof(d()) is an actual distribution: e.g. Normal)\nl: The link function. Input canonicallink(d()) if you want to use the canonical link of d.\n\nOptional arguments\n\nr: The number of success until stopping in negative binomial regression, defaults to 10\nα: Shape parameter of the gamma distribution, defaults to 1\nZu: Effect of non-genetic covariates. Zu should have dimension n × 1. \n\n\n\n\n\nsimulate_random_response(x, k, traits)\n\nSimulates a response matrix Y where each row is an independent multivariate Gaussian with length trait. There are k non-zero β over all traits. Each trait shares overlap causal SNPs. The covariance matrix Σ is positive definite and symmetric.\n\nArguments\n\nx: Design matrix of dimension n × p. Each row is a sample. \nk: the total true number of causal SNPs (predictors)\ntraits: Number of traits\n\nOptional arguments\n\nZu: Effect of non-genetic covariates. Zu should have dimension n × traits. \noverlap: Number of causal SNPs shared by all traits. Shared SNPs does not have the same effect size. \n\nOutputs\n\nY: Response matrix where each row is sampled from a multivariate normal with mean μ[i] = X[i, :] * true_b and variance Σ\nΣ: the symmetric, positive definite covariance matrix used\ntrue_b: A sparse matrix containing true beta values.\ncorrect_position: Non-zero indices of true_b\n\n\n\n\n\n","category":"function"},{"location":"man/api/","page":"API","title":"API","text":"note: Note\nFor negative binomial and gamma, the link function must be LogLink. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  make_bim_fam_files","category":"page"},{"location":"man/api/#MendelIHT.make_bim_fam_files","page":"API","title":"MendelIHT.make_bim_fam_files","text":"make_bim_fam_files(x::SnpArray, y, name::String)\n\nCreates .bim and .bed files from a SnpArray. \n\nArguments:\n\nx: A SnpArray (i.e. .bed file on the disk) for which you wish to create corresponding .bim and .fam files.\nname: string that should match the .bed file (Do not include .bim or .fam extensions in name).\ny: Trait vector that will go in to the 6th column of .fam file. \n\n\n\n\n\n","category":"function"},{"location":"man/api/#Other-Useful-Functions","page":"API","title":"Other Useful Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"MendelIHT additionally provides useful utilities that may be of interest to a few advanced users. ","category":"page"},{"location":"man/api/","page":"API","title":"API","text":"  iht_run_many_models\n  pve","category":"page"},{"location":"man/api/#MendelIHT.iht_run_many_models","page":"API","title":"MendelIHT.iht_run_many_models","text":"Runs IHT across many different model sizes specifed in path using the full design matrix. Same as cv_iht but DOES NOT validate in a holdout set, meaning that this will definitely induce overfitting as we increase model size. Use this if you want to quickly estimate a range of feasible model sizes before  engaging in full cross validation. \n\n\n\n\n\n","category":"function"},{"location":"man/api/#MendelIHT.pve","page":"API","title":"MendelIHT.pve","text":"pve(y, X, β; l = IdentityLink())\n\nEstimates phenotype's Proportion of Variance Explained (PVE) by typed genotypes  (i.e. chip heritability or SNP heritability).\n\nModel\n\nWe compute Var(ŷ) / Var(y) where y is the raw phenotypes, X contains  all the genotypes, and ŷ = Xβ is the predicted (average) phenotype values from the statistical model β. Intercept is NOT included.\n\n\n\n\n\n","category":"function"}]
}

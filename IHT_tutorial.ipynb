{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHT.jl tutorial\n",
    "\n",
    "In this tutorial we explore some of the functionality of the IHT.jl package, which implements iterative hard threhsolding on floating point arrays and binary PLINK data. \n",
    "\n",
    "The first part of the tutorial demonstrates how to handle floating point data. Later we will show how to use [PLINK binary genotype files](http://pngu.mgh.harvard.edu/~purcell/plink/binary.shtml). IHT.jl offers two parallel compute frameworks for PLINK data : multicore CPUs or massively parallel GPUs. This tutorial will only demonstrate the CPU version, but instructions for GPU use are included.\n",
    "\n",
    "IHT minimizes the residual sum of squares $\\frac{1}{2} \\| \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\|$ for the data matrix $\\boldsymbol{X}$, response vector $\\boldsymbol{y}$, and coefficient vector $\\boldsymbol{\\beta}$. If $\\boldsymbol{\\beta}$ is $k$-sparse, then we have a sparse regression problem.\n",
    "\n",
    "Let us start by defining several simulation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x23999 Array{Float64,2}:\n",
       " -1.21187    -0.0372144   0.90941     …  -1.16745    -0.106107   0.492012\n",
       "  0.429356    2.44578    -0.638042        1.64271    -0.438327  -1.05184 \n",
       "  1.47077     0.404689   -0.597546       -0.0787781   0.622279  -0.931525\n",
       " -0.139799   -0.103057    0.163683       -2.75938     0.204186  -0.497543\n",
       " -0.494523   -0.28781    -0.00782332      1.03826    -0.85644    0.496935\n",
       " -1.05075     0.523897   -1.27745     …  -0.849481   -0.281006  -0.236835\n",
       " -0.974419   -1.27537     0.558627       -0.238681   -0.635668  -1.05491 \n",
       " -1.10666    -0.98015    -0.784369        0.0204989   1.22107    1.21211 \n",
       " -2.09573    -2.49759    -0.945998        0.565168    1.21262   -1.16107 \n",
       " -0.938486    0.439453    0.765375       -0.911794    0.642921   1.28155 \n",
       " -0.190322    0.388568   -1.27155     …  -0.49365     1.50609   -0.710576\n",
       "  1.17238    -0.891597   -0.669788        0.507479   -1.41907   -0.141352\n",
       " -1.91608     1.44175     0.0441058      -3.92381    -0.242271   0.36265 \n",
       "  ⋮                                   ⋱                                  \n",
       "  1.12193    -1.51104     1.18153        -1.2036     -1.4669    -1.24218 \n",
       "  0.208391    0.439368    0.829239        0.496732    1.34363    1.91497 \n",
       " -0.379936    1.33726    -1.95194     …   1.67432    -0.12236   -0.946827\n",
       " -0.936262   -0.142562   -1.28112        -0.31079     1.85511    0.811879\n",
       " -1.81118    -0.569146    0.451864        0.801288   -1.20789   -2.13382 \n",
       " -1.63607    -0.804924    0.0125132       0.676692    0.458359  -1.66104 \n",
       " -0.30845     0.169263    0.540949        1.01373     1.79327    0.203354\n",
       "  0.144977    1.24275     0.00286263  …  -0.245945    0.55308   -0.595858\n",
       "  0.530378   -0.821785   -0.15109        -0.0287311  -0.234049   1.15474 \n",
       "  0.420941    0.014556    0.218322        1.2586     -1.24348   -2.02591 \n",
       " -0.0253818   0.542621   -0.113816        0.611086    0.968086   1.25736 \n",
       "  1.05336     0.970829    0.208959       -0.0506156  -0.766039   0.18125 "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(5) # for later parallel XV\n",
    "using IHT, PLINK # PLINK.jl handles PLINK files\n",
    "n = 5000   # number of cases\n",
    "p = 23999 # number of predictors\n",
    "k = 10  # number of true predictors\n",
    "s = 0.1 # standard deviation of noise\n",
    "srand(2016)\n",
    "x_temp = randn(n,p) # data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want our simulation to be reproducible, configure $\\boldsymbol{\\beta}$ with a fixed random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       "  3310\n",
       "  4460\n",
       "  5861\n",
       "  9731\n",
       " 11294\n",
       " 11378\n",
       " 14217\n",
       " 15118\n",
       " 19815\n",
       " 23295"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = zeros(p)\n",
    "b[1:k] = randn(k)\n",
    "shuffle!(b)\n",
    "bidx = find(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a noisy response $\\boldsymbol{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Float64,1}:\n",
       "  1.3482   \n",
       " -1.72093  \n",
       " -1.15338  \n",
       " -3.49947  \n",
       "  1.86919  \n",
       " -0.808959 \n",
       " -4.29945  \n",
       " -0.525047 \n",
       " -1.43292  \n",
       "  0.56259  \n",
       "  2.81115  \n",
       " -1.39126  \n",
       " -0.0480848\n",
       "  ⋮        \n",
       "  0.288666 \n",
       "  0.361842 \n",
       "  4.86104  \n",
       " -0.745529 \n",
       "  1.27571  \n",
       " -1.86058  \n",
       " -2.45003  \n",
       " -1.07489  \n",
       "  2.40557  \n",
       "  0.227523 \n",
       "  0.891287 \n",
       "  0.64908  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x_temp*b + s*randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure a regression problem. In this case, we need a data matrix with a grand mean included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x24000 Array{Float64,2}:\n",
       " -1.21187    -0.0372144   0.90941     …  -0.106107   0.492012  1.0\n",
       "  0.429356    2.44578    -0.638042       -0.438327  -1.05184   1.0\n",
       "  1.47077     0.404689   -0.597546        0.622279  -0.931525  1.0\n",
       " -0.139799   -0.103057    0.163683        0.204186  -0.497543  1.0\n",
       " -0.494523   -0.28781    -0.00782332     -0.85644    0.496935  1.0\n",
       " -1.05075     0.523897   -1.27745     …  -0.281006  -0.236835  1.0\n",
       " -0.974419   -1.27537     0.558627       -0.635668  -1.05491   1.0\n",
       " -1.10666    -0.98015    -0.784369        1.22107    1.21211   1.0\n",
       " -2.09573    -2.49759    -0.945998        1.21262   -1.16107   1.0\n",
       " -0.938486    0.439453    0.765375        0.642921   1.28155   1.0\n",
       " -0.190322    0.388568   -1.27155     …   1.50609   -0.710576  1.0\n",
       "  1.17238    -0.891597   -0.669788       -1.41907   -0.141352  1.0\n",
       " -1.91608     1.44175     0.0441058      -0.242271   0.36265   1.0\n",
       "  ⋮                                   ⋱                           \n",
       "  1.12193    -1.51104     1.18153        -1.4669    -1.24218   1.0\n",
       "  0.208391    0.439368    0.829239        1.34363    1.91497   1.0\n",
       " -0.379936    1.33726    -1.95194     …  -0.12236   -0.946827  1.0\n",
       " -0.936262   -0.142562   -1.28112         1.85511    0.811879  1.0\n",
       " -1.81118    -0.569146    0.451864       -1.20789   -2.13382   1.0\n",
       " -1.63607    -0.804924    0.0125132       0.458359  -1.66104   1.0\n",
       " -0.30845     0.169263    0.540949        1.79327    0.203354  1.0\n",
       "  0.144977    1.24275     0.00286263  …   0.55308   -0.595858  1.0\n",
       "  0.530378   -0.821785   -0.15109        -0.234049   1.15474   1.0\n",
       "  0.420941    0.014556    0.218322       -1.24348   -2.02591   1.0\n",
       " -0.0253818   0.542621   -0.113816        0.968086   1.25736   1.0\n",
       "  1.05336     0.970829    0.208959       -0.766039   0.18125   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = zeros(n,p+1)\n",
    "setindex!(x, x_temp, :, 1:p)\n",
    "x[:,end] = 1.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `x`, run iterative hard thresholding to pull the best model of size `k`. The function call is `L0_reg`, which returns an `IHTReults` object `output` with the following fields:\n",
    "\n",
    "    - output.time => compute time in seconds\n",
    "    - output.iter => iterations taken until convergence\n",
    "    - output.loss => residual sum of squares at convergence\n",
    "    - output.beta => beta vector at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.341588  -0.346533 \n",
       "  0.612107   0.612489 \n",
       "  0.17465    0.175425 \n",
       " -0.494423  -0.494717 \n",
       "  1.36306    1.36302  \n",
       " -0.58849   -0.588894 \n",
       "  0.155924   0.154568 \n",
       "  0.031168   0.0318823\n",
       "  0.374056   0.374349 \n",
       " -1.38168   -1.38392  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = L0_reg(x,y,k);\n",
    "bk = copy(output.beta) # copy the beta for later use\n",
    "[b[bidx] bk[bidx]]     # did we get the correct model and coefficient values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that IHT returns all the correct nonzero coefficients. The coefficient values themselves are fairly close to their originals. We expect this since `s = 0.1` does not yield a very noisy `y`. Observe what happens when we increase the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.341588  -0.346533   -0.326644\n",
       "  0.612107   0.612489    0.604358\n",
       "  0.17465    0.175425    0.0     \n",
       " -0.494423  -0.494717   -0.612899\n",
       "  1.36306    1.36302     1.39415 \n",
       " -0.58849   -0.588894   -0.613471\n",
       "  0.155924   0.154568    0.0     \n",
       "  0.031168   0.0318823   0.0     \n",
       "  0.374056   0.374349    0.0     \n",
       " -1.38168   -1.38392    -1.33059 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 5 # very noisy!\n",
    "y2 = x_temp*b + s*randn(n)\n",
    "output2 = L0_reg(x, y2, k);\n",
    "bk2 = copy(output2.beta)\n",
    "[b[bidx] bk[bidx] bk2[bidx]] # more noisy response = less accurate estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `b` is the original model, `bk` is the estimated model with small noise (`s = 0.1`), and `bk2` is the estimated model with high noise (`s = 5`). The coefficient values are less accurate, and several nonzero values are not recovered. A failure to recover the correct model does not indicate that IHT model selection performance is necessarily bad. To fully analyze model selection performance, we still need some sort of benchmark. A later section on regularization paths and LASSO will address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHT for GWAS\n",
    "\n",
    "IHT.jl ships with facilities for using [PLINK.jl](https://github.com/klkeys/PLINK.jl) for GWAS analysis. PLINK.jl interfaces with PLINK binary genotype files (BED files). In this section of the tutorial we will demonstrate IHT.jl GWAS facilities on simulated binary genotype data. Luckily, PLINK.jl ships with some simulated binary data.\n",
    "\n",
    "Note that PLINK.jl requires both a compressed BED file *and* its transpose in order to ensure fast linear algebra operations. Users who wish to use their own BED-BIM-FAM files should generate a transposed BED file before using PLINK.jl.\n",
    "\n",
    "The aforementioned simulated genotype data contains 5000 cases and 24,000 SNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,24001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = expanduser(\"~/.julia/v0.4/PLINK/data/\")\n",
    "xpath = fpath * \"x_test.bed\"\n",
    "xtpath = fpath * \"xt_test.bed\"\n",
    "xbed = BEDFile(xpath, xtpath);\n",
    "n,p = size(xbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are actually 24,001 predictors. The previous constructor for `x` adds a nongenetic covariate of zeros by default. In our case, we want to use the grand mean (vector of ones). Neither IHT.jl nor PLINK.jl knows this, so we add the ones manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x5000 SharedArray{Float64,2}:\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill!(xbed.covar.x, 1.0)\n",
    "fill!(xbed.covar.xt, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to calculate column means and precisions. Note that the `mean!` and `prec!` functions do not know any information about the predictors themselves. In the case of the grand mean, the functions return a mean of 1 and a precision of infinity. We must manually correct this in order to not penalize the grand mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean!(xbed) # compute means in-place\n",
    "prec!(xbed) # compute precisions in-place\n",
    "xbed.means[end] = 0.0 # index \"end\" substitutes for position of grand mean in x.means! \n",
    "xbed.precs[end] = 1.0 # same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally good practice to compute means and precisions once and then save them to file. Similarly, most applications will read nongenetic covariates from file. In fact, IHT.jl will **require** means, precisions, and covariates from file for crossvalidation functions, a topic that we will discuss later. In the meantime, observe that PLINK.jl contains constructor for `BEDFile` object when all data are stored on the disk:\n",
    "\n",
    "    covpath = fpath * \"covfile.txt\"; # this is a delimited TXT file\n",
    "    mpath = fpath * \"means.bin\"; # note: this is a BINARY file, not a TEXT file; we will explain why later\n",
    "    ppath = fpath * \"precs.bin\"; # same as means\n",
    "    x = BEDFile(xpath, xtpath, covpath, mpath, ppath); # this loads a BEDFile entirely from hard disk\n",
    "    \n",
    "`BEDFile` objects are designed to facilitate linear algebra operations with binary PLINK data. Currently PLINK.jl supports matrix-vector multiplcations $\\boldsymbol{X}^T \\boldsymbol{z}$ with **dense** $\\boldsymbol{z}$ and $\\boldsymbol{X} \\boldsymbol{\\beta}$ with **sparse** $\\boldsymbol{\\beta}$. This is sufficient to run IHT with `BEDFile`s. Let us make a new simulated model with our `BEDFile` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element SharedArray{Float64,1}:\n",
       " -0.24977 \n",
       " -0.026113\n",
       " -1.85642 \n",
       "  0.381802\n",
       "  4.02698 \n",
       " -4.80148 \n",
       " -5.17884 \n",
       "  0.115311\n",
       "  3.6929  \n",
       "  0.777447\n",
       " -0.59551 \n",
       " -1.23767 \n",
       " -5.22835 \n",
       "  ⋮       \n",
       " -3.24521 \n",
       " -0.962122\n",
       "  3.64983 \n",
       " -3.0133  \n",
       " -2.31627 \n",
       "  2.52531 \n",
       " -2.11933 \n",
       " -5.07293 \n",
       " -3.3227  \n",
       "  1.25493 \n",
       " -2.34029 \n",
       " -0.60029 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbed = SharedArray(Float64, p, pids=procs(xbed)) # a model b to use with the BEDFile\n",
    "bbed[1:k] = randn(k) # random coefficients\n",
    "shuffle!(bbed) # random model\n",
    "bidxbed = find(bbed) # store locations of nonzero coefficients\n",
    "idx = bbed .!= 0.0 # need BitArray indices of nonzeroes in b for A_mul_B\n",
    "xb = A_mul_B(xbed, bbed, idx, k, pids=procs(xbed)) # compute x*b\n",
    "ybed2 = xb + 0.1*randn(n) # unfortunately this yields a Vector, which we must then convert to SharedVector\n",
    "ybed = convert(SharedVector{Float64}, ybed2) # our response variable with the BEDFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to IHT is the same as with the floating point `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.176622   -0.179789 \n",
       "  1.80336     1.80377  \n",
       "  0.135377    0.137861 \n",
       " -0.85557    -0.854269 \n",
       "  1.022       1.02445  \n",
       "  0.0903269   0.0930143\n",
       " -0.442234   -0.439589 \n",
       " -1.46398    -1.46272  \n",
       "  0.705111    0.705692 \n",
       "  0.0109637   0.0107966"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = L0_reg(xbed, ybed, k)\n",
    "bk = copy(output.beta) # copy the beta for later use\n",
    "[bbed[bidxbed] bk[bidxbed]]     # did we get the correct model and coefficient values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us close this section with a brief discussion about GPU use. Using GPUs with IHT.jl require both a `BEDFile` object and a filepath to the GPU kernels. The GPU kernels ship with the PLINK.jl package in the package subdirectory `/src/kernels`. Assuming that a suitable GPU device is available on the machine, one can call `L0_reg` with GPU facilities using\n",
    "\n",
    "    kernfile = open(readall, expanduser(\"~/.julia/v0.4/PLINK/src/kernels/iht_kernels64.cl\")) # read Float64 kernels\n",
    "    output = L0_reg(xbed, ybed, k, kernfile) # use GPU for L0_reg\n",
    "\n",
    "The previous code creates a long string object called `kernfile` that contains all of the OpenCL kernel code for the GPU.\n",
    "We will not execute this code here since we cannot assume that the user has a dedicated GPU on their machine.\n",
    "Note that this example assumes that the PLINK module is installed in the default library directory (`~/.julia/v0.4/`). Nonstandard library installations should point to the correct location of the kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Paths and LASSO\n",
    "\n",
    "Until this point we have merely shown how to use IHT to compute the best model of size $k$.\n",
    "Often this information alone is not terribly useful.\n",
    "Ideally we would like to test several models and find the best one.\n",
    "Many tools exist to conduct this model selection procedure, but the [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) is by far the most popular.\n",
    "In this section we will use the LASSO to benchmark the model selection capabilities of IHT.\n",
    "[GLMNet.jl](https://github.com/simonster/GLMNet.jl) is one implementation of the LASSO in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Least Squares GLMNet Solution Path (11 solutions for 24000 predictors in 43 passes):\n",
       "11×3 DataFrames.DataFrame\n",
       "│ Row │ df │ pct_dev    │ λ        │\n",
       "├─────┼────┼────────────┼──────────┤\n",
       "│ 1   │ 0  │ 0.0        │ 1.40759  │\n",
       "│ 2   │ 1  │ 0.00350737 │ 1.34361  │\n",
       "│ 3   │ 2  │ 0.0125526  │ 1.28254  │\n",
       "│ 4   │ 2  │ 0.0923299  │ 0.66872  │\n",
       "│ 5   │ 2  │ 0.094976   │ 0.638325 │\n",
       "│ 6   │ 5  │ 0.115147   │ 0.515862 │\n",
       "│ 7   │ 6  │ 0.145076   │ 0.3      │\n",
       "│ 8   │ 6  │ 0.148561   │ 0.27     │\n",
       "│ 9   │ 9  │ 0.149201   │ 0.265    │\n",
       "│ 10  │ 10 │ 0.149532   │ 0.263    │\n",
       "│ 11  │ 12 │ 0.150085   │ 0.26     │"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using GLMNet\n",
    "srand(2016)\n",
    "dfmax = 11\n",
    "nlambda = 30\n",
    "lambda = [1.40759, 1.34361, 1.28254, 0.66872, 0.638325, 0.515862, 0.3, 0.27, 0.265, 0.263, 0.26] # lambda values\n",
    "penalty_factor = ones(size(x,2)); penalty_factor[end] = 0.0 # do not penalize grand mean\n",
    "lassopath = glmnet(x, y2, lambda=lambda, penalty_factor=penalty_factor) # vector lambda ensures entry with df = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the estimates of $\\boldsymbol{\\beta}$ with `k = 10` nonzeroes. In our case, row 10 contains the model size that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.341588  -0.326644  -0.0570387\n",
       "  0.612107   0.604358   0.351269 \n",
       "  0.17465    0.0        0.0      \n",
       " -0.494423  -0.612899  -0.33027  \n",
       "  1.36306    1.39415    1.12938  \n",
       " -0.58849   -0.613471  -0.361091 \n",
       "  0.155924   0.0        0.0      \n",
       "  0.031168   0.0        0.0      \n",
       "  0.374056   0.0        0.0      \n",
       " -1.38168   -1.33059   -1.07278  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = convert(Matrix{Float64}, lassopath.betas) # must convert the `betas` object to a useable form\n",
    "[b[bidx] bk2[bidx] betas[bidx,10]] # b -> true model, bk2 -> previous estimate from IHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO selects the same nonzeroes as IHT and maintains the correct sign of the coefficients. However, the coefficients are \"deflated\" towards zero because LASSO is a shrinkage operator. Users should consider this fact when choosing a feature selection tool. In some scenarios, such as genome-wide association studies, the coefficient values are often quite small, and shrinkage can drive these estimated coefficients to zero. In doing so, selection with the LASSO complicates accurate estimation of the statistical model.\n",
    "\n",
    "Users should note that the call to `glmnet` returns several models because LASSO is most efficient when used to compute a set of models instead of one model. IHT.jl provides `iht_path` to mimic this behavior.\n",
    "Let us compute model sizes $1, 2, \\ldots, 11$ with IHT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x10 Array{Float64,2}:\n",
       " 0.0       0.0       0.0        0.0       …  -0.335262  -0.337671  -0.334219\n",
       " 0.0       0.0       0.0        0.0           0.614141   0.613477   0.615553\n",
       " 0.0       0.0       0.0        0.0           0.0        0.0        0.0     \n",
       " 0.0       0.0       0.0       -0.597181     -0.611819  -0.612644  -0.612667\n",
       " 1.38657   1.39234   1.38222    1.39867       1.40115    1.40283    1.40294 \n",
       " 0.0       0.0      -0.625356  -0.621444  …  -0.60589   -0.60554   -0.60714 \n",
       " 0.0       0.0       0.0        0.0           0.0        0.0        0.0     \n",
       " 0.0       0.0       0.0        0.0           0.0        0.0        0.0     \n",
       " 0.0       0.0       0.0        0.0           0.0        0.0        0.0     \n",
       " 0.0      -1.32376  -1.32592   -1.33478      -1.34528   -1.34168   -1.3408  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmodels = 10\n",
    "pathidx = collect(1:nmodels)\n",
    "ihtbetas = iht_path(x, y2, pathidx) # note that ihtpath is a sparse matrix...\n",
    "full(ihtbetas[bidx,:]) # progression of coefficients entering model; rightmost entry is k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the rightmost entry with `bk2[bidx]`, we see that the estimated nonzeroes and their coefficients are essentially the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.326644  -0.334219\n",
       "  0.604358   0.615553\n",
       "  0.0        0.0     \n",
       " -0.612899  -0.612667\n",
       "  1.39415    1.40294 \n",
       " -0.613471  -0.60714 \n",
       "  0.0        0.0     \n",
       "  0.0        0.0     \n",
       "  0.0        0.0     \n",
       " -1.33059   -1.3408  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bk2[bidx] full(ihtbetas[bidx,10])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iht_path` works with `BEDFile`s too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.179789   -0.179741 \n",
       "  1.80377     1.80398  \n",
       "  0.137861    0.137905 \n",
       " -0.854269   -0.854323 \n",
       "  1.02445     1.02459  \n",
       "  0.0930143   0.093021 \n",
       " -0.439589   -0.439666 \n",
       " -1.46272    -1.46291  \n",
       "  0.705692    0.705768 \n",
       "  0.0107966   0.0108142"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihtbetasbed = iht_path(xbed, ybed, pathidx)\n",
    "[bk[bidxbed] full(ihtbetasbed[bidxbed,10])] # here we just view k = 10 for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation\n",
    "\n",
    "These exploratory efforts are admittedly not illuminating. In a realistic setting, we wouldn't know the correct model size `k = 10`. LASSO deals with this by crossvalidating the regularization path. IHT can do the same with `cv_iht`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfolds = 5 # number of crossvalidation folds\n",
    "nlambda = 50 # number of lambda values to test\n",
    "pmax = n # maximum number of predictors allowed in a LASSO model; here we allow up to one predictor per case\n",
    "srand(2016)\n",
    "folds = IHT.cv_get_folds(y2, nfolds) # fix the crossvalidation folds; LASSO and IHT will use same fold structure\n",
    "cvlasso = glmnetcv(x, y2, dfmax=nmodels, pmax=pmax, nlambda=nmodels, nfolds=nfolds, folds=folds, parallel=true)\n",
    "cv_output = cv_iht(x, y2, pathidx, nfolds, folds=folds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both IHT and LASSO refit their best model sizes. `cv_iht` returns the best $\\boldsymbol{\\beta}$ directly, but we need to extract it from LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       "  3310\n",
       "  4460\n",
       "  9731\n",
       " 11294\n",
       " 11378\n",
       " 23295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       "  3310\n",
       "  4460\n",
       "  9731\n",
       " 11294\n",
       " 11378\n",
       " 23295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_cv_lasso = cvlasso.path.betas[:,indmin(cvlasso.meanloss)]\n",
    "bidx_cv_lasso = find(b_cv_lasso)\n",
    "display(cv_output.bidx)\n",
    "display(bidx_cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHT generally (but not always!) selects a more parsimonious set of features than LASSO, though IHT consistently underestimates the true model size. In this case, IHT and LASSO return the same models. The response `y2` is remarkably noisy, so correctly estimating the true model size will be quite hard. But take a peek at the estimated best $\\boldsymbol{\\beta}$ coefficient values compared to the true ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.341588  -0.33289   -0.0208224\n",
       "  0.612107   0.615219   0.316291 \n",
       "  0.17465    0.0        0.0      \n",
       " -0.494423  -0.60089   -0.2941   \n",
       "  1.36306    1.40044    1.09358  \n",
       " -0.58849   -0.613623  -0.327838 \n",
       "  0.155924   0.0        0.0      \n",
       "  0.031168   0.0        0.0      \n",
       "  0.374056   0.0        0.0      \n",
       " -1.38168   -1.35266   -1.03589  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfull_cv = zeros(size(b))\n",
    "bfull_cv[cv_output.bidx] = cv_output.b\n",
    "[b[bidx] bfull_cv[bidx] b_cv_lasso[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In crossvalidating the best model size, IHT loses the smallest components but estimates the remaining coefficients reasonably well. In contrast, LASSO coefficient estimates are biased towards zero. The effect of shrinkage is very stark.\n",
    "\n",
    "IHT.jl can leverage PLINK.jl to use PLINK files directly in parallel crossvalidation. However, the interface of `cv_iht` changes dramatically in order to accomodate the `SharedArray` constructors that enable parallel computing. The interface changes in two crucial ways. Firstly, instead of feeding the data `x` and `y` directly, we must point `cv_iht` to the locations of all data files stored on disk. Secondly, all data except the covariates must be stored in *binary* format.\n",
    "\n",
    "Storing data in binary format is actually quite simple. The call in Julia is `write`. For example, to save means and precisions from `xbed` in binary format, we would use\n",
    "\n",
    "    mpath = fpath * \"means.bin\"\n",
    "    ppath = fpath * \"precs.bin\"\n",
    "    write(open(mpath, \"w\"), xbed.means) # open file `mpath` and \"w\"rite vector `xbed.means` to it\n",
    "    write(open(ppath, \"w\"), xbed.precs) # same for `xbed.precs`\n",
    " \n",
    "For this demonstration, we exploit the precomputed means and precision from the PLINK.jl simulated data folder. Let us save `ybed` to the desktop for the time being: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypath = expanduser(\"~/Desktop/y.bin\")\n",
    "write(open(ypath, \"w\"), ybed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us run the crossvalidation routine with binary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IHT.IHTCrossvalidationResults{Float64}([21.7460922040304,20.447256597758287,19.83430694666251,19.391307662294984,19.101888891525793,18.986393164934988,18.9670406313367,18.955692783272347,18.950535468674527,18.950461574374113],[0.0],[0],10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covpath = fpath * \"covfile.txt\"; # this is a delimited TXT file, in this case merely the grand mean\n",
    "mpath = fpath * \"means.bin\"; # note: this is a BINARY file, not a TEXT file; we will explain why later\n",
    "ppath = fpath * \"precs.bin\"; # same as means\n",
    "cv_bed = cv_iht(xpath, xtpath, covpath, ypath, mpath, ppath, pathidx, folds, nfolds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

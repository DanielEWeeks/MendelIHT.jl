{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHT.jl tutorial\n",
    "\n",
    "In this tutorial we explore some of the functionality of the IHT.jl package, which implements iterative hard threhsolding on floating point arrays.\n",
    "\n",
    "IHT minimizes the residual sum of squares $\\frac{1}{2} \\| \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\|$ for data matrix $\\boldsymbol{X}$, response vector $\\boldsymbol{y}$, and coefficient vector $\\boldsymbol{\\beta}$. If $\\boldsymbol{\\beta}$ is $k$-sparse, then we have a sparse regression problem.\n",
    "\n",
    "Let us start by defining several simulation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x23999 Array{Float64,2}:\n",
       "  0.920157     0.255005  -0.327352   …  -0.621671   -0.60151     -0.106313 \n",
       "  0.395504    -0.102723  -0.0716819      0.965816   -0.196628     0.993377 \n",
       " -0.28422      1.32892    0.258232      -0.15159    -0.648796     0.76599  \n",
       " -0.430892     0.182859  -1.73484        0.269101   -1.45088     -0.0857599\n",
       " -1.47349     -0.388611  -1.41136       -0.0954367  -2.04859     -0.317166 \n",
       " -1.49638     -1.05493   -0.98462    …  -0.280058    0.410015    -1.04961  \n",
       "  1.57259     -0.639496   0.214717       0.912603    3.44151     -0.849509 \n",
       " -1.32385      0.319633   1.59429       -1.65875     2.50363      0.426919 \n",
       "  0.513201     0.564551   1.46493       -0.60994     0.121982     3.32438  \n",
       " -0.840922     0.734688   1.03843       -1.21892     0.148621     1.63371  \n",
       "  1.7489       0.717936  -1.20321    …   0.68411     0.518235     1.9636   \n",
       " -0.806362     1.11894    0.500853       2.42465     1.49706     -0.251184 \n",
       "  1.64923     -0.119484   1.37656       -0.0260431  -2.01264     -0.960593 \n",
       "  ⋮                                  ⋱                                     \n",
       " -0.785711     0.911631   0.610481       1.80682     0.407986    -0.592172 \n",
       "  0.102851     1.03728   -0.14861       -1.55677     0.824062     0.104468 \n",
       " -1.71512     -0.929433   1.32943    …   0.551024   -2.07909     -1.49316  \n",
       "  0.262388     0.465046  -0.882142       2.25975    -1.51205      0.284679 \n",
       "  0.521943    -0.492137   0.348153      -1.143       0.0778822    0.116254 \n",
       "  2.80295     -0.101732  -1.61145       -0.175095   -1.04984      0.4914   \n",
       "  0.512541     1.80423    0.346415       0.549381    0.00328396   1.13393  \n",
       "  0.867663     0.604629  -0.281941   …   0.0771052  -0.426471     0.859805 \n",
       "  1.4487       0.87053   -0.512609      -0.480449   -1.91928      0.377799 \n",
       "  1.80212      0.731044  -1.11472       -1.92572     0.594761    -0.451475 \n",
       "  0.540177     0.215494  -0.10147       -0.0625997   0.670315     0.408736 \n",
       "  0.00947786   0.586098   0.86891        0.175094    0.723134     0.433593 "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(5) # for later parallel XV\n",
    "using IHT\n",
    "n = 5000\n",
    "p = 23999\n",
    "k = 10\n",
    "s = 0.1\n",
    "x_temp = randn(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want our simulation to be reproducible, configure $\\boldsymbol{\\beta}$ with a fixed random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       "  5912\n",
       "  6593\n",
       " 10073\n",
       " 13599\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 18357\n",
       " 23140\n",
       " 23528"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(2016)\n",
    "b = zeros(p)\n",
    "b[1:k] = randn(k)\n",
    "shuffle!(b)\n",
    "bidx = find(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a noisy response $\\boldsymbol{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Float64,1}:\n",
       "  2.57     \n",
       " -1.02483  \n",
       " -3.1841   \n",
       "  0.0554894\n",
       "  2.22562  \n",
       " -0.745217 \n",
       "  0.605257 \n",
       " -0.0122337\n",
       "  1.33673  \n",
       " -3.4684   \n",
       " -4.25107  \n",
       " -4.11406  \n",
       " -6.93096  \n",
       "  ⋮        \n",
       " -1.31403  \n",
       "  0.775042 \n",
       " -0.445469 \n",
       " -2.31673  \n",
       " -1.59955  \n",
       "  0.45805  \n",
       " -0.837979 \n",
       "  1.77936  \n",
       " -0.62778  \n",
       "  0.898485 \n",
       " -0.777918 \n",
       " -4.6423   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x_temp*b + s*randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure a regression problem. In this case, we need a data matrix with a grand mean included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x24000 Array{Float64,2}:\n",
       "  0.920157     0.255005  -0.327352   …  -0.60151     -0.106313   1.0\n",
       "  0.395504    -0.102723  -0.0716819     -0.196628     0.993377   1.0\n",
       " -0.28422      1.32892    0.258232      -0.648796     0.76599    1.0\n",
       " -0.430892     0.182859  -1.73484       -1.45088     -0.0857599  1.0\n",
       " -1.47349     -0.388611  -1.41136       -2.04859     -0.317166   1.0\n",
       " -1.49638     -1.05493   -0.98462    …   0.410015    -1.04961    1.0\n",
       "  1.57259     -0.639496   0.214717       3.44151     -0.849509   1.0\n",
       " -1.32385      0.319633   1.59429        2.50363      0.426919   1.0\n",
       "  0.513201     0.564551   1.46493        0.121982     3.32438    1.0\n",
       " -0.840922     0.734688   1.03843        0.148621     1.63371    1.0\n",
       "  1.7489       0.717936  -1.20321    …   0.518235     1.9636     1.0\n",
       " -0.806362     1.11894    0.500853       1.49706     -0.251184   1.0\n",
       "  1.64923     -0.119484   1.37656       -2.01264     -0.960593   1.0\n",
       "  ⋮                                  ⋱                              \n",
       " -0.785711     0.911631   0.610481       0.407986    -0.592172   1.0\n",
       "  0.102851     1.03728   -0.14861        0.824062     0.104468   1.0\n",
       " -1.71512     -0.929433   1.32943    …  -2.07909     -1.49316    1.0\n",
       "  0.262388     0.465046  -0.882142      -1.51205      0.284679   1.0\n",
       "  0.521943    -0.492137   0.348153       0.0778822    0.116254   1.0\n",
       "  2.80295     -0.101732  -1.61145       -1.04984      0.4914     1.0\n",
       "  0.512541     1.80423    0.346415       0.00328396   1.13393    1.0\n",
       "  0.867663     0.604629  -0.281941   …  -0.426471     0.859805   1.0\n",
       "  1.4487       0.87053   -0.512609      -1.91928      0.377799   1.0\n",
       "  1.80212      0.731044  -1.11472        0.594761    -0.451475   1.0\n",
       "  0.540177     0.215494  -0.10147        0.670315     0.408736   1.0\n",
       "  0.00947786   0.586098   0.86891        0.723134     0.433593   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = zeros(n,p+1)\n",
    "setindex!(x, x_temp, :, 1:p)\n",
    "x[:,end] = 1.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `x`, run iterative hard thresholding to pull the best model of size `k`. The function call is `L0_reg`, which returns a `Dict{ASCIIString,Any}` with the following key-value pairs:\n",
    "\n",
    "    - \"time\" => compute time\n",
    "    - \"iter\" => iterations taken\n",
    "    - \"loss\" => residual sum of squares at convergence\n",
    "    - \"beta\" => beta vector at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.938486  -0.93735 \n",
       " -1.05075   -1.05182 \n",
       "  1.47077    1.47139 \n",
       " -0.139799  -0.141464\n",
       " -1.21187   -1.21198 \n",
       " -0.974419  -0.972236\n",
       " -1.10666   -1.10799 \n",
       "  0.429356   0.428427\n",
       " -2.09573   -2.09758 \n",
       " -0.494523  -0.49493 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = L0_reg(x,y,k);\n",
    "bk = copy(output[\"beta\"])\n",
    "[b[bidx] bk[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that IHT returns all the correct nonzero coefficients. The coefficient values themselves are fairly close to their originals. We should expect this since `s = 0.1` is not very strong noise. Observe what happens when we increase the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.938486  -0.93735   -1.14044 \n",
       " -1.05075   -1.05182   -0.779405\n",
       "  1.47077    1.47139    1.40886 \n",
       " -0.139799  -0.141464   0.0     \n",
       " -1.21187   -1.21198   -0.963054\n",
       " -0.974419  -0.972236  -1.1041  \n",
       " -1.10666   -1.10799   -1.14329 \n",
       "  0.429356   0.428427   0.0     \n",
       " -2.09573   -2.09758   -2.31173 \n",
       " -0.494523  -0.49493   -0.802226"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 10\n",
    "y2 = x_temp*b + s*randn(n)\n",
    "output2 = L0_reg(x, y2, k);\n",
    "bk2 = copy(output2[\"beta\"])\n",
    "[b[bidx] bk[bidx] bk2[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Paths and LASSO\n",
    "\n",
    "The coefficient values are less accurate, and we lost three nonzeroes. Still, the model recovery performance of IHT is not bad. We still need some sort of benchmark. [GLMNet.jl](https://github.com/simonster/GLMNet.jl) offers one way to test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Least Squares GLMNet Solution Path (11 solutions for 24000 predictors in 47 passes):\n",
       "11×3 DataFrames.DataFrame\n",
       "│ Row │ df │ pct_dev   │ λ        │\n",
       "├─────┼────┼───────────┼──────────┤\n",
       "│ 1   │ 0  │ 0.0       │ 2.28262  │\n",
       "│ 2   │ 1  │ 0.0123469 │ 1.94746  │\n",
       "│ 3   │ 1  │ 0.0213341 │ 1.66151  │\n",
       "│ 4   │ 1  │ 0.0278759 │ 1.41755  │\n",
       "│ 5   │ 2  │ 0.0364219 │ 1.20941  │\n",
       "│ 6   │ 5  │ 0.050224  │ 1.03183  │\n",
       "│ 7   │ 6  │ 0.0644825 │ 0.880322 │\n",
       "│ 8   │ 8  │ 0.0767626 │ 0.751062 │\n",
       "│ 9   │ 8  │ 0.0874198 │ 0.640782 │\n",
       "│ 10  │ 12 │ 0.0963946 │ 0.546695 │\n",
       "│ 11  │ 33 │ 0.10967   │ 0.466422 │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using GLMNet\n",
    "srand(2016)\n",
    "nmodels = 30\n",
    "lassopath = glmnet(x, y2, dfmax=nmodels, nlambda=nmodels) # try to test only a few models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the estimates of $\\boldsymbol{\\beta}$ with `k = 10` nonzeroes. In this example, the LASSO does not return the precise model size that we want (a pointed advantage of IHT!) but row 9 will give us a decent approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -1.14044   -0.48379 \n",
       " -0.779405  -0.146337\n",
       "  1.40886    0.751371\n",
       "  0.0        0.0     \n",
       " -0.963054  -0.337626\n",
       " -1.1041    -0.505814\n",
       " -1.14329   -0.525992\n",
       "  0.0        0.0     \n",
       " -2.31173   -1.65493 \n",
       " -0.802226  -0.159854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = convert(Matrix{Float64}, lassopath.betas)\n",
    "[bk2[bidx] betas[bidx,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO selects the correct nonzeroes, but the coefficients are shrunk because LASSO is a shrinkage operator. Users should consider this fact when choosing a feature selection tool. In some scenarios, such as genome-wide association studies, the coefficient values are often quite small, and shrinkage can complicate accurate estimation of the statistical model.\n",
    "\n",
    "LASSO is most efficient when used to compute a regularization path. IHT.jl provides `iht_path` to mimic this behavior.\n",
    "Let us compute model sizes $1, 2, \\ldots, 30$ with IHT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -1.14044   -1.14763 \n",
       " -0.779405  -0.752326\n",
       "  1.40886    1.40883 \n",
       "  0.0        0.0     \n",
       " -0.963054  -0.971061\n",
       " -1.1041    -1.11848 \n",
       " -1.14329   -1.16075 \n",
       "  0.0        0.0     \n",
       " -2.31173   -2.30384 \n",
       " -0.802226  -0.806393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colidx  = countnz(sub(betas, :, 9))\n",
    "pathidx = collect(1:nmodels)\n",
    "ihtpath = iht_path(x, y2, pathidx) # ihtpath is a sparse matrix\n",
    "[bk2[bidx] full(ihtpath[bidx,colidx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation\n",
    "\n",
    "These exploratory efforts are admittedly not illuminating. In a realistic setting, we wouldn't know the correct model size `k = 10`. LASSO deals with this by crossvalidating the regularization path. IHT can do the same with `cv_iht`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n"
     ]
    }
   ],
   "source": [
    "nfolds = 5\n",
    "srand(2016)\n",
    "cvlasso = glmnetcv(x, y2, dfmax=nmodels, nlambda=nmodels, nfolds=nfolds)\n",
    "errors, b_cv_iht, bidx_cv_iht = cv_iht(x, y2, pathidx, nfolds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both IHT and LASSO refit their best model sizes. `cv_iht` returns the best $\\boldsymbol{\\beta}$ directly, but we need to extract it from LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       "  5912\n",
       " 10073\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 23140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12-element Array{Int64,1}:\n",
       "  4796\n",
       "  5912\n",
       "  6593\n",
       " 10073\n",
       " 13550\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 18357\n",
       " 21358\n",
       " 23140\n",
       " 23528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_cv_lasso = cvlasso.path.betas[:,indmin(cvlasso.meanloss)]\n",
    "bidx_cv_lasso = find(b_cv_lasso)\n",
    "display(bidx_cv_iht)\n",
    "display(bidx_cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO selects far too many features. IHT selects a more parsimonious set, though it underestimates the true model size. Take a peek at the estimated best $\\boldsymbol{\\beta}$ values compared to the true ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.938486  -1.13444   -0.580837 \n",
       " -1.05075    0.0       -0.238076 \n",
       "  1.47077    1.42045    0.847557 \n",
       " -0.139799   0.0        0.0      \n",
       " -1.21187   -0.993201  -0.430349 \n",
       " -0.974419  -1.1232    -0.594152 \n",
       " -1.10666   -1.15451   -0.61679  \n",
       "  0.429356   0.0        0.0177542\n",
       " -2.09573   -2.30607   -1.7504   \n",
       " -0.494523   0.0       -0.254586 "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfull_cv = zeros(size(b))\n",
    "bfull_cv[bidx_cv_iht] = b_cv_iht\n",
    "[b[bidx] bfull_cv[bidx] b_cv_lasso[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In crossvalidating the best model size, IHT loses the smallest components but estimates the remaining coefficients reasonably well. LASSO grabs more of the true model but also pulls in a lot of garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

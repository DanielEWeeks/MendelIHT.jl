{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHT.jl tutorial\n",
    "\n",
    "In this tutorial we explore some of the functionality of the IHT.jl package, which implements iterative hard threhsolding on floating point arrays.\n",
    "\n",
    "IHT minimizes the residual sum of squares $\\frac{1}{2} \\| \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\|$ for data matrix $\\boldsymbol{X}$, response vector $\\boldsymbol{y}$, and coefficient vector $\\boldsymbol{\\beta}$. If $\\boldsymbol{\\beta}$ is $k$-sparse, then we have a sparse regression problem.\n",
    "\n",
    "Let us start by defining several simulation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x23999 Array{Float64,2}:\n",
       "  0.257141  -0.724777   0.859458   …   0.07989    -0.0800017   0.595581 \n",
       "  0.740772   0.268289  -0.145353       0.303898   -0.0645964  -1.64504  \n",
       "  0.816615  -0.688146  -0.232626       1.40207    -0.125299    0.789344 \n",
       " -1.02271   -1.67805   -1.04104       -0.225945    0.692461   -0.692703 \n",
       "  0.373389   0.832239  -0.589455       0.925253    0.777032   -0.97044  \n",
       "  0.78856   -1.83465    2.15891    …   0.0815397  -0.0720443  -0.24342  \n",
       "  1.92936    0.346339  -0.609725      -1.8729     -1.97128    -0.762739 \n",
       " -0.181114   1.84331   -1.84085        0.584192    0.979634   -1.74861  \n",
       "  0.592886  -0.564346  -0.53547        0.489543   -1.02777    -1.46408  \n",
       "  0.605932   0.459408   0.583701       0.905324   -0.0502125  -0.988436 \n",
       " -0.579504  -1.52012    0.603341   …  -0.810088   -0.990011   -0.33724  \n",
       " -1.79151   -0.150712   0.459638       0.561198    1.80619     1.44635  \n",
       "  0.269643   0.575173   1.00205       -0.874883    0.84332     1.9512   \n",
       "  ⋮                                ⋱                                    \n",
       "  0.648608   1.15789   -0.0619428     -1.26899     0.323898    1.91252  \n",
       "  1.26272   -0.622567   1.16077       -0.0101824   0.673453   -0.199273 \n",
       "  2.60465   -0.450946  -0.999143   …  -0.334364    0.544976   -0.508279 \n",
       " -0.797807   0.895987  -1.0412        -0.0824833  -0.315735   -0.827675 \n",
       " -0.296944   0.427605   1.66712       -0.13281    -1.25864    -0.726688 \n",
       "  0.991459   1.53483    0.225661       0.339169   -0.511203   -0.518208 \n",
       "  0.256427   0.744305  -0.476027       1.80979    -1.44664    -1.40918  \n",
       " -1.69247   -0.598303   1.23371    …   0.244398   -1.0927     -0.615681 \n",
       " -1.85276    2.0766    -1.26709       -0.918315   -1.31123     0.259527 \n",
       "  0.419523  -0.403663  -1.72676        1.05595    -0.523378   -2.08133  \n",
       " -1.63963    0.474983   0.396802      -0.241855   -0.195927    1.35916  \n",
       " -0.609568   0.178131   0.0615888      0.3655     -1.82675     0.0976275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(5) # for later parallel XV\n",
    "using IHT\n",
    "n = 5000\n",
    "p = 23999\n",
    "k = 10\n",
    "s = 0.1\n",
    "x_temp = randn(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want our simulation to be reproducible, configure $\\boldsymbol{\\beta}$ with a fixed random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       "  5912\n",
       "  6593\n",
       " 10073\n",
       " 13599\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 18357\n",
       " 23140\n",
       " 23528"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(2016)\n",
    "b = zeros(p)\n",
    "b[1:k] = randn(k)\n",
    "shuffle!(b)\n",
    "bidx = find(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a noisy response $\\boldsymbol{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Float64,1}:\n",
       "  1.80556  \n",
       "  1.04619  \n",
       "  3.7542   \n",
       "  1.09357  \n",
       "  0.411478 \n",
       "  4.64548  \n",
       "  1.55792  \n",
       "  2.62063  \n",
       " -8.06077  \n",
       " -1.68568  \n",
       " -0.998167 \n",
       "  1.17064  \n",
       " -0.502512 \n",
       "  ⋮        \n",
       " -0.558193 \n",
       " -1.39937  \n",
       " -3.71941  \n",
       " -2.14715  \n",
       "  1.70349  \n",
       " -0.0725569\n",
       "  5.45024  \n",
       " -2.19511  \n",
       " -3.33305  \n",
       " -0.269835 \n",
       "  1.07424  \n",
       " -0.153398 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x_temp*b + s*randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure a regression problem. In this case, we need a data matrix with a grand mean included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000x24000 Array{Float64,2}:\n",
       "  0.257141  -0.724777   0.859458    0.395087   …  -0.0800017   0.595581   1.0\n",
       "  0.740772   0.268289  -0.145353    0.339015      -0.0645964  -1.64504    1.0\n",
       "  0.816615  -0.688146  -0.232626   -0.413353      -0.125299    0.789344   1.0\n",
       " -1.02271   -1.67805   -1.04104     1.03776        0.692461   -0.692703   1.0\n",
       "  0.373389   0.832239  -0.589455    0.600091       0.777032   -0.97044    1.0\n",
       "  0.78856   -1.83465    2.15891     0.136858   …  -0.0720443  -0.24342    1.0\n",
       "  1.92936    0.346339  -0.609725   -2.13567       -1.97128    -0.762739   1.0\n",
       " -0.181114   1.84331   -1.84085     0.123631       0.979634   -1.74861    1.0\n",
       "  0.592886  -0.564346  -0.53547     1.30807       -1.02777    -1.46408    1.0\n",
       "  0.605932   0.459408   0.583701   -0.0100444     -0.0502125  -0.988436   1.0\n",
       " -0.579504  -1.52012    0.603341    0.38614    …  -0.990011   -0.33724    1.0\n",
       " -1.79151   -0.150712   0.459638   -1.43802        1.80619     1.44635    1.0\n",
       "  0.269643   0.575173   1.00205     0.411787       0.84332     1.9512     1.0\n",
       "  ⋮                                            ⋱                             \n",
       "  0.648608   1.15789   -0.0619428   0.127343       0.323898    1.91252    1.0\n",
       "  1.26272   -0.622567   1.16077     0.533978       0.673453   -0.199273   1.0\n",
       "  2.60465   -0.450946  -0.999143   -0.431492   …   0.544976   -0.508279   1.0\n",
       " -0.797807   0.895987  -1.0412     -0.177307      -0.315735   -0.827675   1.0\n",
       " -0.296944   0.427605   1.66712     0.524997      -1.25864    -0.726688   1.0\n",
       "  0.991459   1.53483    0.225661   -0.865576      -0.511203   -0.518208   1.0\n",
       "  0.256427   0.744305  -0.476027   -0.0501275     -1.44664    -1.40918    1.0\n",
       " -1.69247   -0.598303   1.23371    -1.77814    …  -1.0927     -0.615681   1.0\n",
       " -1.85276    2.0766    -1.26709     0.442449      -1.31123     0.259527   1.0\n",
       "  0.419523  -0.403663  -1.72676    -0.533168      -0.523378   -2.08133    1.0\n",
       " -1.63963    0.474983   0.396802   -0.16476       -0.195927    1.35916    1.0\n",
       " -0.609568   0.178131   0.0615888   0.0710666     -1.82675     0.0976275  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = zeros(n,p+1)\n",
    "setindex!(x, x_temp, :, 1:p)\n",
    "x[:,end] = 1.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `x`, run iterative hard thresholding to pull the best model of size `k`. The function call is `L0_reg`, which returns an `IHTReults` with the following fields:\n",
    "\n",
    "    - \"time\" => compute time\n",
    "    - \"iter\" => iterations taken\n",
    "    - \"loss\" => residual sum of squares at convergence\n",
    "    - \"beta\" => beta vector at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.938486  -0.936704\n",
       " -1.05075   -1.05165 \n",
       "  1.47077    1.47351 \n",
       " -0.139799  -0.142716\n",
       " -1.21187   -1.212   \n",
       " -0.974419  -0.97422 \n",
       " -1.10666   -1.10694 \n",
       "  0.429356   0.428181\n",
       " -2.09573   -2.09561 \n",
       " -0.494523  -0.492939"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = L0_reg(x,y,k);\n",
    "bk = copy(output.beta)\n",
    "[b[bidx] bk[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that IHT returns all the correct nonzero coefficients. The coefficient values themselves are fairly close to their originals. We expect this since `s = 0.1` does not yield a very noisy `y`. Observe what happens when we increase the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.938486  -0.936704  -0.881068\n",
       " -1.05075   -1.05165   -0.993175\n",
       "  1.47077    1.47351    1.23718 \n",
       " -0.139799  -0.142716   0.0     \n",
       " -1.21187   -1.212     -1.27654 \n",
       " -0.974419  -0.97422   -1.0409  \n",
       " -1.10666   -1.10694   -1.3352  \n",
       "  0.429356   0.428181   0.599866\n",
       " -2.09573   -2.09561   -1.97351 \n",
       " -0.494523  -0.492939  -0.53444 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 10\n",
    "y2 = x_temp*b + s*randn(n)\n",
    "output2 = L0_reg(x, y2, k);\n",
    "bk2 = copy(output2.beta)\n",
    "[b[bidx] bk[bidx] bk2[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient values are less accurate, and we lost three nonzeroes. Still, the model recovery performance of IHT is not bad. We still need some sort of benchmark. \n",
    "\n",
    "## Regularization Paths and LASSO\n",
    "[GLMNet.jl](https://github.com/simonster/GLMNet.jl) offers one way to benchmark model recovery with IHT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Least Squares GLMNet Solution Path (11 solutions for 24000 predictors in 46 passes):\n",
       "11×3 DataFrames.DataFrame\n",
       "│ Row │ df │ pct_dev   │ λ        │\n",
       "├─────┼────┼───────────┼──────────┤\n",
       "│ 1   │ 0  │ 0.0       │ 2.28262  │\n",
       "│ 2   │ 0  │ 0.0       │ 1.94746  │\n",
       "│ 3   │ 1  │ 0.0070533 │ 1.66151  │\n",
       "│ 4   │ 1  │ 0.0136703 │ 1.41755  │\n",
       "│ 5   │ 4  │ 0.0230167 │ 1.20941  │\n",
       "│ 6   │ 5  │ 0.039381  │ 1.03183  │\n",
       "│ 7   │ 7  │ 0.0546554 │ 0.880322 │\n",
       "│ 8   │ 7  │ 0.0675024 │ 0.751062 │\n",
       "│ 9   │ 7  │ 0.0768536 │ 0.640782 │\n",
       "│ 10  │ 9  │ 0.0843812 │ 0.546695 │\n",
       "│ 11  │ 34 │ 0.0974616 │ 0.466422 │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using GLMNet\n",
    "srand(2016)\n",
    "nmodels = 11\n",
    "lambda = [2.28262\n",
    "1.94746\n",
    "1.66151\n",
    "1.41755\n",
    "1.20941\n",
    "1.03183\n",
    "0.880322\n",
    "0.751062\n",
    "0.640782\n",
    "0.546695\n",
    "0.466422]\n",
    "lassopath = glmnet(x, y2, lambda=lambda) # try to test only a few models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the estimates of $\\boldsymbol{\\beta}$ with `k = 10` nonzeroes. The precise model chosen by the LASSO can vary! Consequently, the LASSO may not return the precise model size that we want (a pointed advantage of IHT!) but row 10 should give us a decent approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.881068  -0.360174\n",
       " -0.993175  -0.44382 \n",
       "  1.23718    0.697118\n",
       "  0.0        0.0     \n",
       " -1.27654   -0.717244\n",
       " -1.0409    -0.554452\n",
       " -1.3352    -0.79415 \n",
       "  0.599866   0.046781\n",
       " -1.97351   -1.38309 \n",
       " -0.53444    0.0     "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = convert(Matrix{Float64}, lassopath.betas)\n",
    "[bk2[bidx] betas[bidx,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO selects the correct nonzeroes, but the coefficients are shrunk because LASSO is a shrinkage operator. Users should consider this fact when choosing a feature selection tool. In some scenarios, such as genome-wide association studies, the coefficient values are often quite small, and shrinkage can complicate accurate estimation of the statistical model.\n",
    "\n",
    "LASSO is most efficient when used to compute a regularization path. IHT.jl provides `iht_path` to mimic this behavior.\n",
    "Let us compute model sizes $1, 2, \\ldots, 11$ with IHT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x2 Array{Float64,2}:\n",
       " -0.881068  -0.887491\n",
       " -0.993175  -1.00572 \n",
       "  1.23718    1.2469  \n",
       "  0.0        0.0     \n",
       " -1.27654   -1.29271 \n",
       " -1.0409    -1.06223 \n",
       " -1.3352    -1.33721 \n",
       "  0.599866   0.598986\n",
       " -1.97351   -1.96237 \n",
       " -0.53444   -0.545949"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colidx  = countnz(sub(betas, :, 10))\n",
    "pathidx = collect(1:nmodels)\n",
    "ihtbetas = iht_path(x, y2, pathidx) # ihtpath is a sparse matrix\n",
    "[bk2[bidx] full(ihtbetas[bidx,10])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation\n",
    "\n",
    "These exploratory efforts are admittedly not illuminating. In a realistic setting, we wouldn't know the correct model size `k = 10`. LASSO deals with this by crossvalidating the regularization path. IHT can do the same with `cv_iht`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n",
      "WARNING: glmnet: number of non-zero coefficients along path exceeds pmax=min(dfmax*2+20,nvars) at 1010000th lambda value\n"
     ]
    }
   ],
   "source": [
    "nfolds = 5\n",
    "srand(2016)\n",
    "folds = IHT.cv_get_folds(y, nfolds)\n",
    "cvlasso = glmnetcv(x, y2, dfmax=nmodels, nlambda=nmodels, nfolds=nfolds, folds=folds)\n",
    "cv_output = cv_iht(x, y2, pathidx, nfolds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both IHT and LASSO refit their best model sizes. `cv_iht` returns the best $\\boldsymbol{\\beta}$ directly, but we need to extract it from LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Int64,1}:\n",
       "  5912\n",
       "  6593\n",
       " 10073\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 23140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7-element Array{Int64,1}:\n",
       "  5912\n",
       "  6593\n",
       " 10073\n",
       " 14572\n",
       " 14929\n",
       " 18057\n",
       " 23140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_cv_lasso = cvlasso.path.betas[:,indmin(cvlasso.meanloss)]\n",
    "bidx_cv_lasso = find(b_cv_lasso)\n",
    "display(cv_output.bidx)\n",
    "display(bidx_cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHT generally (but not always!) selects a more parsimonious set of features than LASSO, though IHT consistently underestimates the true model size. Take a peek at the estimated best $\\boldsymbol{\\beta}$ values compared to the true ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10x3 Array{Float64,2}:\n",
       " -0.938486  -0.887153  -0.163926\n",
       " -1.05075   -0.985036  -0.241787\n",
       "  1.47077    1.24926    0.491653\n",
       " -0.139799   0.0        0.0     \n",
       " -1.21187   -1.28114   -0.505937\n",
       " -0.974419  -1.06152   -0.365709\n",
       " -1.10666   -1.32569   -0.595549\n",
       "  0.429356   0.0        0.0     \n",
       " -2.09573   -1.96175   -1.16551 \n",
       " -0.494523   0.0        0.0     "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfull_cv = zeros(size(b))\n",
    "bfull_cv[cv_output.bidx] = cv_output.b\n",
    "[b[bidx] bfull_cv[bidx] b_cv_lasso[bidx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In crossvalidating the best model size, IHT loses the smallest components but estimates the remaining coefficients reasonably well. LASSO tends to grab more of the true model but also tends to pull in a lot of garbage. The effect of shrinkage is very stark; we see that the estimated coefficients are biased towards zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
